<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="main.css">
    <link href='https://fonts.googleapis.com/css?family=Raleway' rel='stylesheet'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>

 <div class="footer-social-icons">
   
    <ul class="social-icons">
        <li><a href="https://github.com/zaidalyafeai" class="social-icon"> <i class="fa fa-github"></i></a></li>
        <li><a href="https://twitter.com/zaidalyafeai" class="social-icon"> <i class="fa fa-twitter"></i></a></li>
        <li><a href="https://www.facebook.com/zaid.alyafey" class="social-icon"> <i class="fa fa-facebook"></i></a></li>
    </ul>
</div>
    
    <p>
        My Name is Zaid Alyafeai from YEMEN. I am a Master student at KFUPM in Saudi Arabia. I work on machine learning and its applications 
        in computer visions. I like to create my own projects and try to solve them using deep learning. I also like to write about deep learning
        mainly tutorials or notebooks with code. I am mostly active on twitter <a href = "https://twitter.com/zaidalyafeai">@zaidalyafeai</a>. I am open to speaking on conferences or work on part-time projects. 
    </p>
<hr>
<h1 class = "sec">Projects </h1>
<hr>

<h2 class = "subsec">Fast pix2pix in the Browser <span class = 'LINK'><a href = "https://goo.gl/oXk2qV">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/rJE1uP">[DEMO]</a></span></h2>
<p> The main purpose of this project is to create a web applications for quick image to image translation.  The implementation is based
    on the paper "Image-to-Image Translation with Conditional Adversarial Networks" by <a href  = "https://arxiv.org/abs/1611.07004">Philip Isola, et al. </a>    . The user can draw a sketch or a semantic map to the left and the application will render it to a real image on the right canvas. 
    The model was trained using Tensorflow and converted to a web application using Tensorflow.js. 
</p>

<video controls><source src="videos/pix2pix.mp4" type="video/mp4"></video>

<h2 class = "subsec" >Real Time Face Segmentation <span class = 'LINK'><a href = "https://goo.gl/mE9Tsi">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/qp3EMg">[DEMO]</a></span> </h2>
<p> The main purpose of this project is to design an application to make face segmentation directly from the webcam. The basic model is a U-Net 
    model extracted from pix2pix trained on this faces <a href = "http://www.mut1ny.com/face-headsegmentation-dataset">dataset</a>. To make sure that the model runs in real time the model was trained with less
    parameters and more augmented dataset.  
</p>

<video controls><source src="videos/segmentation.mp4" type="video/mp4"></video>


<h2 class = "subsec">Webcam Image Reconstruction <span class = 'LINK'><a href = "https://goo.gl/SAhkoE">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/ZJAhev">[DEMO]</a></span></h2>
<p> In this project the model takes live feed from the webcam and tries to reconstuct the original image. It is based on two models. The first 
    model is based on the face semgnetation project; so basically it creates face segmentation. The second model was trained on a celebrity dataset
    where the input is a segmented face and the output is a celebirty face. 
</p>

<video controls><source src="videos/reconstruction.mp4" type="video/mp4"></video>


<h2 class = "subsec">Sketcher <span class = 'LINK'><a href = "https://goo.gl/uqsznz">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/crTzY9">[DEMO]</a></span></h2>
<p> This projects is designed to recognize drawings in real time. The user draws a shape in the canvas and the model will predict the top 5 
    objects that match the drawing. It was trained on 100 classes taken from the quick draw dataset by <a href ="https://github.com/googlecreativelab/quickdraw-dataset">Google creative lab</a>. 
</p>

<video controls><source src="videos/sketcher.mp4" type="video/mp4"></video>

<h2 class = "subsec">Texter <span class = 'LINK'><a href = "https://goo.gl/jzd1Qb">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/TnD2Np">[DEMO]</a></span></h2>
<p> This project is designed to recognize latex symbols in real time. The user draws a shape in the canvas and the model will predict the top 5 
    symbols that match the drawing. It was trained on 369  classes taken from the <a href="https://github.com/MartinThoma/HASY">HASY dataset</a>. 
</p>

<video controls><source src="videos/texter.webm" type="video/mp4"></video>

<h2 class = "subsec">Style Transfer <span class = 'LINK'><a href = "https://goo.gl/jzd1Qb">[CODE]</a></span> <span class = 'LINK'><a href = "https://goo.gl/TnD2Np">[DEMO]</a></span></h2>
<p>  
This projects implements 5 pretrained fast style transfer models. The web model was converted from these keras <a href = "https://github.com/misgod/fast-neural-style-keras">models</a>. 
The keras implementation is based on the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by <a href = "https://arxiv.org/abs/1603.08155">Justin Johnson, et al.</a>
</p>
    
    <video controls><source src="videos/style-transfer.mp4" type="video/mp4"></video>

  <h2 class = "subsec">Real Time Style Transfer <span class = 'LINK'><a href = "https://github.com/zaidalyafeai/zaidalyafeai.github.io/tree/master/WST">[CODE]</a></span> <span class = 'LINK'><a href = "https://zaidalyafeai.github.io/WST/">[DEMO]</a></span></h2>
<p>  
I trained a pix2pix model to learn a style transfer on portrait images. First the dataset is generated by using this 
    <a href="https://github.com/misgod/fast-neural-style-keras"> model</a>. We save the image and its styled pair and we 
    input it into a pix2pix model. The dataset contains around 700 celebrity images. 
</p>
    
    <video controls><source src="videos/wst.mp4" type="video/mp4"></video>
    
 <h2 class = "subsec">QuickDraw10 <span class = 'LINK'><a href = "https://github.com/zaidalyafeai/QuickDraw10">[CODE]</a></span> </h2>
<p>  
    This dataset was collected by Google from people drawing different objects. The dataset is a collection of 50 million drawings from 345 different objects that is available publically for everyone. 
    We extract a simple subset of 10 classes of the dataset for simple machine learning tasks. 
    The dataset is suggested as an alternative for MNIST.
  </p>
    
<img src = "imgs/quickdraw.png"></img>
 


<h1 class = "sec">Tutorials </h1>
<hr>

<h2 class = "subsec">A Gentle Introduction to Tensorflow.js <span class = 'LINK'><a href = "https://goo.gl/Ezu1SU">[LINK]</a></span></h2>
<p>A complete walk-through on how to create machine learning models in the browser. The new Tensorflow.js library from Google opens new possibities
    for developers to start training and deploying machine learning models using javascript. I start by explaining the basic blocks of the language 
    then I devele deep to explain how to create models and load pretrained ones. 

</p>

<img src = "imgs/tfjs.PNG"></img>

<h2 class = "subsec">From Colab to the Browser <span class = 'LINK'><a href = "https://goo.gl/BYv8Ez">[LINK]</a></span></h2>
<p>In this tutorial I explain how to train a model in colab with tf.keras then deploy it in the browser with Tensorflow.js. Colab from google allows
    training on GPU and TPU for free for around 12 hours. 
</p>

<img src = "imgs/colab-browser.PNG"></img>

<h2 class = "subsec">Sentiment Classification from Keras to the Browser <span class = 'LINK'><a href = "https://goo.gl/fsYsv2">[LINK]</a></span></h2>
<p>In this tutorial I explain how to make a sentiement classification in keras then deploy it in the browser using Tensorflow.js. The RNN model 
    is trained on a movie review dataset that classifies bad/good movies from text inputs. The model is designed with keras and converted to a web 
    model using Tensorflow.js.  

</p>

<img src = "imgs/sentiement.PNG"></img>


<h2 class = "subsec">Train  and Deploy ML models using One Notebook <span class = 'LINK'><a href = "https://goo.gl/Yyhp1t">[LINK]</a></span></h2>
<p>In this tutorial I explain how to create a Notebook on google colab. The notebook explores the possiblitiy of training, testing and deploying a model 
    a model in the browser without leaving the notebook. I also explained how to post the project on GitHub directely from Colab using tokens. 
</p>

<img src = "imgs/one-place.PNG"></img>


<h2 class = "subsec">Fast pix2pix in the Browser <span class = 'LINK'><a href = "https://goo.gl/gQAdMh">[LINK]</a></span></h2>
<p>In this tutorial I explain how I created the fast pix2pix application. It gives a compete walk-though of training then porting the model to the 
    browser. 

</p>

<img src = "imgs/pix2pix.PNG"></img>

<h2 class = "subsec">A dive into the latent space of BigGan<span class = 'LINK'><a href = "https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/">[LINK]</a></span></h2>
<p>
An article featuerd in the Gradient publication about using state of the art image generation method (BigGan) to create art. It featuers
    many people's work with BigGan and also included my work on different experiemnts to explain the latent space of such powerful
    GANs. 
</p>

<img src = "imgs/biggan.jpg"></img>


<h1 class = "sec">Notebooks</h1>
<hr>
<h2 class = "subsec">Training pix2pix </h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/tf_pix2pix.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p> This notebook shows a simple pipeline for training pix2pix on a simple dataset. 

</p>

<h2 class = "subsec">One Place </h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/ONePlace.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p> This notebook shows how to train, test then deploy models in the browser directly from one notebook.  We use a simple XOR example 
    to prove this simple concept. 

</p>

<h2 class = "subsec">TPU vs GPU </h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/GPUvsTPU.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p> Google recently allowed training on TPUs for free on colab. This notebook explains how to enable TPU training. Also, it reports some 
    benchmarks using mnist dataset by comparing TPU and GPU performance.   
</p>

<h2 class = "subsec">Keras Custom Generator</h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Custom_Data_Generator_in_Keras.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>

<p> This notebook shows to create a custom data genertor in keras.  
</p>

<h2 class = "subsec">Eager Execution and Gradient </h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Gradient_.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p> As we know that TenosrFlow works with static graphs. So, first you have to create the graph then execute it later. This makes debugging a bit complicated. 
    With Eager Execution you can now evalute operations directly without creating a session.
</p>

<h2 class = "subsec">Eager Execution Enabled </h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Enabled.ipynb">
    <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p> In this notebook I explain different concepts in eager execution. I go over variables, ops, gradients, custom gradients, 
    callbacks, metrics and creating models with tf.keras and saving/restoring them. 
</p>

<h2 class = "subsec">Sketcher</h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Sketcher.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>

<p>
Create a simple app to recognize 100 drawings from the quickdraw dataset.
A simple CNN model is created and served to deoploy in the browser to create a sketch recognizer app. 
</p>

<h2 class = "subsec">QuickDraw10</h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/QuickDraw10.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
In this notebook we provide QuickDraw10 as an alternative for MNIST. A script is provided to download and load a preprocessed dataset for 10 classes with training and testing split. 
    Also, a simple CNN model is implemented for training and testing.
</p>

<h2 class = "subsec">AutoEncoders</h2>
<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/AutoEncoders.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
Autoencoders consists of two structures: the encoder and the decoder. The encoder network downsamples the data into lower dimensions and the decoder network reconstructs the original data from the lower dimension representation. 
The lower dimension representation is usually called latent space representation.
</p>

<h2 class = "subsec">Weight Transfer</h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/WeightTransfer.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
In this tutorial we explain how to transfer weights from a static graph model built with TensorFlow to a dynamic graph built with Keras. 
We will first train a model using Tensorflow then we will create the same model in keras and transfer the trained weights between the two models. 
    </p>


<h2 class = "subsec">BigGan</h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/BigGan.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
Create some cool gifs by interpolation in the latent space of the BigGan model. The model is imported from tensorflow hub.
 </p>

<h2 class = "subsec">BigGanEx</h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/BigGanEx.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
In this notebook I give a basic introduction to bigGans. I also, how to interpolate between z-vector values. Moreover, I show the 
results of multiple experiments I made in the latent space of BigGans. 
</p>

<h2 class = "subsec">Mask R-CNN</h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Mask_RCNN.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
In this notebook a pretrained Mask R-CNN model is used to predict the bounding box and the segmentation mask of objects. 
    I used this notebook to create the dataset for training the pix2pix model.
</p>

<h2 class = "subsec">QuickDraw Strokes </h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Strokes_QuickDraw.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
A notebook exploring the drawing data of quickdraw. I also illustrate how to make a cool animation of the drawing process in 
colab. 
</p>

<h2 class = "subsec">U-Net </h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/unet.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
The U-Net model is a simple fully convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. 
    In this notebook we use it to segment cats and dogs from arbitrary images.
</p>

<h2 class = "subsec">Localizer </h2>

<a href = "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Localizer.ipynb">
     <img  style = "max-width:20%" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" align="left">
</a>
<br>
<p>
A simple CNN with a regression branch to predict bounding box parameters. The model is trained on a dataset 
of dogs and cats with bounding box annotations around the head of the pets. 
</p>
<br>
<h1 class = "sec">Math Notes </h1>
<hr>
<h2 class = "subsec">Advanced Integration Techniques <span class = 'LINK'><a href = "https://goo.gl/C6A2tc">[LINK]</a></span></h2>
<p> A book exploring different approaches to solve advanced integrals. In this book I gather many special functions like, gamma, beta, zeta, 
    hypergeometric, etc .. and explore the analyticity of such functions. Also, I use complex analysis approaches to solve some advanced integrals
    that are unsolvable by elementary approaches. 
</p>

<img src = "imgs/adi.PNG"></img>

<h2 class = "subsec">Probability Notes <span class = 'LINK'><a href = "https://goo.gl/8bHFEU">[LINK]</a></span></h2>
<p> A summary of the probability course by MIT.  
</p>

<img src = "imgs/probability.PNG"></img>

<h1 class = "sec">Open Source Contribution </h1>
<hr>
<h2 class = "subsec">Tensors Rank 5 <span class = 'LINK'><a href = "https://github.com/tensorflow/tfjs-core/pull/1022">[LINK]</a></span></h2>
<p> WebGL coding to sport tensors rank 5 in Tensorflow.js. This opens the possiblity of for training, predicting models with 
    4 dimenional inputs. 
</p>


<h2 class = "subsec">Cropping2D Layer <span class = 'LINK'><a href = "https://github.com/tensorflow/tfjs-layers/pull/155">[LINK]</a></span></h2>
<p> This layer is very important to implement many models like auto-encoders.  
</p>

<h2 class = "subsec">ML Cheatsheet to Arabic <span class = 'LINK'><a href = "https://github.com/shervinea/cheatsheet-translation/pull/85">[LINK]</a></span></h2>
<p> I translated the linear algebra document of the cheatsheet by Stanford to Arabic.  
</p>

<h1 class = "sec">Public Talks</h1>
<hr>

<h2 class = "subsec">Deploying Machine Learning Models in the Browser using TensorFlow.js</h2>
<p> In this presentation at <a href = "https://twitter.com/DataSharqiyah">Sharqiah Data Geeks</a> I talk about depoloying 
    machine learning models in the browser with TensorFlow.js. I show a case study of recognizing drawings directly in the 
    browser. I also I show a couple of interesting projects that are done with TensorFlow.js. 
</p>

<center>
 <iframe width="420" height="315"
src="https://www.youtube.com/embed/CpT_ExT3qIg">
</iframe> 
</center>
</body>
</html>
