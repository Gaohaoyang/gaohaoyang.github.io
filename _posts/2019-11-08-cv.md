---
layout: post
title:  "计算机视觉-Computer Vision"
date:   2019-11-08 16:52:00
categories: 计算机视觉
tags: 深度学习 计算机视觉 GAN 扫地机器人 自动驾驶 何恺明 CVPR 论文 sota OCR 硕士论文 ocr opencv 数字图像 滤波
excerpt: 图像风格迁移是什么原理？具体如何实施？可以迁移到文本吗？
mathjax: true
permalink: /cv
---

* content
{:toc}


# 说明

- 【2022-8-25】生成人脸：[this-person-does-not-exist.com](https://this-person-does-not-exist.com/en)
- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, 文心·一格
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。
- 【2022-7-11】AI工具：[解你描述的东西还可以画出来的AI，真就全靠想象](https://www.ixigua.com/7117201327523892488?wid_try=1)
  - 一、[DALL•E2](https://openai.com/dall-e-2/)：openai出品，理解并画出你描述的所有东西，如：一直粉红色大象在撒哈拉沙漠玩扑克
  - 二、[AI智能图片放大](https://bigjpg.com/zh)：还原马赛克图片，就连表情包也能高清重置
  - 三、线稿自动上色：[Style2paints](http://paintstransfer.com/), [GitHub 链接](https://github.com/lllyasviel/style2paints)
    - 用 AI 技术为黑白线稿快速自动上色。在最近推出的 2.0 版中，研究人员使用了完全无监督的生成对抗网络（GAN）训练方法大幅提高了上色的准确性。Style2paints 的作者表示，该工具在精细度、漫画风格转换等方面超越了目前其他所有工具。
- 【2021-11-22】image2text的反向：text2image，NVIDIA Demo; 用深度学习模型(GauGAN)可以将文本转换成图片, demo，[GauGAN AI Art Demo](http://gaugan.org/gaugan2)，只要输入一句简短的描述就可以生成图片了。下图是他们演示的“海浪击打岩石”的效果。
  - [The absurd beauty of hacking Nvidia's GauGAN 2 AI image machine](https://www.zdnet.com/article/the-absurd-beauty-of-hacking-nvidias-gaugan-2-ai-image-machine/)
  - ![](https://p6.toutiaoimg.com/img/tos-cn-i-qvj2lq49k0/50832d2763094d32ab1c2e974d7a625a~tplv-obj:480:272.gif)
- 汇总计算机视觉的应用案例
- 【2021-3-26】[视频大脑：视频内容理解的技术详解和应用](https://www.infoq.cn/article/vhIXoD0CAqmojPHeKP5f/)，[极客时间视频](https://time.geekbang.org/dailylesson/detail/100022917),黄君实 奇虎360 人工智能研究院资深研发科学家
  - ![](https://static001.infoq.cn/resource/image/46/6a/46ccd482ad8a09752bca0a184aaca56a.png)

# sota

【2021-6-22】[CVPR 2021大奖公布！何恺明获最佳论文提名，代码已开源](https://mp.weixin.qq.com/s/sdboE0KmvCV-Zc2R6hs0Tg)

推特上，有学者打趣说，CV论文可以分为这几类：
- 「只想混文凭」
- 「教电脑生成更多猫的照片」
- 「ImageNet上实验结果提升0.1%！」
- 「手握超酷数据集但并不打算公开」
- 「3年过去了，代码仍在赶来的路上」
- 「实验证明还是老baseline性能更牛」
- 「我们的数据集更大！」
- 「研究范围超广，无他，我们有钱」
- 「花钱多，结果好」......

何恺明和Xinlei Chen的论文Exploring Simple Siamese Representation Learning（探索简单的连体表征学习）获得了最佳论文提名。

「连体网络」（Siamese networks）已经成为最近各种无监督视觉表征学习模型中的一种常见结构。这些模型最大限度地提高了一个图像的两个增量之间的相似性，但必须符合某些条件以避免collapse的解决方案。在本文中，我们报告了令人惊讶的经验结果，即简单的连体网络即使不使用以下任何一种情况也能学习有意义的表征。(i) 负样本对，(ii) 大batch，(iii) 动量编码器。我们的实验表明，对于损失和结构来说，collapse的解决方案确实存在，但stop-gradient操作在防止collapse方面发挥了重要作用。我们提供了一个关于stop-gradient含义的假设，并进一步展示了验证该假设的概念验证实验。我们的 「SimSiam 」方法在ImageNet和下游任务中取得了有竞争力的结果。我们希望这个简单的基线能促使人们重新思考连体结构在无监督表征学习中的作用。

代码已开源 https://github.com/facebookresearch/simsiam


## 何恺明编年史

![](https://pic1.zhimg.com/v2-5e022845e2440e673f98a11f99ac6dac_1440w.jpg?source=172ae18b)
- [何恺明：从高考状元到CV领域年轻翘楚，靠“去雾算法”成为“CVPR最佳论文”首位华人得主](https://zhuanlan.zhihu.com/p/55621213)

何恺明履历
- 出生于广州的何恺明是家中独子，父母均在企业里从事管理工作，从小就接触到优良的教学环境。实际上，能从众多学子中脱颖而出，除了教学环境之外，更多的是靠自己的努力。
- 何恺明年少时就被送到少年宫学习绘画，有时一待就是大半天，这也不断使他练就出沉稳的性格。同绘画一样，他对于文化课的钻研也十分耐得住性子，学习成绩优秀而且稳定。在老师的心目中，他是一个“性格比较内向”但是“目标明确”的学生，“从小就立志上清华”。
- 高中时，全国物理竞赛一等奖被保送进清华大学机械工程及其自动化专业，不去，偏要考，结果成了2003年广东理科状元；
- 大学期间，何恺明继续着自己沉稳而优秀的表现，不仅连续3年获得清华奖学金，2007年，还未毕业的他就进入了微软亚洲研究院（MSRA）实习。
- 本科毕业后，他进入香港中文大学攻读研究生，师从AI名人汤晓鸥；
- 2009年，第一篇论文“Single ImageHaze Removalusing Dark Channel Prior”被计算机视觉领域顶级会议CVPR接收并被评为年度最佳论文，CVPR创办25年来华人学者第一次获此殊荣，也使何恺明在CV领域声名鹊起
- 2011年，博士毕业的何恺明正式加入MSRA计算机视觉和深度学习的研究工作。
- 2015年的ImageNet图像识别大赛中，何恺明和他的团队凭借152层深度残差网络ResNet-152，击败谷歌、英特尔、高通等业界团队，荣获第一。目前ResNets也已经成为计算机视觉领域的流行架构，同时也被用于机器翻译、语音合成、语音识别和AlphaGo的研发上。
- 2016年，何恺明凭借ResNets论文再次获得CVPR最佳论文奖，也是目前少有的一人两次获得CVPR最佳论文奖的学者。
- 后来，何恺明和孙剑相继离开MSRA。与孙剑的选择不同，何凯明走得还是那条学院路。他选择了去Facebook，担任其人工智能实验室研究科学家，选择了进一步走学术之路。
- 2017年3月，何恺明和同事公布了其最新的研究Mask R-CNN，提出了一个概念上简单、灵活和通用的用于目标实例分割（object instance segmentation）框架，能够有效地检测图像中的目标，同时还能为每个实例生成一个高质量的分割掩码。同年，凭借《利用焦点损失提升物体检测效果》这篇论文，他一举夺下了另一个计算机视觉顶级会议ICCV最佳论文奖。
- 2018年，何恺明在美国盐湖城召开的CVPR上，获得了PAMI青年研究者奖。几个月前，何恺明等人发表论文称，ImageNet预训练却并非必须。何恺明和其同事使用随机初始化的模型，不借助外部数据就取得了不逊于COCO 2017冠军的结果，再次引发业内关注。


【2022-1-12】[何恺明编年史](https://zhuanlan.zhihu.com/p/415353143)

别人的荣誉都是在某某大厂工作，拿过什么大奖，而何恺明的荣誉是best，best，best ...... kaiming科研嗅觉顶级，每次都能精准的踩在最关键的问题上，提出的方法简洁明了，同时又蕴含着深刻的思考，文章赏心悦目，实验详尽扎实，工作质量说明一切。

何恺明的研究兴趣大致分成这么几个阶段：
- 传统视觉时代：Haze Removal(3篇)、Image Completion(2篇)、Image Warping(3篇)、Binary Encoding(6篇)
- 深度学习时代：Neural Architecture(11篇)、Object Detection(7篇)、Semantic Segmentation(11篇)、Video Understanding(4篇)、Self-Supervised(8篇)

代表作
- 2009 CVPR best paper Single Image Haze Removal Using Dark Channel Prior
  - 利用实验观察到的暗通道先验，巧妙的构造了图像**去雾算法**。现在主流的图像去雾算法还是在Dark Channel Prior的基础上做的改进。
- 2016 CVPR best paper Deep Residual Learning for Image Recognition
  - 通过**残差连接**，可以训练非常深的卷积神经网络。不管是之前的CNN，还是最近的ViT、MLP-Mixer架构，仍然摆脱不了残差连接的影响。
- 2017 ICCV best paper **Mask R-CNN**
  - 在Faster R-CNN的基础上，增加一个实例分割分支，并且将RoI Pooling替换成了RoI Align，使得实例分割精度大幅度提升。虽然最新的实例分割算法层出不穷，但是精度上依然难以超越Mask R-CNN。
  - ![](https://pic1.zhimg.com/80/v2-55b2b7227b553659dd7deea52082bef4_720w.jpg)
- 2017 ICCV best student paper Focal Loss for Dense Object Detection
  - 构建了一个**One-Stage**检测器RetinaNet，同时提出Focal Loss来处理One-Stage的类别不均衡问题，在目标检测任务上首次One-Stage检测器的速度和精度都优于Two-Stage检测器。近些年的One-Stage检测器(如FCOS、ATSS)，仍然以RetinaNet为基础进行改进。
  - ![](https://pic3.zhimg.com/80/v2-7628af32f42bc07197bdc27bc02f9d52_720w.jpg)
- 2020 CVPR Best Paper Nominee Momentum Contrast for Unsupervised Visual Representation Learning
  - 19年末，NLP领域的Transformer进一步应用于Unsupervised representation learning，产生后来影响深远的BERT和GPT系列模型，反观CV领域，ImageNet刷到饱和，似乎遇到了怎么也跨不过的屏障。就在CV领域停滞不前的时候，Kaiming He带着**MoCo**横空出世，横扫了包括PASCAL VOC和COCO在内的7大数据集，至此，CV拉开了Self-Supervised研究新篇章。


# 计算机视觉

## 什么是计算机视觉

What is computer vision?
- Computer vision is the field of computer science that focuses on creating **digital systems** that can process, analyze, and make sense of visual data (images or videos) in the same way that humans do. The concept of computer vision is based on teaching computers to process an image at a pixel level and understand it. Technically, machines attempt to retrieve visual information, handle it, and interpret results through special software algorithms.
- 向人脑一样处理、分析、感知视觉数据（图像/视频）的数字系统 [img](https://xd.adobe.com/ideas/wp-content/uploads/2020/07/what-is-computer-vision-how-does-it-work-1.png.webp)
- ![](https://xd.adobe.com/ideas/wp-content/uploads/2020/07/what-is-computer-vision-how-does-it-work-1.png.webp)

Here are a few common tasks that computer vision systems can be used for:
- `Object classification`. The system parses visual content and classifies the object on a photo/video to the defined category. For example, the system can find a dog among all objects in the image.
- `Object identification`. The system parses visual content and identifies a particular object on a photo/video. For example, the system can find a specific dog among the dogs in the image.
- `Object tracking`. The system processes video finds the object (or objects) that match search criteria and track its movement.

- ![image](https://xd.adobe.com/ideas/wp-content/uploads/2020/07/what-is-computer-vision-how-does-it-work-2.png.webp)
- ![video](https://lh5.googleusercontent.com/23ABhlO-DZ-8f2LGZ7mmu0VWASGDp9qRLp0efXiIqCmJL5BdAVBUWWy9UzcKWC701XB4s4XDMCtud7ZoRWE_I1D_hGyyk6OadSlqrP9Y-IA-HyRfKcQGwf1FNXj_DTvPmi0X0FUGeM09bc7E_OALgG7iR12W3pXsRLy-fLC4IJFa6_DCX0CBl81ncyFFfw)

## 人如何接收环境信息

[视觉、听觉、嗅觉、味觉和触觉：人体如何接收感觉信息](https://www.visiblebody.com/zh/learn/nervous/five-senses)
 
![五感示意图](https://www.visiblebody.com/hubfs/learn/assets/zh/nervous/2020-The%20Senses%20Chinese.jpg "五感示意图")
 
**神经系统**必须接收和处理**外界信息**以作出反应、进行通信并确保身体的健康与安全。
- 这些信息大部分来自感觉器官：`眼睛`、`耳朵`、`鼻子`、`舌头`和`皮肤`。
- 这些器官中的细胞和组织会接收原始刺激，并将其转化为神经系统可以使用的**信号**。神经将信号传递到大脑，大脑将其解释为**影像**（视觉）、**声音**（听觉）、**气味**（嗅觉）、**味道**（味觉）和**触感**（触觉）。

大脑必须依靠感觉器官来收集感觉讯息。相关的五种感觉器官是：
- 耳朵（听觉）
- 皮肤及毛发（触觉）
- 眼睛（视觉）
- 舌头（味觉）
- 鼻子（嗅觉

![](https://askthescientists.com/wp-content/uploads/2019/06/AdobeStock_244914939.jpg)
 
### 1. 眼睛将光线转化为供大脑处理的图像信号

![眼睛横截面示意图](https://www.visiblebody.com/hubfs/learn/assets/zh/nervous/2020-The%20Eye%20Chinese.jpg "眼睛横截面示意图")
 
眼睛位于头骨的眼眶中，受到骨骼和脂肪的保护。眼睛的白色部分是**巩膜**。它保护内部结构，包围着角膜、虹膜和瞳孔形成的圆形入口。角膜是透明的，允许光线进入眼睛，并发生弯曲来引导光线进入其后的瞳孔。瞳孔实际上是虹膜有色圆盘中的开孔。虹膜扩张或收缩，调节有多少光线通过瞳孔并进入晶状体。接下来，弯曲的晶状体将图像会在视网膜（眼的内层）上聚焦。视网膜是一层精细的神经组织膜，包含了感光细胞。这些视杆细胞和视锥细胞将光转化为神经信号。视神经将信号从眼睛传递到脑部，由脑将其解释为视觉图像。
 
### 2. 耳朵使用听小骨和液体将声波转换为声音信号
 
![单击查看描述听觉如何进行的动画描述](https://www.visiblebody.com/hubfs/Learn_Articles/Nervous_System/Hearing-Clip-Cover.jpg)
 
音乐、笑声、汽车鸣笛—都作为空气中的声波传入耳朵。外耳将声波沿着耳道（外耳道）汇聚到鼓膜（“耳鼓”）。声波敲击鼓膜，在膜中产生机械振动。鼓膜将这些振动传递到三块被称为听小骨的微小骨骼，可在充满空气的中耳腔室内看到它们。这些骨骼（锤骨、砧骨和镫骨）承载振动并敲击内耳的开口。内耳由充满液体的管道组成，包括螺旋形的耳蜗。随着听小骨的撞击，耳蜗中的特殊毛细胞会检测到液体中的压力波。它们激活神经感受器，通过蜗神经向脑发送信号，后者将信号解释为声音。
 
### 3. 皮肤中的特殊感受器向大脑发送触觉信号
 
![显示出触觉感受器的皮肤截面图](https://www.visiblebody.com/hubfs/learn/assets/zh/nervous/2020-Touch%20Receptors%20Chinese.jpg "显示出触觉感受器的皮肤截面图")
 
皮肤由三个主要组织层构成：外层表皮、中层真皮和内层皮下组织。

这些组织层中专门的感受器细胞检测触觉，并通过周围神经向脑部传递信号。不同类型感受器的存在和位置使特定身体部位更加敏感。嘴唇、手和外生殖器官下表皮可见Merkel细胞。迈斯纳小体可见于无毛发皮肤的上真皮层内—指甲、乳头、嘴唇、足底、阴蒂、龟头和舌尖。这两种感受器都可以检测到触摸、压力和振动。其他触觉感受器还包括环层小体，它也能记录压力和振动。还有能感受到疼痛、瘙痒和刺痒的特异性神经自由末梢。
 
### 4. 嗅觉：空气中的化学物质刺激发出被大脑解释为气味的信号。
 
![单击查看描述嗅觉如何进行的动画描述](https://www.visiblebody.com/hubfs/Learn_Articles/Nervous_System/Olfaction-Clip-Cover.jpg)
 
气味的感觉被称为嗅觉。它开始于鼻腔的顶部上皮中的毛状纤毛上的感受器。当我们用鼻子嗅和吸气时，一些空气中的化学物质会与感受器结合。这会触发一个顺着神经纤维向上传递的信号，穿过上皮和上方的颅骨到达嗅球。嗅球包含神经元细胞体，其将信号顺着延伸至嗅球的颅神经传递。然后将信号朝着大脑皮层的嗅觉区域向下传递到嗅神经。
 
### 5. 味蕾的载体：舌头是味觉的主要器官

舌头的四块内在肌协同工作，使舌头具有很大的灵活性。
- ![舌头和味蕾示意图](https://www.visiblebody.com/hubfs/learn/assets/zh/nervous/2020-Tongue%20and%20Taste%20Buds%20Chinese.jpg "舌头和味蕾示意图")
 
舌头上的那些小疙瘩是什么？它们叫做舌乳头。它们中包括轮廓乳头和菌状乳头在内的多数都包含味蕾。当我们进食时，食物中的化学物质会进入舌乳头并到达味蕾。这些化学物质（或促味剂）刺激味蕾内部特殊的味觉细胞，激活神经感受器。感受器将信号发送至面神经、舌咽神经和迷走神经的神经纤维。这些神经将信号传递到延髓，延髓将信号传递到脑部的丘脑和大脑皮层。


## 教程

【2022-12-28】
- 华盛顿大学，[Computer Vision: Algorithms and Applications, 2nd](http://szeliski.org/Book/)
- CMU计算机视觉课程：[16-385 Computer Vision, Spring 2020](http://www.cs.cmu.edu/~16385/), 包含ppt
  - Course Description
  - This course provides a comprehensive introduction to computer vision. Major topics include image processing, detection and recognition, geometry-based and physics-based vision and video analysis. Students will learn basic concepts of computer vision as well as hands on experience to solve real-life vision problems.
- 斯坦福cs 231, CS231n 是顶级院校斯坦福出品的深度学习与计算机视觉方向专业课程，核心内容覆盖神经网络、CNN、图像识别、RNN、神经网络训练、注意力机制、生成模型、目标检测、图像分割等内容。
  - 解读：[深度学习与CV教程](https://www.showmeai.tech/tutorials/37)

目录
- CV引言与基础
- 图像分类与机器学习基础
- 损失函数与最优化
- 神经网络与反向传播
- 卷积神经网络
- 神经网络训练技巧 (上)
- 神经网络训练技巧 (下)
- 常见深度学习框架介绍
- 典型CNN架构 (Alexnet, VGG, Googlenet, Restnet等)
- 轻量化CNN架构 (SqueezeNet, ShuffleNet, MobileNet等)
- 循环神经网络及视觉应用
- 目标检测 (两阶段, R-CNN系列)
- 目标检测 (SSD, YOLO系列)
- 图像分割 (FCN, SegNet, U-Net, PSPNet, DeepLab, RefineNet)
- 视觉模型可视化与可解释性
- 生成模型 (PixelRNN, PixelCNN, VAE, GAN)
- 深度强化学习 (马尔可夫决策过程, Q-Learning, DQN)
- 深度强化学习 (梯度策略, Actor-Critic, DDPG, A3C)

## 基础任务

基本任务
- （1）`图像分类`：给定一张输入图像，图像分类任务旨在判断该图像所属类别。
- （2）`目标定位`：以包围框的(bounding box)形式得到图像类别位置，通常只有一类目标或固定数目的目标和背景类。
- （3）`目标检测`：相对于目标定位，目标种类和数目不定。
  - 详见专题文章：[目标检测](object-detection)
- （4）`图像分割`
  - `语义分割`：语义分割需要判断图像中哪些像素属于哪个目标。
  - `实例分割`：语义分割不区分属于相同类别的不同实例。

例如，当图像中有多只猫时
- `语义分割`会将两只猫整体的所有像素预测为“猫”这个类别。
- `实例分割`需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/08f68c43e558443c8d2d10dde6ea91be~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

难点
- 语义鸿沟，拍摄视角变化，目标占据图像的比例变化，光照变化，背景相似，目标形变，遮挡。

作者：[离殇花开](https://juejin.cn/post/7023196910957953032)


## 应用

核心应用和关键知识点
- 基础部分：**图像安全**（加密、解密、信息隐藏、隐身）、**图像识别**（答题卡、手势、车牌、指纹、数字）、**物体计数**、图像**检索**、次品排查等。
- 机器学习：KNN**字符识别**（数字、字母）、数独求解（KNN）、SVM**数字识别**、**行人检测**、艺术画（K均值聚类）等。
- 深度学习：图像**分类**、**目标检测**（YOLO、SSD方法）、语义分割、实例分割、**风格迁移**、**姿势识别**等。
- 人脸相关：人脸检测、**人脸识别**、勾勒五官、人脸对齐、**表情识别**、**疲劳驾驶**检测、易容术、性别与年龄识别等。

案例：
- [计算机视觉40例](https://zhuanlan.zhihu.com/p/549251554)
- 商汤科技[技术演示](https://www.sensetime.com/cn/technology-detail?categoryId=43&gioNav=1)，覆盖图像、视频

### 特征识别

特征匹配
- ![](https://pic1.zhimg.com/80/v2-1b53412ec3736ef215799825dcc999bc_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-82ff0cae618329fdb09dd84dfd671db1_1440w.webp)


### 图像处理

图像滤波
- ![](https://pic3.zhimg.com/80/v2-935991f18dd8d5b3b6988e09860e491a_1440w.webp)

风格迁移
- ![](https://pic3.zhimg.com/80/v2-22bda1142ff108bd4bd15bef7f3993c2_1440w.webp)

### 图像识别

数字识别
- ![](https://pic2.zhimg.com/80/v2-49554e752e9ffeb8fb61c60462fa813d_1440w.webp)

人脸识别
- ![](https://pic4.zhimg.com/80/v2-1637c2414c72ca3fd8434869c3b4f623_1440w.webp)

属性识别（年龄、性别）
- ![](https://pic4.zhimg.com/80/v2-35af17b3c50063b36f9d9e465c9fd397_1440w.webp)

手势识别
- ![](https://pic3.zhimg.com/80/v2-1c844e2e524f07f26135b25c6eb71426_1440w.webp)

缺陷检测
- ![](https://pic2.zhimg.com/80/v2-3ea564958b58f32661c206127e64d67d_1440w.webp)

车牌识别
- ![](https://pic4.zhimg.com/80/v2-834582c9b73b823bedee7908c4c14ecf_1440w.webp)

疲劳驾驶检测
- ![](https://pic1.zhimg.com/80/v2-829a72902cc5dd8a03aaea0ec32bb278_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-772eafa621aaa7a68c070e01f388c2b1_1440w.webp)

### 目标检测

行人检测
- ![](https://pic3.zhimg.com/80/v2-715909ee83eed755d72ea3f3728761d6_1440w.webp)
- ![](https://tryolabs.com/assets/guides/introductory-guide-computer-vision/object-detection-cars-48e1a4793f.gif)

目标计数
- ![](https://pic1.zhimg.com/80/v2-43204d761de013ce83f35f3c72c2d7fc_1440w.webp)

姿势识别
- ![](https://pic3.zhimg.com/80/v2-dd79a5708fde72afda4ebe6fd73eb512_1440w.webp)

### 图像分割

语义分割与实例分割
- ![img](https://pic2.zhimg.com/80/v2-09db83410da4affd809a93c908fa955d_1440w.webp)

### 信息安全

数字水印
- ![](https://pic3.zhimg.com/80/v2-f122c8ab40e528066a3c42b34375380a_1440w.webp)

隐身术
- ![](https://pic1.zhimg.com/80/v2-245e8ff807f04f51a742c62573a558b8_1440w.webp)

### 图像检索

图像哈希
- ![](https://pic3.zhimg.com/80/v2-cfff2d6ad18088ddac3f76270ee7d9f2_1440w.webp)


## 图像处理-基础知识

《数字图像处理》，冈萨雷斯第三版，MATLAB实践
- 《数字图像处理》冈萨雷斯（第四版）[读书笔记目录](https://zhuanlan.zhihu.com/p/569167720)
- [北大ppt课件](https://wenku.baidu.com/view/055297b327fff705cc1755270722192e45365883.html?_wkts_=1672129716473), 百度文库版
- github上的[课件资料:数字图像处理](https://github.com/fei-hdu/courses/tree/main/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/2022), 杭州电子科技大学，[高飞](https://aiart.live/)
- 计算机视觉[ppt资料](https://github.com/fei-hdu/courses/tree/main/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89)


目录
- 第一章 概述
- 第二章 图像处理知识点全面整理
- 第三章 灰度变换与空间滤波知识点整理
- 第四章 频率域图像处理知识点整理
- 第五章 图像复原与重建知识点整理
- 第六章 彩色图像处理基础

### 灰度图

图像通常分为彩色图像和灰度图像两种。
- 在灰度图像中，每个像素都只有**一个**分量用来表示该像素的灰度值。这个分量就是该点的亮度值。
  - 常用的表示像素值所需的数据长度有8 bit或10 bit两种，即图像的位深为8 bit（256）或10 bit（1024）。
- 二值图
  - 灰度图有过度，二值图没有过渡，只有两种0(黑)、1(白)
- 在彩色图像中，每个像素都由**多个**颜色分量组成；每个颜色分量被称为一个通道（Channel）。
  - 图像中所有像素的通道数是一致的，即每个通道都可以表示为一幅与原图像内容相同但颜色不同的分量图像。
  - 以RGB格式的彩色图像为例，一幅完整的图像可以被分割为蓝（B分量）、绿（G分量）、红（R分量）三基色的单色图
  - ![](https://pic2.zhimg.com/80/v2-55fa93deb68f2b99370953a3ea7b8ea1_720w.webp)

RGB图像的宽、高为1920 像素×1080 像素，每个颜色通道的图像位深为 8 bit，则图像的数据体积为1920×1080×3×8bit，即49,766,400bit，约为5.93MB左右。

注： 
- ② 1 Byte＝8bit（位）。
- ③ 1KB＝1024Byte（字节）
- ④ 1 MB＝1024KB

![](https://pic4.zhimg.com/80/v2-627bf6d9d2f637442d2346958e67420b_720w.webp)

### 颜色空间

![](https://pic2.zhimg.com/80/v2-12c573c72e81a5be358de22a5d683675_720w.webp)

`颜色空间`（彩色模型、色彩空间、 彩色系统etc）是对色彩的一种描述方式，定义有很多种，区别在于面向不同的应用背景。
- 显示器中采用的`RGB`颜色空间是基于**物体发光**定义的（RGB正好对应光的三原色：Red，Green，Blue）；
- 工业印刷中常用的`CMY`颜色空间是基于**光反射**定义的（CMY对应了绘画中的三原色：Cyan，Magenta，Yellow）；
- `HSV`、`HSL`两个颜色空间都是从**人视觉**的直观反映而提出来的（H是色调，S是饱和度，I是强度）。

#### RGB（加法混色）

RGB颜色空间 基于颜色的**加法混色**原理，从黑色不断叠加Red，Green，Blue的颜色，最终可以得到白色光。 
- 将R、G、B三个通道作为笛卡尔坐标系中的X、Y、Z轴，就得到了一种对于颜色的空间描述
- ![](https://i0.hdslb.com/bfs/article/347f4ac7d52d301eb2560fc24c2394d99abad0e5.png@675w_635h_progressive.webp)

计算机中编程RGB每一个分量值都用8位（bit）表示，可以产生256*256*256=16777216中颜色，这就是经常所说的“24位真彩色”。 

作者：[unity_某某师_高锦锦](https://www.bilibili.com/read/cv5841645/)

#### CMY（减法混色）

相比于`RGB`，`CMY`（CMYK）颜色空间是另一种基于颜色**减法混色**原理的颜色模型。
- 在工业印刷中它描述的是需要在白色介质上使用何种油墨，通过光的**反射**显示出颜色的模型。
- CMYK描述的是`青`，`品红`，`黄`和`黑`四种油墨的数值。
- ![](https://i0.hdslb.com/bfs/article/20eefed4bae71f1f107b4519b029395d9dca9d93.png@300w_285h_progressive.webp)

打印机彩打时，会发现屏幕上看见的图像和实际打印出来的图像颜色不一样。
- 体现了肉眼的量化能力很好
- 打印机采用了不同的CMY颜色空间（映射关系）

CMYK颜色空间的颜色值与RGB颜色空间中的取值可以通过线性变换相互转换。

#### HSV（人视觉）

`HSV`颜色空间是根据颜色的直观特性, 由A. R. Smith在1978年创建的一种颜色空间, 也称`六角锥体模型`(Hexcone Model)。
- `RGB`和`CMY`颜色模型都是面向**硬件**的
- 而`HSV`（Hue Saturation Value）颜色模型是面向**用户**的。

这个模型中颜色的参数分别是：`色调`（H：hue），`饱和度`（S：saturation），`亮度`（V：value）。
- 这是根据人观察色彩的生理特征而提出的颜色模型
- 人的视觉系统对亮度的敏感度要强于色彩值，这也是为什么计算机视觉中通常使用灰度即亮度图像来处理的原因之一。

- 色调H：用角度度量，取值范围为0°～360°，从红色开始按逆时针方向计算，红色为0°，绿色为120°,蓝色为240°。它们的补色是：黄色为60°，青色为180°,品红为300°；
- 饱和度S：取值范围为0.0～1.0；
- 亮度V：取值范围为0.0(黑色)～1.0(白色)。

HSV模型的三维表示从RGB立方体演化而来。设想从RGB沿立方体对角线的白色顶点向黑色顶点观察，就可以看到立方体的六边形外形。六边形边界表示色彩，水平轴表示纯度，明度沿垂直轴测量。与加法减法混色的术语相比，使用色相，饱和度等概念描述色彩更自然直观。
- ![](https://i0.hdslb.com/bfs/article/e9e72c7ba0963c50c0d6b5cbe9b4bdf0a2681d72.png@942w_942h_progressive.webp)

HSL颜色空间与HSV类似，只不过把V：Value替换为了L：Lightness。这两种表示在用目的上类似，但在方法上有区别。二者在数学上都是圆柱，但HSV（色相，饱和度，色调）在概念上可以被认为是颜色的倒圆锥体（黑点在下顶点，白色在上底面圆心），HSL在概念上表示了一个双圆锥体和圆球体（白色在上顶点，黑色在下顶点，最大横切面的圆心是半程灰色）。注意尽管在HSL和HSV中“色相”指称相同的性质，它们的“饱和度”的定义是明显不同的。对于一些人，HSL更好的反映了“饱和度”和“亮度”作为两个独立参数的直觉观念，但是对于另一些人，它的饱和度定义是错误的，因为非常柔和的几乎白色的颜色在HSL可以被定义为是完全饱和的。对于HSV还是HSL更适合于人类用户界面是有争议的。 作者：unity_某某师_高锦锦 https://www.bilibili.com/read/cv5841645/ 出处：bilibili

#### 颜色空间转换

颜色空间之间的转换关系可以分类两类：
- 一类是颜色空间之间可以直接变换；
- 另一类是颜色空间之间不能直接转换，它们之间的变换需要通过借助其他颜色空间的过渡来实现，如，RGB和CIE L*a*b*。
- ![](https://pic2.zhimg.com/80/v2-80732f19238cdd34cbd04ac0f63e0cb9_720w.webp)

### 图像表示

[数字图像与模拟图像](https://zhuanlan.zhihu.com/p/252635549)

#### 模拟图像

（1）模拟图像
- `模拟图像`：在图像处理中，像纸质照片、电视模拟图像等，这种通过某种物理量（如光、电等）的强弱变化来记录图像亮度信息的图像。
- 特点：物理量的变化是**连续**的。

#### 数字图像

（2）数字图像
- `数字图像`：用一个数字阵列来表达客观物体的图像，离散采样点的集合，每个点具有其各自的属性。
- 特点：把**连续**模拟图像离散化成规则网格，并用计算机以数字的方式来记录图像上各网格点的亮度信息的图像。

小结：
- 一切肉眼能看见的，都是模拟图像（投影仪透出到幕布上的PPT也是模拟图像）。
- 而数字图像肉眼看不见，本质就是一个存储数字的矩阵，一团数据。

数码相机屏幕上呈现的图像是模拟图像。但是你看见的图像是你的相机把经过了光学透镜的模拟图像（光信号），先转化为电信号存储为你内存卡上的一个图片文件（模数转换），即是一个数字图像（在内存里存着，你看不见），然后经过数字图像上记录的信息经过相应颜色空间的映射（数模转换），最后又以光信号的方式呈现在相机的监视屏幕上（又变成模拟图像了）。

数字图像呈现在现实世界中（不论黑白/彩色）时，这个图像究竟是数字图像还是模拟图像？答案当然是模拟图像

#### 模数转换 or 数模转换

数字图像与模拟图像转换
- ![img](https://pic1.zhimg.com/80/v2-35da1f8acba8c338f5ff7afba5acebe8_720w.webp)
- 数字相机经过了模数转换的过程之后，实际是将现实世界的连续的以物理量（比如光）呈现的图像以一定的分辨率（像素）离散化了

#### 遥感图像

遥感图像都是有着一定的大小的，比如3000像素×5000像素。而空间分辨率即是指图片中的每一个像素代表的实际空间大小，比如250m×250m。


### 图像处理-基本步骤

图像处理[基本步骤](https://zhuanlan.zhihu.com/p/558711657)
- （1）图像获取
- （2）图像**增强**，对图像进行某种操作，使其在某种应用中比原图像更能得到合适的结果。
- （3）图像**复原**，改进图像外观，图像增强技术是主观的，图像复原是客观的。复原技术倾向于以图像退化的数学或概率模型为基础，而增强技术以好的增强效果这种主观偏好为基础
- （4）**彩色图像**处理
- （5）**小波变换**和其他**图像变换**，小波是以不同分辨率来表示图像的基础，小波变换可应用与图像压缩和金字塔特征表示。
- （6）**压缩**是指减少图像存储量或降低传输图像的带宽的处理。
- （7）**形态学**处理是提取图像中用于表示和描述形状的成分处理工具。
- （8）**分割**可以将一幅图像或分为各个组成部分或目标。
- （9）**特征提取**，在分割后一步进行，从分割中获取组成图像区域的像素的集合。特征提取包括特征检测和特征描述。特征检测是指寻找一幅图像中的特征、区域或边界。特征描述是指对检测到的特征规定量化属性。
- (10) 图像**模式分类**是指根据目标特征描述对子目标属于标记的过程。

总结：[img](https://pic4.zhimg.com/80/v2-2c50e078dc083216a5ea78338cb2a073_1440w.webp), [中文](https://pic1.zhimg.com/80/v2-17417fabe9df8bf82f62e1bd26921780_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-2c50e078dc083216a5ea78338cb2a073_1440w.webp)
- ![](https://pic1.zhimg.com/80/v2-17417fabe9df8bf82f62e1bd26921780_1440w.webp)


### 图像处理-知识点

数字图像处理
1. 模拟图像和数字图像
  - `模拟图像`指空间坐标和亮度都是连续变化
  - `数字图像`是一种空间坐标和灰度均不连续，用离散数字表示的图像
2. 数字图像处理系统组成
  - `采集`、`显示`、`存储`、`通信`、`图像处理与分析`五个模块
3. 相比模拟图像处理，数字图像处理的优点
  - 精度高，再现性好，灵活性、通用性强
4. 图像数字化步骤：`采样` -> `量化`
  - `采样`：将空间上连续的图像变换成离散点。圆，椭圆，正方，长方。采样间隔大，像素少，空间分辨率低。
  - `量化`：将像素灰度变换成离散的整数值。量化等级越多，层次越丰富，灰度分辨率越高，但数据量大。
5. `灰度直方图`：一幅图像中各灰度等级像素出现的频率之间的关系
  - 反应图像灰度的分布情况，不反映像素位置，
  - 应用：判断图像量化是否恰当，确定图像**二值化**阀值，计算图像中物体面积，
6. 图像变换目的：使图像处理问题简化，有利于图像特征提取，有利于从概念上增强对图像信息的理解
7. 图像变换算法：`二维傅立叶`变换，`沃尔什—哈达玛`变换，`哈尔`变换，`小波`变换
  - 二维傅立叶后的频谱：四角低频（反映大概样貌），中央高频（反应细节）
8. 图像增强不是以图像保真度为原则，而是通过处理设法有选择地突出，便于人或机器分析某些感兴趣的信息，抑制一些无用信息。
9. 图像增强方法可分为`空间域`（直接对像素灰度操作）和`频域`（对图像经傅立叶变换后的频谱操作）
  - `空间域`：
    - 灰度级校正，灰度变换，直方图修正法；
    - 局部平滑，最大均匀性平滑；
    - 阶梯锐化，Laplacian增强算子
  - `频域`： 
    - `平滑`：**低通**滤波器
    - `锐化`：**高通**滤波器
  - `彩色增强`：
    - 伪彩色增强：密度分割 
    - 彩色图像增强：彩色平衡
10. 图像复原与重建：图像增强可以不顾增强后是否失真，图像复原是要恢复原形
11. 图像恢复方法：
  - 1. 代数恢复：无约束复原 
  - 2. 频率域恢复：逆滤波恢复法 ，维纳滤波器
  - 3. 几何校正：像素灰度内插（最近邻元法，双线性内插）
12. 几种图像退化：模糊、失真、有噪声等
13. 保真度准则：描述解码图像相对原始图像偏离程度的测度：客观保真度准则+主观保真度准则
14. 图像压缩技术：有损压缩和无损压缩
  - `无损`压缩：霍夫曼编码，香农编码，算数编码
  - `有损`压缩：预测编码（线性预测和非线性预测），变换编码
15. 无损和有损预测编码算法不同之处？
  - 无损编码中删除的仅仅是图像数据中冗余的数据，经解码重建的图像没有任何失真。
  - 有损编码是指解码重建的图像与原图像相比有失真，不能精确的复原，但视觉效果上基本相同，是实现高压缩比的编码方式。
16. `图像分割`是指把图像分成互不重叠的区域并提取出感兴趣目标的技术
17. 按分割途径不同可以分为：
  - 1. 基于边缘提取的分割算法
  - 2. 区域分割算法 
  - 3. 区域增长（简单区域扩张法、质心形增长）
  - 4. 分裂—合并分割算法
18. 几个常用的边缘检测算子：
  - Canny 算子
  - Laplacian算子
  - Sobel 算子
19. Hough变换检测直线和圆算法

## 硕士论文

【2022-01-24】
- 硕士毕业论文：[图像分割配准技术在小鼠舌头三维重建中的应用](https://www.docin.com/p-1966389845.html)
- 计算机工程学报：[基于形态学的小鼠舌头切片图像分割与实现](https://jz.docin.com/p-533278994.html)

## 图像处理内容

包含
- · 基本概念：亮度、对比度、分辨率、饱和度、尖锐化等基础概念
- · 图像灰度变换：线性、分段线性、对数、反对数、幂律(伽马)变换等
- · 图像滤波：线性滤波和非线性滤波、空间滤波和频率域滤波，均值滤波、中值滤波、高斯滤波、逆滤波、维纳滤波等各种图像的基本操作

高级的图像操作:
- · 文本图像的倾斜矫正方法：霍夫变换、透视变换等
- · 图像边缘检测：canny算子、sobel算子、Laplace算子、Scharr滤波器等


## 图像分割

- 【2020-7-17】图像分割（Image Segmentation）是计算机视觉领域中的一项重要基础技术，是图像理解中的重要一环。图像分割是将数字图像细分为多个图像子区域的过程，通过简化或改变图像的表示形式，让图像能够更加容易被理解。
   - 图像分割技术自 60 年代数字图像处理诞生开始便有了研究，随着近年来深度学习研究的逐步深入，图像分割技术也随之有了巨大的发展。
   - 早期的图像分割算法不能很好地分割一些具有抽象语义的目标，比如文字、动物、行人、车辆。这是因为早期的图像分割算法基于简单的像素值或一些低层的特征，如边缘、纹理等，人工设计的一些描述很难准确描述这些语义，这一经典问题被称之为“语义鸿沟”。
   - 第三代图像分割很好地避免了人工设计特征带来的“语义鸿沟”，从最初只能基于像素值以及低层特征进行分割，到现在能够完成一些根据高层语义的分割需求。
   - 参考：[基于深度学习的图像分割在高德的实践](https://yqh.aliyun.com/detail/15920?utm_content=g_1000154176)
   - ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/9811c9fff31a4fe282dbce591f7642b8~tplv-obj:745:306.image)

## 图片小工具

【2022-10-19】[神奇海螺试验场](https://lab.magiconch.com/)出品
- [电子包浆](https://magiconch.com/patina/)，图片赛博做旧
- [梗图生成器](https://x.magiconch.com/)
- [颜色图片识别](https://magiconch.com/nsfw/)
- 切图：[九宫格](https://v.magiconch.com/sns-image)
- [你画我猜](https://draw.magiconch.com/)在线游戏


## 图像动态化

[Live2D](https://www.live2d.com/en/download/cubism/)

Live2D是一种应用于电子游戏的绘图渲染技术，通过一系列的连续图像和人物建模来生成一种类似二维图像的三维模型。对于以动画风格为主的冒险游戏来说非常有用。该技术由日本Guyzware公司开发，Live2D的前身为TORA系统，衍生技术是OIU系统。
- 知乎：[如何看待live2D这项技术？](https://www.zhihu.com/question/28130936)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://vdn.vzuu.com/SD/fc42fe58-2322-11eb-a20b-9a794694b530.mp4" type="video/mp4" />
</video>


## 图像3D化

- [2017-9-21]自拍照三维重建[3D Face Reconstruction from a Single Image](http://www.cs.nott.ac.uk/~psxasj/3dme/index.php)
- ![demo](https://cdn.vox-cdn.com/thumbor/fXbE0rbXW6WlcmtB1cKBiTsV1b0=/0x0:482x334/1820x1213/filters:focal(203x129:279x205):no_upscale()/cdn.vox-cdn.com/uploads/chorus_image/image/56734861/3d_mark_take_2.0.gif)
- 【2020-7-23】2D照片转3D的效果，代码：[3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting)
- ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/54a7f500dc92415f91e0766e2f74c45a~tplv-obj:340:424.image?from=post)

- 【2020-11-18】端到端面部表情合成 Speech-Driven Animation [Github代码](https://github.com/DinoMan/speech-driven-animation)
   - ![](https://github.com/DinoMan/speech-driven-animation/raw/master/example.gif)
- 【2021-3-10】面部表情迁移：吴京+甄子丹 [微博示例](https://video.weibo.com/show?fid=1034:4609199536013325)
- 【2020-12-29】[单张图片三维重建](https://blog.csdn.net/zouxy09/article/details/8083553),Andrew Ng介绍他的两个学生用单幅图像去重构这个场景的三维模型。
   - [斯坦福大学](http://ai.stanford.edu/~asaxena/reconstruction3d/)
      - ![](http://ai.stanford.edu/~asaxena/reconstruction3d/Results/mountain_mesh_small.jpg)
   - [康奈尔大学](http://www.cs.cornell.edu/~asaxena/learningdepth/)

## opencv

【2022-10-7】[opencv-python快速入门篇](https://zhuanlan.zhihu.com/p/44255577)

### opencv简介

opencv 是用于快速处理图像处理、计算机视觉问题的工具，支持多种语言进行开发如c++、python、java等

### Python opencv安装

环境：
- 1、 python3
- 2、 numpy
- 3、 opencv-python

```shell
# 安装numpy
pip install numpy
# 安装opencv-python
pip install opencv-python
```

测试：
- 执行 import cv2

### 图像读取

（1）imread函数：读取数字图像

cv2.imread(path_of_image, intflag)
- 参数一： 需要读入图像的完整路径
- 参数二： 标志以什么形式读入图像，可以选择一下方式：
  - · cv2.IMREAD_COLOR： 加载彩色图像。任何图像的透明度都将被忽略。它是默认标志
  - · cv2.IMREAD_GRAYSCALE：以灰度模式加载图像
  - · cv2.IMREAD_UNCHANGED：保留读取图片原有的颜色通道
    - · 1 ：等同于cv2.IMREAD_COLOR
    - · 0 ：等同于cv2.IMREAD_GRAYSCALE
    - · -1 ：等同于cv2.IMREAD_UNCHANGED

### 图像显示

（2）imshow 函数
- imshow函数作用是在窗口中显示图像，窗口自动适合于图像大小，我们也可以通过imutils模块调整显示图像的窗口的大小。
- 函数官方定义：cv2.imshow(windows_name, image)
  - 参数一： 窗口名称(字符串)
  - 参数二： 图像对象，类型是numpy中的ndarray类型，注：这里可以通过imutils模块改变图像显示大小

### 图像写入

（3）imwrite 函数
- imwrite函数检图像保存到本地，官方定义：cv2.imwrite(image_filename, image)
  - 参数一： 保存的图像名称(字符串)
  - 参数二： 图像对象，类型是numpy中的ndarray类型


### 颜色空间

图像颜色主要是由于图像受到外界光照影响随之产生的不同颜色信息，同一个背景物的图像在不同光源照射下产生的不同颜色效果的图像，因此在做图像特征提取和识别过程时，要的是图像的**梯度信息**，也就是图像的本质内容，而**颜色信息**会对梯度信息提取造成一定的干扰，因此会在做图像特征提取和识别前将图像转化为**灰度图**，这样同时也降低了处理的数据量并且增强了处理效果。

图像色彩空间变换函数cv2.cvtColor

函数定义：cv2.cvtColor(input_image, flag)
- 参数一： input_image表示将要变换色彩的图像ndarray对象
- 参数二： 表示图像色彩空间变换的类型，以下介绍常用的两种：
  - · cv2.COLOR_BGR2GRAY： 表示将图像从BGR空间转化成灰度图，最常用
  - · cv2.COLOR_BGR2HSV： 表示将图像从RGB空间转换到HSV空间

如果想查看参数flag的全部类型，请执行以下程序便可查阅，总共有274种空间转换类型：

```python
import cv2
flags = [i for i in dir(cv2) if i.startswith('COLOR_')]
print(flags)
```

### 自定义图像

绘图简单图像
- 对于一个长宽分别为w、h的RGB彩色图像来说，每个像素值是由(B、G、R)的一个tuple组成，opencv-python 中每个像素三个值的顺序是B、G、R，而对于灰度图像来说，每个像素对应的便只是一个整数，如果要把像素缩放到0、1，则灰度图像就是二值图像，0便是黑色，1便是白色

```python
import cv2
#这里图像采用的仍旧是上面那个卡通美女啦
rgb_img = cv2.imread('E:/peking_rw/ocr_project/base_prehandle/img/cartoon.jpg')
print(rgb_img.shape)     #(1200, 1600, 3)
print(rgb_img[0, 0])     #[137 124  38]
print(rgb_img[0, 0, 0])  #137

gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
print(gray_img.shape)    #(1200, 1600)
print(gray_img[0, 0])    #100
```

彩色图像的高度height = 1200， 宽度w=1600且通道数为3， 像素(0， 0)的值是(137 124 38)，即R=137, G=124, B=38， 对于灰度图像来说便只是单通道的了

因此(0, 0, 0)便是代表一个黑色像素，(255, 255, 255)便是代表一个白色像素。这么想，B=0, G=0, R=0相当于关闭了颜色通道也就相当于无光照进入，所以图像整个是黑的，而(255, 255, 255)即B=255, G=255, R=255， 相当于打开了B、G、R所有通道光线全部进入，因此便是白色。

#### 图像绘制方法

各种绘制方法
- 直线cv2.line、长方形cv2.rectangle、圆cv2.circle、椭圆cv2.ellipse、多边形cv2.polylines等集合图像绘制函数

公共参数：
- · img： 表示需要进行绘制的图像对象ndarray
- · color： 表示绘制几何图形的颜色，采用BGR即上述说的(B、G、R)
- · thickness： 表示绘制几何图形中线的粗细，默认为1，对于圆、椭圆等封闭图像取-1时是填充图形内部
- · lineType ： 表示绘制几何图形线的类型，默认8-connected线是光滑的，当取cv2.LINE_AA时线呈现锯齿状

##### (1) cv2.line函数

直线绘制函数， 函数官方定义为：
- cv2.line(image, starting, ending, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数starting、ending分别表示线的起点像素坐标、终点像素坐标

##### (2) cv2.rectangle函数

长方形绘制函数，函数官方定义：
- cv2.rectangle(image, top-left, bottom-right, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数top-left、bottom-right分别表示长方形的左上角像素坐标、右下角像素坐标


##### (3) cv2.circle函数
圆形绘制函数，官方定义函数为：
- cv2.circle(image, center, radius, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数center、radius分别表示圆的圆心像素坐标、圆的半径长度，圆绘制函数中当参数thickness = -1 时绘制的是实心圆，当thickness >= 0 时绘制的是空心圆


##### (4) cv2.ellipse函数

椭圆绘制函数，官方定义为：
- cv2.circle(image, center, (major-axis-length, minor-axis-length), angle, startAngle, endAngle, color, thickness, lineType)
- 椭圆的参数较多，首先参数image、color、thickness、lineType分别是上述公共定义，椭圆绘制函数中当参数thickness = -1 时绘制的是实心椭圆，当thickness >= 0 时绘制的是空心椭圆，其他参数如下
  - · center： 表示椭圆中心像素坐标
  - · major-axis-length： 表示椭圆的长轴长度
  - · minor-axis-length： 表示椭圆的短轴长度
  - · angle： 表示椭圆在逆时针方向旋转的角度
  - · startAngle： 表示椭圆从主轴向顺时针方向测量的椭圆弧的起始角度
  - · endAngle： 表示椭圆从主轴向顺时针方向测量的椭圆弧的终止时角度


##### (5) cv2.polylines函数

多边形绘制函数，官方定义函数为：
- cv2.polylines(image, \[point-set], flag, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，其他参数如下：
  - · \[point-set]： 表示多边形点的集合，如果多边形有m个点，则便是一个m*1*2的数组，表示共m个点
  - · flag： 当flag = True 时，则多边形是封闭的，当flag = False 时，则多边形只是从第一个到最后一个点连线组成的图像，没有封闭


#### 图像绘制示例

```python
import cv2
import numpy as np

img = np.ones((512,512,3), np.uint8)
img = 255*img
img = cv2.line(img, (100,100), (400,400),(255, 0, 0), 5)
img = cv2.rectangle(img,(200, 20),(400,120),(0,255,0),3)
img = cv2.circle(img,(100,400), 50, (0,0,255), 2)
img = cv2.circle(img,(250,400), 50, (0,0,255), 0)
img = cv2.ellipse(img,(256,256),(100,50),0,0,180,(0, 255, 255), -1)
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
img = cv2.polylines(img,[pts],True,(0, 0, 0), 2)

cv2.imshow('img', img)
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
```

![](https://pic3.zhimg.com/80/v2-37c3e0653291eafc7d16ce071fdf9db6_1440w.webp)

### 像素操作

#### (1) 对图像取反

```python
reverse_img = 255 - gray_img
```

#### (2) 对图像像素线性变换

```python
for i in range(gray_img.shape[0]):
    for j in range(gray_img.shape[1]):
        random_img[i, j] = gray_img[i, j]*1.2
```

![](https://pic4.zhimg.com/80/v2-8fca4ea068a45033056e89236ae1644b_1440w.webp)

完整代码

```python
import cv2
import imutils
import numpy as np

rgb_img = cv2.imread('E:/peking_rw/ocr_project/base_prehandle/img/cartoon.jpg')
gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
reverse_img = 255 - gray_img

random_img = np.zeros((gray_img.shape[0], gray_img.shape[1]), dtype=np.uint8)
for i in range(gray_img.shape[0]):
    for j in range(gray_img.shape[1]):
        random_img[i, j] = gray_img[i, j]*1.2
cv2.imshow('reverse_img', imutils.resize(reverse_img, 800))
cv2.imshow('random_img', imutils.resize(random_img, 800))
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
```

### 窗口销毁

（4）窗口销毁函数
- 当使用imshow函数展示图像时，最后需要在程序中对图像展示窗口进行销毁，否则程序将无法正常终止
- 常用的销毁窗口的函数有下面两个：
  - ① cv2.destroyWindow(windows_name) # 销毁单个特定窗口
    - 参数： 将要销毁的窗口的名字
  - ② cv2.destroyAllWindows() # 销毁全部窗口，无参数

何时销毁窗口？肯定不能图片窗口一出现就将窗口销毁，这样便没法观看窗口，试想有两种方式：
- ① 让窗口停留一段时间然后自动销毁；
- ② 接收指定的命令，如接收指定的键盘敲击然后结束我们想要结束的窗口

以上两种情况都将使用cv2.waitKey函数， 首先产看函数定义：cv2.waitKey(time_of_milliseconds)
- 唯一参数 time_of_milliseconds是整数，可正可负也可是零，含义和操作也不同，分别对应上面说的两种情况
- ① time_of_milliseconds > 0 ：此时time_of_milliseconds表示时间，单位是毫秒，含义表示等待 time_of_milliseconds毫秒后图像将自动销毁
- ② time_of_milliseconds <= 0 ： 此时图像窗口将等待一个键盘敲击，接收到指定的键盘敲击便会进行窗口销毁。我们可以自定义等待敲击的键盘，通过下面的例子进行更好的解释

### opencv 汇总示例

```python
import numpy as np
import cv2

gray_img = cv2.imread('img/cartoon.jpg', 0)  #加载灰度图像
rgb_img = cv2.imread('img/cartoon.jpg', 1)   #加载RGB彩色图像

cv2.imshow('origin image', rgb_img)   #显示原图
cv2.imshow('origin image', imutils.resize(rgb_img, 800))  #利用imutils模块调整显示图像大小
cv2.imshow('gray image', imutils.resize(gray_img, 800))
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()

cv2.imwrite('rgb_img.jpg', rgb_img)   #将图像保存成jpg文件
cv2.imwrite('gray_img.png', gray_img) #将图像保存成png文件

#表示等待10秒后，将销毁所有图像
if cv2.waitKey(10000):
    cv2.destroyAllWindows()
#表示等待10秒，将销毁窗口名称为'origin image'的图像窗口
if cv2.waitKey(10000):
    cv2.destroyWindow('origin image')
#当指定waitKey(0) == 27时，当敲击键盘 Esc 时便销毁所有窗口
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
#当接收到键盘敲击A时，便销毁名称为'origin image'的图像窗口
if cv2.waitKey(-1) == ord('A'):
    cv2.destroyWindow('origin image')
```

### imutils 工具包

imutils 是在OPenCV基础上的一个封装，达到更为简结的调用OPenCV接口的目的，它可以轻松的实现图像的平移，旋转，缩放，骨架化等一系列的操作。

安装方法

```shell
# 在安装前应确认已安装numpy,scipy,matplotlib和opencv
pip install imutils
# pip install NumPy SciPy opencv-python matplotlib imutils
```

图像操作：[参考](https://walkonnet.com/archives/364235)
- 图像平移
  - 相对于原来的cv，使用imutiles可以直接指定平移的像素，不用构造平移矩阵
  - OpenCV中也提供了图像平移的实现，要先计算平移矩阵，然后利用仿射变换实现平移，在imutils中可直接进行图像的平移。
  - translated = imutils.translate(img,x,y)
- 缩放函数：imutils.resize(img,width=100)
- 图像旋转
  - 逆时针旋转 rotated = imutils.rotate(image, 90)
  - 顺时针旋转 rotated_round = imutils.rotate_bound(image, 90)
- 骨架提取（边缘提取）
  - 骨架提取（边缘提取），是指对图片中的物体进行拓扑骨架(topological skeleton)构建的过程。
  - imutils提供的方法是skeletonize()
- 转RGB
  - img = cv.imread("lion.jpeg") 
  - plt.figure() 
  - plt.imshow(imutils.opencv2matplotlib(img))


### 完整示例


```python
import cv2
#pip install imutils
import imutils
import numpy as np

rgb_img = cv2.imread('/Users/wqw/Desktop/二十面体.png')
# 颜色空间转换：rgb → gray
gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
# -------------
# 总共有274种空间转换类型：
# flags = [i for i in dir(cv2) if i.startswith('COLOR_')]
# print(flags)
# -----------
cv2.imshow('origin image', imutils.resize(rgb_img, 800))
cv2.imshow('gray image', imutils.resize(gray_img, 800))
cv2.imwrite('rgb_img.jpg', rgb_img)
cv2.imwrite('gray_img.png', gray_img)

# 等待一定时间自动销毁图像窗口
#if cv2.waitKey(10000):
#    cv2.destroyAllWindows()
#if cv2.waitKey(10000):
#    cv2.destroyWindow('origin image')
# 接收特定键盘销毁图像窗口
#if cv2.waitKey(-1) == ord('A'):
#    cv2.destroyWindow('origin image')
if cv2.waitKey(0) == 27: # 按 Esc键销毁所有窗口
    cv2.destroyAllWindows()
```

# OCR

- `光学字符识别`(`OCR`,Optical Character Recognition)是指对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。OCR技术非常专业，一般多是印刷、打印行业的从业人员使用，可以快速的将纸质资料转换为电子资料。

## OCR工具


### 简易工具

- 【2021-7-26】免费在线OCR工具 [ocrmaker](http://ocrmaker.com/)
- [UU Tool](https://uutool.cn/ocr/)：截图黏贴图片到网站，提取文本；text转语音
- [城华OCR](https://zhcn.109876543210.com/)，将图片转成各种文档格式，限制次数
- [白描](https://web.baimiaoapp.com/image-to-excel)：提取图片中的文字、数学公式等

### 对比总结

【2022-1-25】kaggle笔记：各类OCR方法对比：[Keras-OCR vs EasyOCR vs PYTESSERACT](https://www.kaggle.com/odins0n/keras-ocr-vs-easyocr-vs-pytesseract)
- ![](https://www.kaggleusercontent.com/kf/72864633/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..bibMciKL4OvFX6946nkcFw.H2h7vNPLD0EzD2z6onWrw4R9VV561rtI-O7dAgl4zRkrjH216E96Cg8_ZO6-4Xny6XZL45qeH7xqBdHs7DWKpxJwl6PSES-c3wCDkH1pZifDsjEiIhboIFocwMjIEWDWNFlTY-gafig2CIc9OMmr8Kj2HyhJ_Xmg88Lbsa25dpCF2XkWG6DDLww1eL9wmXE66SzF7sM1_rsUxLvmAplprAQVNPOo2dVSKaGtD5Q1FOD8NvkeRPVeA-MiFGHe8bCtu0paeoX7aPC1z6WzunEsbpGjAeHOWrHXDtYZMPde_Qc77FVe2Qc91b6W_aAYgFoWuehxHKhOgp-jtcSA8cr_UocTj3chqBQgJKkwFodQdZInVeknz7L1HA9IGJgpWEy8DPZcNjhNuXgoWqpjqJLslljIJa-8N3Dy3qqu5p8Ku54YnzDSak2rMgdn_ThhC5AtDM3_7aB_s6vI4LoeVFxYTJ4JLVyw3v_YqIOe1BG7qD-QN2bqZixhJvtajOYzllcLP21NqMesuo7dHa-favmNVYo6o9zirwLvyYFrW4z0BpdBGkf_nQ_6n452u6GMiaRwmpNgNpD3zVv1BRCNbvMrJyzm5Mb7iqmedml2Yi6NFMxEgyOvb6rclteSyWU8_CMhP0bl3KGxEgeqNSD9Z02teSGWd9Gl8Nb6F9SByo90TtEZPJy7kIpa9Y9VPHwV7JAD.PtUtOX_gh2gJUMvxM9Wyvw/__results___files/__results___14_1.png)

CONCLUSIONS 对比结论
* **Keras-OCR** is image specific OCR tool. If text is inside the image and their fonts and colors are unorganized, Keras-ocr consumes time if used on CPU 
* **EasyOCR** is lightweight model which is giving a good performance for receipt or PDF conversion. It is giving more accurate results with organized texts like pdf files, receipts, bills. EasyOCR also performs well on noisy images 适合发票、pdf格式、噪声图片
* **Pytesseract** is performing well for **high-resolution** images. Certain morphological operations such as dilation, erosion, OTSU binarization can help increase pytesseract performance. It also provides better results on handwritten text as compared to EasyOCR 适合高分辨率图、手写字体
* All these results can be further improved by performing specific image operations.



### Tesseract

- Tesseract 的OCR引擎最先由HP实验室于1985年开始研发，至1995年时已经成为OCR业内最准确的三款识别引擎之一。
- Tesseract 目前已作为开源项目发布在Google Project，其最新版本3.0已经支持中文OCR，并提供了一个命令行工具。

安装：
- pip install pytesseract

调用代码

```python
import cv2                        # OpenCV: Computer vision and image manipulation package
import pytesseract                # python Tesseract: OCR in python
import matplotlib.pyplot as plt   # plotting
import numpy as np                # Numpy for arrays
from PIL import Image             # Pillow: helps us read remote images
import requests                   # Requests: to fetch remote URLs
from io import BytesIO            # Helps read remote images

def get_image(url):
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  return img

img = get_image('https://github.com/jalammar/jalammar.github.io/raw/master/notebooks/cv/label.png')
# OCR结果
print(pytesseract.image_to_string(img))
```

### EasyOCR

- 【2020-8-7】[一个超好用的开源OCR](https://www.toutiao.com/i6858234401206043140/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1596809559&app=news_article&utm_source=mobile_qq&utm_medium=toutiao_android&use_new_style=1&req_id=20200807221239010147083076216022E3&group_id=6858234401206043140)：[EasyOCR](https://github.com/JaidedAI/EasyOCR)，目前能够支持58种语言，其中有中文(简体和繁体)、日语、泰语、韩语等
   - EasyOCR的模型主要分为两个，基于CRAFT的文字检测模型和基于ResNet+LSTM+CTC的识别模型
   - ![](http://p6-tt.byteimg.com/large/pgc-image/2402e44dff954e4985f6762de5b07ce6?from=pc)
   - 第三方基于easyOCR提供了几个demo地址，大家可以试试自己的数据看看效果：
      - [Demo1](https://colab.fan/easyocr)
      - [Demo2](https://hub.docker.com/r/challisa/easyocr)
      - [Demo3](https://easyocrgpu-wook-2.endpoint.ainize.ai/)
      - ![](http://p3-tt.byteimg.com/large/pgc-image/a56400ef928d419c8ef29c64abede5da?from=pc)

### Pix2Text

【2022-9-21】[Pix2Text: 替代 Mathpix 的免费 Python 开源工具](https://www.toutiao.com/article/7145465980930556450)
- [Pix2Text](https://github.com/breezedeus/pix2text) 期望成为 Mathpix 的免费开源 Python 替代工具，完成与 Mathpix 类似的功能。当前 Pix2Text 可识别截屏图片中的数学公式、英文、或者中文文字。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a81b8f2c95794d0596a1d7b803df2a34~noop.image?_iz=58558&from=article.pc_detail&x-expires=1664345981&x-signature=1blBfmn0jnAV0%2FOB5VuIKt1Jd3Q%3D)

Pix2Text首先利用图片分类模型来判断图片类型，然后基于不同的图片类型，把图片交由不同的识别系统进行文字识别：
- 如果图片类型为 **formula** ，表示图片为数学公式，此时调用 LaTeX-OCR 识别图片中的数学公式，返回其Latex表示；
- 如果图片类型为 **english**，表示图片中包含的是英文文字，此时使用 CnOCR (https://github.com/breezedeus/cnocr) 中的英文模型识别其中的英文文字；英文模型对于纯英文的文字截图，识别效果比通用模型好；
- 如果图片类型为 **general**，表示图片中包含的是常见文字，此时使用 CnOCR 中的通用模型识别其中的中或英文文字。


```python
#pip install pix2text -i https://pypi.doubanio.com/simple
from pix2text import Pix2Text

img_fp = './docs/examples/formula.jpg'
p2t = Pix2Text()
out_text = p2t(img_fp)  # 也可以使用 `p2t.recognize(img_fp)` 获得相同的结果
print(out_text)
```


### 中文OCR比赛第一

【2022-1-25】[第一次比赛，拿了世界人工智能大赛 Top1 ！](https://blog.csdn.net/Datawhale/article/details/122613233)，“世界人工智能创新大赛”——手写体 OCR 识别竞赛（任务一），取得了Top1的成绩
- [赛题地址](http://ailab.aiwin.org.cn/competitions/65)
- 背景：银行日常业务中涉及到各类凭证的识别录入，例如身份证录入、支票录入、对账单录入等。以往的录入方式主要是以人工录入为主，效率较低，人力成本较高。近几年来，OCR相关技术以其自动执行、人为干预较少等特点正逐步替代传统的人工录入方式。但OCR技术在实际应用中也存在一些问题，在各类凭证字段的识别中，手写体由于其字体差异性大、字数不固定、语义关联性较低、凭证背景干扰等原因，导致OCR识别率准确率不高，需要大量人工校正，对日常的银行录入业务造成了一定的影响
- 数据集：原始手写体图像共分为三类，分别涉及银行名称、年月日、金额三大类，分别示意如下：
  - ![](https://img-blog.csdnimg.cn/img_convert/4cfda26453767dec3b2d436540d3c6b8.png)
- 相应图片切片中可能混杂有一定量的干扰信息
  - ![](https://img-blog.csdnimg.cn/img_convert/cd77146fdad3c8b41f455b2992a6b784.png)

OCR比赛最常用的模型是 CRNN + CTC，选择代码：Attention_ocr.pytorch-master.zip

模型改进：crnn的卷积部分类似VGG，我对模型的改进主要有一下几个方面：
- 1、加入激活函数Swish。
- 2、加入BatchNorm。
- 3、加入SE注意力机制。
- 4、适当加深模型。

```python
self.cnn = nn.Sequential(
   nn.Conv2d(nc, 64, 3, 1, 1), Swish(), nn.BatchNorm2d(64),
   nn.MaxPool2d(2, 2),  # 64x16x50
   nn.Conv2d(64, 128, 3, 1, 1), Swish(), nn.BatchNorm2d(128),
   nn.MaxPool2d(2, 2),  # 128x8x25
   nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   SELayer(256, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 256x4x25
   nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   nn.Conv2d(512, 512, 1), nn.BatchNorm2d(512), Swish(),
   nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   SELayer(512, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 512x2x25
   nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), Swish()
)  # 512x1x25
# SE和Swish
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=True),
            nn.LeakyReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
```

## 验证码识别

- 验证码是一种区分用户是计算机还是人的公共全自动程序。可以防止：恶意破解密码、刷票、论坛灌水，有效防止某个黑客对某一个特定注册用户用特定程序暴力破解方式进行不断的登陆尝试，实际上用验证码是现在很多网站通行的方式。由于验证码可以由计算机生成并评判，但是必须只有人类才能解答，所以回答出问题的用户就可以被认为是人类。
- 目前的验证码通常的种类及特点如下：
   - （1）最基础的英文验证码：纯粹的英文与数字组合，白色背景，这是最容易实现OCR识别的验证码。
   - （2）字体变形的英文验证码：可以通过简单的机器学习实现对英文与数字的识别，准确率较高。
   - （3）加上扰乱背景线条的验证码：可以通过程序去除干扰线，准确率较高。
   - （4）中文验证码：中文由于字体多样，形状多变，数量组合众多，实现起来难度较大，准确率不高。
   - （5）中文字体变形验证码：准确率更低。
   - （6）中英文混合验证码：非常考验OCR程序的判断能力，基本上识别起来非常有难度。
   - （7）提问式验证码：这是需要OCR结合人工智能才能实现，目前基本上无法识别。
   - （8）GIF动态图验证码：由于GIF图片是动态图，无法定位哪一帧是验证码，所以难度很大。
   - （9）划动式验证码：虽然程序可以模拟人的操作，但是具体拖动到哪个位置很难处理。
   - （10）视频验证码：目前OCR识别还未实现。
   - （11）手机验证码：手机验证码实现自动化是很容易的，但是手机号码不那么容易获得。
   - （12）印象验证码：目前无解。

![](https://pic1.zhimg.com/80/v2-2b9748a5ca5498ba1955eec9a5b79db4_720w.jpg)

- 附录：
   - [利用Tesseract-OCR实现验证码识别](https://zhuanlan.zhihu.com/p/34530032)

- 「Happy Captcha」，一款易于使用的 Java 验证码软件包，旨在花最短的时间，最少的代码量，实现 Web 站点的验证码功能。
   - Captcha缩写含义：Completely Automated Public Turing test to tell Computers and Humans Apart
- 效果图
   - ![](https://pic3.zhimg.com/v2-971f594800cdd101950f916f92cb7b1e_b.webp)

## GAN方法

- 【2018-12-14】[基于GAN的验证码识别工具，0.5秒宣告验证码死刑！](https://baijiahao.baidu.com/s?id=1619803729564462538)
   - 中英两国研究人员联合开发了一套基于GAN的验证码AI识别系统，能在0.5秒之内识别出验证码，从 实际测试结果看，可以说宣布了对验证码的“死刑判决”。
      - ![](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=280512761,907748494&fm=173&app=49&f=JPEG?w=640&h=273&s=0D30E51281D85DC04A55B0CB0000D0B3)
      - [论文地址](http://www.lancaster.ac.uk/staff/wangz3/publications/ccs18.pdf)，博文介绍：[An A.I. cracks the internet’s squiggly letter bot test in 0.5 seconds](https://www.digitaltrends.com/cool-tech/ai-cracks-captcha-05-seconds/)
   - 该系统已在不同的33个验证码系统中进行了成功测试，其中11个来自世界上最受欢迎的一些网站，包括eBay和维基百科等。
   - 这种方法的新颖之处在于：使用生成对抗网络（GAN）来创建训练数据。不需要收集和标记数以百万计的验证码文本数据，只需要500组数据就可以成功学习。而且可以使用这些数据，来生成数百万甚至数十亿的合成训练数据，建立高性能的图像分类器。
   - 结果显示，<font color='red'>该系统比迄今为止所见的任何验证码识别器系统的识别精度都高。</font>
   - ![](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1299396691,4195542946&fm=173&app=49&f=JPEG?w=640&h=416&s=A498E633795644CA4A6580DA0000C0B3)


# 图像风格化

[图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

图像风格迁移科技树
- ![](https://pic4.zhimg.com/80/v2-526f16430324d3fbd8c07ff3d1c05c0b_hd.jpg)

## Demo

- 【2021-5-11】[案例解析：用Tensorflow和Pytorch计算骰子值](https://www.toutiao.com/i6622845628902801933/)
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/557db089145144b5a7ae5195cf6d4aef?from=pc)，[github](https://github.com/sugi-chan/2-stage-dice-pipeline)
- 【2020-12-04】AI姿势传递模型，[论文地址](https://arxiv.org/pdf/2012.01158.pdf)，不愿意出节目的码农的年会神器？[将舞蹈化为己用-视频](https://weibo.com/tv/show/1034:4578074625245199?from=old_pc_videoshow)
- 【2020-12-02】【MaskDetection：滴滴开源的口罩检测模型】 by DiDi [GitHub](https://github.com/didi/maskdetection)
- 【2020-12-04】[孪生网络用于图片搜索](https://www.pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/)《Siamese networks with Keras, TensorFlow, and Deep Learning - PyImageSearch》by Adrian Rosebrock
  - ![](https://www.pyimagesearch.com/wp-content/uploads/2020/11/keras_siamese_networks_header.png)
- 【2022-10-8】北大博士的图像、视频风格化展示：[williamyang1991](https://williamyang1991.github.io/)，还有[文本字体风格化](https://williamyang1991.github.io/project.html#research1)

## 风格化

【2022-11-25】图像风格化工具
- ① [Image to Sketch AI](https://imagetosketch.com/)，caricature漫画；素描
- ② [newprofilepic](https://newprofilepic.com/)，多种风格可选
- ③ [AnimeGANv2](https://huggingface.co/spaces/akhaliq/AnimeGANv2)，卡通画，hugging face上

【2022-1-25】[5个方便好用的Python自动化脚本](https://www.toutiao.com/i7056585992664269344)

### 素描风格

自动生成素描草图
- 这个脚本可以把彩色图片转化为铅笔素描草图，对人像、景色都有很好的效果。而且只需几行代码就可以一键生成，适合批量操作，非常的快捷。

第三方库：
- Opencv - 计算机视觉工具，可以实现多元化的图像视频处理，有Python接口

安装

```shell
# 安装opencv的Python库
pip install opencv-python
```

示例

```python
""" Photo Sketching Using Python """
import cv2

img = cv2.imread("elon.jpg")
## Image to Gray Image
gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
## Gray Image to Inverted Gray Image
inverted_gray_image = 255-gray_image
## Blurring The Inverted Gray Image
blurred_inverted_gray_image = cv2.GaussianBlur(inverted_gray_image, (19,19),0)
## Inverting the blurred image
inverted_blurred_image = 255-blurred_inverted_gray_image
### Preparing Photo sketching
sketck = cv2.divide(gray_image, inverted_blurred_image,scale= 256.0)
cv2.imshow("Original Image",img)
cv2.imshow("Pencil Sketch", sketck)
cv2.waitKey(0)
```

素描草图：马斯克
- ![](https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/8c5a6fdc6274477ca9e3b85990f6942f?from=pc)


### fast-style

- 图像风格迁移，[深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
- Fast style transfer check [demo](https://wqw547243068.github.io/demo/fast-style/)

![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/fast-style.PNG)

### 风格化案例

- 【2019-07-19】[图像风格迁移示例](https://reiinakano.github.io/arbitrary-image-stylization-tfjs)

<iframe src="https://reiinakano.github.io/arbitrary-image-stylization-tfjs" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

### Real Time style transfer 实时风格迁移

Real Time style transfer check [demo](https://wqw547243068.github.io/demo/RST/)
- ![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/rst.png)

## 扩散模型

【2022-8-31】苏剑林的[生成扩散模型漫谈](https://kexue.fm/archives/9119)
- 生成模型中，VAE、GAN“如雷贯耳”，还有一些比较小众的选择，如flow模型、VQ-VAE等，颇有人气，尤其是VQ-VAE及其变体VQ-GAN，近期已经逐渐发展到“图像的Tokenizer”的地位，用来直接调用NLP的各种预训练方法。
- 除此之外，还有一个本来更小众的选择——`扩散模型`（Diffusion Models）——正在生成模型领域“异军突起”，当前最先进的两个文本生成图像—— OpenAI 的 `DALL·E 2` 和 Google的`Imagen`，都是基于`扩散模型`来完成的。

生成扩散模型的大火，始于2020年所提出的[DDPM](https://arxiv.org/abs/2006.11239)（Denoising Diffusion Probabilistic Model），虽然也用了“**扩散模型**”这个名字，但事实上除了采样过程的形式有一定的相似之外，DDPM与传统基于`朗之万`方程采样的扩散模型完全不一样，一个新的起点、新的篇章。

### 扩散模型资源

- 【2022-9-20】[扩散模型大全](https://github.com/heejkoo/Awesome-Diffusion-Models)
- hugginface的扩散模型包：[diffusers](https://github.com/huggingface/diffusers/tree/main/examples)，[colab笔记](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=13NnZ4rVioLs), demo: [stable-diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion)

经典论文
- 《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》 2015年 扩散模型起源
- 《Denoising Diffusion Probabilistic Models》 2020年 扩散模型兴起, 对应[pytorch实现](https://github.com/lucidrains/denoising-diffusion-pytorch)
- 《Improved Denoising Diffusion Probabilistic Models》 2021年 第二篇论文的改进, 对应[pytorch实现](https://github.com/openai/improved-diffusion)

技术文章
- [The recent rise of diffusion-based models](https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html) 可以了解到扩散模型近年比较经典的应用
- [Introduction to Diffusion Models for Machine Learning](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/) 从中可以了解到一个实现扩散模型的库denoising_diffusion_pytorch，博客中有使用案例
- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) 也是扩散模型的一个理论介绍博客，推导挺详细的
- [Diffusion Models as a kind of VAE](https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html) 探究了VAE和扩散模型的联系
- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) 扩散模型理论和代码实现，代码我进行理解加了注释与理论对应，方便大家理解
- [An introduction to Diffusion Probabilistic Models](https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html) 也是一个介绍性博客，公式也很工整

[扩散模型原理和pytorch代码实现初学资料汇总](https://blog.csdn.net/qq_44941689/article/details/126513283)


### 扩散模型原理

标准的`扩散模型`（diffusion models）涉及到**图像变换**（添加高斯噪声）和**图像反转**。但是扩散模型的生成并不强烈依赖于图像降解的选择。通过实验证明了基于完全确定性的降解（例如模糊、masking 等），也可以轻松训练一个扩散生成模型。
- [项目地址](https://github.com/arpitbansal297/cold-diffusion-models)
- [论文地址](https://arxiv.org/abs/2208.09392)
这个工作成功地质疑了社区对扩散模型的理解：它并非依赖于**梯度郎之万动力学**（gradient Langevin dynamics）或**变分推理**（variational inference）。


准确来说，`DDPM`叫“**渐变模型**”更为准确一些，扩散模型这一名字反而容易造成理解上的误解，传统扩散模型的**能量模型**、**得分匹配**、`朗之万`方程等概念，其实跟DDPM及其后续变体都没什么关系。
- DDPM的数学框架其实在ICML2015的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》就已经完成了，但DDPM是首次将它在高分辨率图像生成上调试出来了，从而引导出了后面的火热。由此可见，一个模型的诞生和流行，往往还需要时间和机遇

### 多模态图像生成

【2022-9-7】[当 AI 邂逅绘画艺术，能迸发出怎样的火花？](https://posts.careerengine.us/p/63183f351bd2e15b54dda53f?from=latest-posts-panel&type=title)

`多模态图像生成`（Multi-Modal Image Generation）旨在利用文本、音频等模态信息作为指导条件，生成具有自然纹理的逼真图像。不像传统的根据噪声生成图像的单模态生成技术，多模态图像生成一直以来就是一件很有挑战的任务，要解决的问题主要包括：
- （1）如何跨越“语义鸿沟”，打破各模态之间固有的隔阂？
- （2）如何生成合乎逻辑的，多样性的，且高分辨率的图像？

近两年，随着 Transformer 在自然语言处理（如 GPT）、计算机视觉（如 ViT）、多模态预训练（如 CLIP）等领域的成功应用，以及以 VAE、GAN 为代表的图像生成技术有逐渐被后起之秀——扩散模型（Diffusion Model）赶超之势，多模态图像生成的发展一发不可收拾。

按照训练方式采用的是 Transformer 自回归还是扩散模型的方式，近两年多模态图像生成重点工作分类如下
- Transformer 自回归：将文本和图像分别转化成 tokens 序列，然后利用生成式的 Transformer 架构从文本序列（和可选图像序列）中预测图像序列，最后使用图像生成技术（VAE、GAN等）对图像序列进行解码，得到最终生成图像。
- 扩散模型：扩散模型（Diffusion Model）是一种图像生成技术，最近一年发展迅速，被喻为 GAN 的终结者。如图所示，扩散模型分为两阶段：
  - （1）加噪：沿着扩散的马尔可夫链过程，逐渐向图像中添加随机噪声；
  - （2）去噪：学习逆扩散过程恢复图像。常见变体有去噪扩散概率模型（DDPM）等。
- ![](https://static.careerengine.us/api/aov2/https%3A_%7C__%7C_mmbiz.qpic.cn_%7C_mmbiz_png_%7C_Z6bicxIx5naJlJ5U7H2h9WvOKicVvP1IMQsP7Beoqq3agsokoH4E75sO33rXmPORQ4djtdEB3IAMBnsk8bugYcKQ_%7C_640%3Fwx_fmt%3Dpng)

采取扩散模型方式的多模态图像生成做法，主要是通过带条件引导的扩散模型学习文本特征到图像特征的映射，并对图像特征进行解码得到最终生成图像


### 文字→视频

【2022-10-8】[图像生成卷腻了，谷歌全面转向文字→视频生成](https://www.toutiao.com/article/7151774108186083843)，挑战分辨率和长度; 文本转图像上卷了大半年之后，Meta、谷歌等科技巨头又将目光投向了一个新的战场：文本转视频。
- 上周，Meta公布了一个能够生成高质量短视频的工具——[Make-A-Video](https://makeavideo.studio/)，利用这款工具生成的视频非常具有想象力。
  - [Introducing Make-A-Video: An AI system that generates videos from text](https://ai.facebook.com/blog/generative-ai-text-to-video/)
  - ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4bc8759f50f747ff99ac4cd92eb2816b~noop.image)
- 谷歌也不甘示弱。刚刚，该公司 CEO Sundar Pichai 亲自安利了他们在这一领域的最新成果：两款文本转视频工具——Imagen Video 与 Phenaki。前者主打视频品质，后者主要挑战视频长度，可以说各有千秋。
  - ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/d85eca195ae541da80c8b7b93d247fa7~noop.image)
  - 生成式建模在最近的文本到图像 AI 系统中取得了重大进展，比如 DALL-E 2、Imagen、Parti、CogView 和 Latent Diffusion。特别地，扩散模型在密度估计、文本到语音、图像到图像、文本到图像和 3D 合成等多种生成式建模任务中取得了巨大成功。
  - 谷歌想要做的是从文本生成视频。以往的视频生成工作集中于具有自回归模型的受限数据集、具有自回归先验的潜变量模型以及近来的非自回归潜变量方法。扩散模型也已经展示出了出色的中等分辨率视频生成能力。
  - 在此基础上，谷歌推出了 [Imagen Video](https://imagen.research.google/video/)，[论文地址](https://imagen.research.google/video/paper.pdf),它是一个基于级联视频扩散模型的文本条件视频生成系统。给出文本提示，Imagen Video 就可以通过一个由 frozen T5 文本编码器、基础视频生成模型、级联时空视频超分辨率模型组成的系统来生成高清视频。

## 应用案例

### 杂货店货架识别

- 【2021-1-23】[Grocery-Product-Detection](https://github.com/sayakpaul/Grocery-Product-Detection)
  - This repository builds a product detection model to recognize products from grocery shelf images. The dataset comes from [here](https://github.com/gulvarol/grocerydataset). Everything from data preparation to model training is done using [Colab Notebooks](https://colab.research.google.com/) so that no setup is required locally. All the relevant commentaries have been included inside the Colab Notebooks.

### 文本生成图片

【2022-8-16】[TikTok 乱拳打死老师傅：硅谷大厂还在发论文，它产品已经上线了](https://www.sohu.com/a/577300364_114819)
- 不少家互联网大厂都在试图测试、开发 AI 文字转图片技术，结果没想到，TikTok 却率先将这项技术应用到了产品里，在 AI 创作潮流中异军突起。
- TikTok 的特效菜单下，最近增加了一个名叫“AI 绿幕” (AI Greenscreen) 的新选项。
- 点击这个选项，然后在屏幕中间的对话框里输入一段文字描述，只用不到5秒的时间，TikTok 就可以根据文字描述生成一张竖版画作，用作短视频的背景：
- ![](https://p3.itc.cn/q_70/images03/20220816/5fdb55b70e054099a88ea6bc5bfeca09.png)
- 生成结果具有非常强的水彩/油画感觉，风格迁移 (style transfer) 的痕迹明显，而且用的颜色也都鲜亮明快，给人一种耳目一新的感受。
- ![](https://p4.itc.cn/q_70/images03/20220816/902b3d3e8e8d45e0aba3be1bdf1694e6.png)

#### Disco Diffusion

Disco Diffsion 存在问题

基于多模态图像生成模型 Disco Diffusion（DD）进行 AI 创作目前存在以下几个问题：
- （1）生成图像质量参差不齐：根据生成任务的难易程度，粗略估算描述内容较难的生成任务良品率 20%～30%，描述内容较容易的生成任务良品率 60%～70%，大多数任务良品率在 30～40% 之间。
- （2）生成速度较慢+内存消耗较大：以迭代 250 steps 生成一张 1280*768 图像为例，需要大约花费 6分钟，以及使用 V100 16G 显存。
- （3）严重依赖专家经验：选取一组合适的描述词需要经过大量文本内容试错及权重设置、画家画风及艺术社区的了解以及文本修饰词的选取等；调整参数需要对 DD 包含的 CLIP 引导次数/饱和度/对比度/噪点/切割次数/内外切/梯度大小/对称/... 等概念深刻了解，同时要有一定的美术功底。众多的参数也意味着需要较强的专家经验才能获得一张还不错的生成图像。

#### Stable Diffusion

Stable Diffusion is a state of the art text-to-image model that generates images from text.

- transformers上的 [Stable Diffusion Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion)
- For faster generation and forthcoming API access you can try [DreamStudio Beta](http://beta.dreamstudio.ai/)
- <iframe src="https://beta.dreamstudio.ai/dream">


#### DALL·E


#### 文心-一格

- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, [文心·一格](https://yige.baidu.com/#/)
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。

看似“一句话生成图片”不难，其实对AI语义理解和图像生成能力提出了进一步要求。
- 为了能更好地理解文本、提升输出效果，文心·一格还在百度文心的图文生成跨模态模型ERNIE-VilG的基础上，进行了更详细的优化。
- 为了提升图文理解能力，在知识增强的基础上，引入跨模态多视角对比学习；
- 为了降低输入要求同时提升效果，采用基于知识的文本联想能力，让模型学会自己扩展提示词的细节和风格；
- 为了提升图像生成能力，采用渐进式扩散模型训练算法，让模型来选择效果最好的生成网络。


StableDiffusion 图像生成能力一探！Int8量化教程与ONNX导出推理
- CPU下推理StableDiffusion，以及OpenVINO加速的代码，同时，也包含了量化脚本

```shell
#git clone https://github.com/luohao123/gaintmodels
git clone https://huggingface.co/CompVis/stable-diffusion-v1-4
git lfs install
cd stable-diffusion-v1-4
git lfs pull
```

测试StableDiffusion
- 来看看生成的效果，由于模型只能编码英文，我们就以英文作为promopt。
- A green car with appearance of Tesla Model 3 and Porsche 911
- A robot Elon Musk in cyberpunk, driving on a Tesla Model X


#### 自定义图片的text2image

【2022-9-7】[An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/)
- [github-textual_inversion](https://github.com/rinongal/textual_inversion)
- 基于潜在扩散模型（Latent Diffusion Models, LDM），允许用户使用自然语言指导 AI 生成包含特定独特概念的图像。
- 例如我想将心爱的宠物猫咪变成一幅独特的画作——抽象派猫猫，只需要提供3-5张照片，然后通过控制自然语言输入，来得到一个我家猫咪的抽象画作。
- 简单介绍下过程：首先，模型会通过学习这些图片，使用一些单词去表示图片。其次，这些单词可以组合成自然语言句子，通过 prompt 形式指导模型进行个性化创作。好处在于，图像的自然语言表示对用户非常友好。用户可以自由修改 prompt 内容以获取他们想要的风格、主题和独一无二的结果。
- We learn to generate **specific concepts**, like personal objects or artistic styles, by describing them using new "words" in the embedding space of pre-trained **text-to-image** models. These can be used in new sentences, just like any other word.
- Our work builds on the publicly available [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion)
- ![](https://textual-inversion.github.io/static/images/editing/teaser.JPG)
- ![](https://textual-inversion.github.io/static/images/training/training.JPG)

#### 视频风格化

【2022-9-7】通过将预训练的语言图像模型（pretrained language-image models）调整为视频识别，以此将对比语言图像预训练方法（contrastive language-image pretraining）扩展到视频领域；
- 为了捕捉视频中帧沿时间维度的远程依赖性，提出了一个跨帧的注意力机制，明确了跨帧的信息交换。此外该模块非常轻量化，可以无缝插入预训练的语言图像模型。
- [项目地址](https://github.com/microsoft/videox)
- [论文地址](https://arxiv.org/abs/2208.02816)


## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 风格迁移简介

- [深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)

`风格迁移`（style transfer）最近两年非常火，可谓是深度学习领域很有创意的研究成果。它主要是通过神经网络，将一幅艺术风格画（style image）和一张普通的照片（content image）巧妙地融合，形成一张非常有意思的图片。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/4a0dbd0ba7031a2b9e0f97d222d5050799764b92b7b135ffba3edfda4fd2feea)

因为新颖而有趣，自然成为了大家研究的焦点。目前已经有许多基于风格迁移的应用诞生了，如移动端风格画应用Prisma，手Q中也集成了不少的风格画滤镜：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/de7624d2c5163daeb833b4a4a4d4bbbf340fbc2a8289763833f8de2608f89b9c)

本文将对风格迁移[^1]的实现原理进行下简单介绍，然后介绍下它的快速版，即fast-style- transfer[^2]。

## 1. 风格迁移开山之作

2015年，Gatys等人发表了文章[^1]《A Neural Algorithm of Artistic Style》，首次使用深度学习进行艺术画风格学习。把风格图像Xs的绘画风格融入到内容图像Xc，得到一幅新的图像Xn。则新的图像Xn：即要保持内容图像Xc的原始图像内容（内容画是一部汽车，融合后应仍是一部汽车，不能变成摩托车），又要保持风格图像Xs的特有风格（比如纹理、色调、笔触等）。

### 1.1 内容损失（Content Loss）

在CNN网络中，一般认为较低层的特征描述了图像的具体视觉特征（即纹理、颜色等），较高层的特征则是较为抽象的图像内容描述。 所以要比较两幅图像的内容相似性，可以比较两幅图像在CNN网络中高层特征的相似性（欧式距离）。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/880c6f8c21936bb4c29a2e6952ce357f7e844e7328c86f2a730f500768e66802)

1.2 风格损失（Style Loss）
而要比较两幅图像的风格相似性，则可以比较它们在CNN网络中较低层特征的相似性。不过值得注意的是，不能像内容相似性计算一样，简单的采用欧式距离度量，因为低层特征包含较多的图像局部特征（即空间信息过于显著），比如两幅风格相似但内容完全不同的图像，若直接计算它们的欧式距离，则可能会产生较大的误差，认为它们风格不相似。论文中使用了Gram矩阵，用于计算不同响应层之间的联系，即在保留低层特征的同时去除图像内容的影响，只比较风格的相似性。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bc72f7cd6be684f73c7c7e3649dbba4b030bb2607c66370104e043c71b2ac31c)

那么风格的相似性计算可以用如下公式表示：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/1a7ce05010b913ae2c5f58ef362aa76638199c79293f493856feb80d99703476)

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/eff0adf1fd4b183cb872d79e2a5a70ca66d6d21845a59bbf6faf31012532be3a)

### 1.3 总损失（Total Loss）

这样对两幅图像进行“内容+风格”的相似度评价，可以采用如下的损失函数：
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3d679c5b0a174e93a56eba66383e6abd57431c24e76805f0fdcf8d7caa3d89ef)

### 1.4 训练过程

文章使用了著名的VGG19网络[3]来进行训练（包含16个卷积层和5个池化层，但实际训练中未使用任何全连接层，并使用平均池化average- pooling替代最大池化max-pooling）。
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3f981ef8277f3d56dbc0dfb8cb9fb38bbcf6c35914f8bc3e53fda29ac2eed6f6)

内容层和风格层的选择：将`内容图像`和`风格图像`分别输入到VGG19网络中，并将网络各个层的特征图（feature map）进行可视化（重构）。

内容重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv2_1 (b)
- 3. conv3_1 (c)
- 4. conv4_1 (d)
- 5. conv5_1 (e)
风格重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv1_1 and conv2_1 (b) 
- 3. conv1_1, conv2_1 and conv3_1 (c)
- 4. conv1_1, conv2_1, conv3_1 and conv4_1 (d)
- 5. conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 (e)

通过实验发现：对于内容重构，(d)和(e)较好地保留了图像的高阶内容（high-level content）而丢弃了过于细节的像素信息；对于风格重构，(e)则较好地描述了艺术画的风格。如下图红色方框标记：
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/44a2b217d7d007c6110d5248c90ccf0f66c6296f320954668d73c1af6b0d5fa7)

在实际实验中，内容层和风格层选择如下：
- 内容层：conv4_2
- 风格层：conv11, conv2_1, conv3__1_, _conv4_1, conv5_1
- 训练过程：以白噪声图像作为输入(x)到VGG19网络，conv4_2层的响应与原始内容图像计算出内容损失（Content Loss），“conv1_1, conv2_1, conv3_1, conv4_1, conv5_1”这5层的响应分别与风格图像计算出风格损失，然后它们相加得到总的风格损失（Style Loss），最后Content Loss + Style Loss = Total Loss得到总的损失。采用梯度下降的优化方法求解Total Loss函数的最小值，不断更新x，最终得到一幅“合成画”。

### 1.5 总结

每次训练迭代，更新的参数并非VGG19网络本身，而是随机初始化的输入x；
由于输入x是随机初始化的，最终得到的“合成画”会有差异；
每生成一幅“合成画”，都要重新训练一次，速度较慢，难以做到实时。

## 2. 快速风格迁移

2016年Johnson等人提出了一种更为快速的风格迁移方法[2]《[Perceptual losses for real-time style transfer and super- resolution](http://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)》。

### 2.1 网络结构
它们设计了一个变换网络（Image Transform Net），并用VGG16网络作为损失网络（Loss Net）。输入图像经由变换网络后，会得到一个输出，此输出与风格图像、内容图像分别输入到VGG16损失网络，类似于[1]的思路，使用VGG16不同层的响应结果计算出内容损失和风格损失，最终求得总损失。然后使用梯度下降的优化方法不断更新变换网络的参数。 
- 内容层：relu3_3
- 风格层：relu12, relu2_2, relu3_3, _relu4_3
其中变换网络（Image Transform Net）的具体结构如下图所示： 

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bea3e1a19df5198b9a31f7d241361cb129f13e0d0d5f4f4e0d14439d6d0b8126)

### 2.2 跑个实验

Johnson等人将论文的代码实现在[github](https://github.com/jcjohnson/fast-neural-style)上进行了开源，包括了论文的复现版本，以及将“Batch-Normalization ”改进为“Instance Normalization”[[4](https://arxiv.org/pdf/1607.08022.pdf)]的版本。咱们可以按照他的说明，训练一个自己的风格化网络。我这里训练了一个“中国风”网络，运行效果如下： 
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/c628d678344dddaef81c122d33fcff1cd00d1d7f2b90834410492ae03bf005d4)

### 2.3 总结

网络训练一次即可，不像Gatys等人[1]的方法需要每次重新训练网络；
可以实现实时的风格化滤镜：在Titan X GPU上处理一张512x512的图片可以达到20FPS。下图为fast-style-transfer与Gatys等人[1]方法的运行速度比较，包括了不同的图像大小，以及Gatys方法不同的迭代次数。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/66b64458ff003281762ca3b3da2a7ad3e769b6274259431e2dbd82f9fd5543dd)

3. 参考资料

- Gatys L A, Ecker A S, Bethge M. A neural algorithm of artistic style[J]. arXiv preprint arXiv:1508.06576, 2015.
- Johnson J, Alahi A, Fei-Fei L. Perceptual losses for real-time style transfer and super-resolution[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 694-711.
- Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.
- Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.
- [Fast Style Transfer(快速风格转移)](http://closure11.com/fast-style-transfer%E5%BF%AB%E9%80%9F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/)
- [图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

