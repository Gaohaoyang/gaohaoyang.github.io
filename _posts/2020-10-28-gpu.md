---
layout: post
title:  "并行计算(GPU/TPU)及推理加速"
date:   2020-10-28 19:25:00
categories: 编程语言
tags: CPU GPU TPU Tensorflow Pytorch 并行计算 加速 分布式 cuda tensorrt 推理加速 onnx zero lora
excerpt: 高性能计算知识点，如CPU/GPU/TPU，及代码实现
author: 鹤啸九天
mathjax: true
permalink: /gpu
---

* content
{:toc}

# 总结

- 【2020-10-28】[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

# 简介

- 2006年，NVIDIA公司发布了`CUDA`，CUDA是建立在NVIDIA的CPUs上的一个通用并行计算平台和编程模型，基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。
- 近年来，`GPU`最成功的一个应用就是**深度学习**领域，基于GPU的并行计算已经成为训练深度学习模型的标配。截止2018年3月，最新的CUDA版本为CUDA 9。
- GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此说GPU并行计算时，其实是指的**基于CPU+GPU的异构计算架构**。
- 在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为**主机端**（host），而GPU所在位置称为**设备端**（device），如下图所示。
  - ![](https://img-blog.csdn.net/20180318132344300?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- GPU包括更多的运算核心，其特别适合**数据并行**的**计算密集型**任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。
- 另外，CPU上的线程是重量级的，**上下文切换开销大**，但是GPU由于存在很多核心，其线程是轻量级的。
- 因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。
- ![](https://img-blog.csdn.net/20180318132422473?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- [在显卡界，对于「A 卡」和「N 卡」的信仰是如何形成的？](https://www.zhihu.com/question/28520691)
- ![](https://pic4.zhimg.com/80/8bfb07ebbf27f2e486a0ae933515a58d_720w.jpg?source=1940ef5c)

- 显卡性能天梯图
  - [如何通过显卡名字判断显卡的好坏？](https://www.zhihu.com/question/34970298)


## CPU

- CPU 如何来执行这样的大型矩阵运算任务呢？一般 CPU 是基于冯诺依曼架构的通用处理器，这意味着 CPU 与软件和内存的运行方式如下

## CUDA

- CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言，这里我们选择CUDA C/C++接口对CUDA编程进行讲解。开发平台为Windows 10 + VS 2013，Windows系统下的CUDA安装教程可以参考[这里](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)
- CUDA编程模型支持的编程语言
    - ![](https://img-blog.csdn.net/2018031813244714?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- CUDA编程模型基础
    - 在给出CUDA的编程实例之前，这里先对CUDA编程模型中的一些概念及基础知识做个简单介绍。CUDA编程模型是一个异构模型，需要CPU和GPU协同工作。在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：
        - 分配host内存，并进行数据初始化；
        - 分配device内存，并从host将数据拷贝到device上；
        - 调用CUDA的核函数在device上完成指定的运算；
        - 将device上的运算结果拷贝到host上；
        - 释放device和host上分配的内存。
- 参考：[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

查看cuda版本, [参考](https://www.cnblogs.com/wuliytTaotao/p/11453265.html)

```sh
# 查看cuda版本
/usr/local/cuda/bin/nvcc -V
/usr/local/cuda/bin/nvcc --version
# 查看 cuDNN 版本
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
```

如果没有，可能没有安装 cuDNN。Linux 服务器上推荐使用 conda 安装，使用 conda 可以很方便安装 PyTorch/TensorFlow 以及对应版本的 CUDA 和 cuDNN。

用 PyTorch 查看 CUDA 和 cuDNN 版本
- NVIDIA cuDNN 是用于深度神经网络的GPU加速库。

```py
import torch

print('pytorch版本:', torch.__version__)
print('cuda版本: ', torch.version.cuda)
print('cuDNN版本: ', torch.backends.cudnn.version())
```

## GPU

Kaggle GPU 性能对比：使用GPU后提速**12倍**
- 用 ASL Alphabet 数据集训练模型为例，在 Kaggle Kernels 上用 GPU 的总训练时间为 994 秒，而此前用 CPU 的总训练时间达 13,419 秒。直接让你训练模型的时间缩短为原来的 1/12

### 行情

GPU市场上，NVIDIA占了大部分（**N卡**），AMD（**A卡**）次之，接着是**苹果**（好像是intel），详见[图](https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg)
- <img src="https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg" width = "400" height = "300" alt="设备对比" align=center />
- [2019显卡天梯图汇总](http://www.ppnames.com/html/360.html)
- ![](http://www.ppnames.com/img2019/20181017/102.png)

【2023-3-22】[英伟达发布ChatGPT专用GPU，推理速度提升了10倍](https://www.toutiao.com/article/7213170230963782201)
- 3 月 22 日，GTC 大会正式召开，在刚刚进行的 Keynote 上，英伟达 CEO 黄仁勋搬出了为 ChatGPT 准备的芯片
- 2012 年，计算机视觉模型 AlexNet 动用了 GeForce GTX 580，每秒可处理 262 PetaFLOPS。该模型引发了 AI 技术的爆炸
- 2017年，Transformer 出现了
- 2020年，GPT-3 动用了 323 ZettaFLOPS 的算力，是 AlexNet 的 100 万倍
- 2022年底，创造了 ChatGPT 这个震惊全世界的 AI。

崭新的计算平台出现了，AI 的 iPhone 时代已经来临。
- 当前唯一可以实际处理 ChatGPT 的 GPU 是英伟达 HGX `A100`。
- 针对算力需求巨大的 ChatGPT，英伟达发布了 NVIDIA `H100` NVL，具有 **94GB** 内存和加速 Transformer Engine 的大语言模型（LLM）专用解决方案，配备了双 GPU NVLINK 的 PCIE H100 GPU。与前者相比，现在一台搭载四对 `H100` 和双 NVLINK 的标准服务器速度能快 10 倍，可以将大语言模型的处理成本降低一个数量级

AI 的繁荣推动[英伟达股价](http://stock.finance.sina.com.cn/usstock/quotes/NVDA.html)在今年上涨了 77%，目前，英伟达的市值为 6400 亿美元，已是英特尔的近五倍。

### 显卡对比

【2022-11-02】[NVIDIA Tesla GPU系列P4、T4、P40以及V100显卡性能的对比](https://codeantenna.com/a/AyKWIZHwTX)

NVIDIA Tesla GPU系列P4、T4、P40以及V100，详见[京东店铺描述](https://item.jd.com/64885135126.html)

|显卡类型|特点|单精度性能(FP32)|半精度性能(FP16)|整数运算能力(INT8)|整数运算能力(INT4)|GPU显存|显存带宽|系统接口/外形规格|功率|硬件加速视频引擎|
|---|---|---|---|---|---|---|---|---|---|---|
|Tesla `P4`|适用于推理吞吐量服务器务器|5.5 TFLOPS|-|22 TOPS|-|<span style='color:blue'>8GB</span>|192GB/秒|PCI Express半高外形|50 W/75 W |1个解码引擎,2个编码引擎|
|Tesla `T4`|世界领先的推理加速器|8.1 TFLOPS|65 TFLOPS|130 TOPS|260 TOPS|<span style='color:blue'>16GB</span>|320GB/秒|PCI Express半高外形|70W|1个解码引擎，2个编码引擎|
|Tesla `P100`|-|-|-|-|-|<span style='color:blue'>24GB</span>|||||
|Tesla `P40`|适用于超高效、外扩型服|12 TFLOPS|-|47 TOPS|-|<span style='color:blue'>24GB</span>|346GB/秒|PCI Express双插槽全高外形|250 W|1个解码引擎，2个编码引擎|
|Tesla `VI00`|通用数据中心GPU|14 TFLOPS (PCIe)|112 TFLOPS (PCIe)|-|-|<span style='color:blue'>32/16GB</span> HBM2|900GB/秒|PCI Express双插槽全高外形 SXM2/NVLink|250 W (PCIe)<br>300 W (SXM2) |-|
|Tesla `VI00S`|通用数据中心GPU|16.4 TFLOPS|112 TFLOPS (PCIe)|-|-|<span style='color:blue'>32GB</span> HBM2|1134GB/秒|PCI Express双插槽全高外形|250 W |-|
|NVIDIA `A10`|-|-|-|-|-|24|||||
|NVIDIA `A100`|-|-|-|-|-|40/80|||||

【2022-11-26】Kaggle 免费 GPU
- 型号：GPU T4 * 2 或者 GPU P100， TPU v3-8
- ![gpu](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/7ecb9cc5897a429abf35df8f59871d80~tplv-obj:1854:1530.image?_iz=97245&from=post&x-expires=1684972800&x-signature=j1FWQ4ZPYN0ou57iIxkviyVSQb0%3D)

【2023-2-25】GPU中主要有三类基础运算：`整数`运算、`单精度浮点数`运算和`双精度浮点数`运算，其中
- `单精度浮点`运算速度最快而`双精度浮点`运算速度最慢
- `FLOPS`（floating-point operations per second, 每秒执行的**浮点运算次数**）也是衡量GPU运算性能的关键指标
- 如果一个程序内只有`单精度浮点数`运算，将发挥硬件的最大功效，因此应该尽量多使用单精度浮点数运算，而避免使用`双精度浮点`运算。

#### NVIDIA TESLA V100

NVIDIA Tesla V100采用NVIDIA Volta架构，非常适合为要求极为苛刻的双精度计算工作流程提供加速，并且还是从P100升级的理想路径。该GPU的渲染性能比Tesla P100提升了高达80%，借此可缩短设计周期和上市时间。

Tesla V100的每个GPU均可提供125 teraflops的推理性能，配有8块Tesla V100的单个服务器可实现1 petaflop的计算性能。阿里云GPU参考 dashi.aliyun.com/site/cloud/gpu 详细说明

#### NVIDIA TESLA P40

The Tesla P40能够提供高达2倍的专业图形性能。Tesla P40能够对组织中每个vGPU虚拟化加速图形和计算（NVIDIA CUDA® 和 OpenCL）工作负载。支持多种行业标准的2U服务器。

Tesla P40可提供出色的推理性能、INT8精度和24GB板载内存。

#### NVIDIA TESLA T4

NVIDIA Tesla T4的帧缓存高达P4的2倍，性能高达M60的2倍，对于利用NVIDIA Quadro vDWS软件开启高端3D设计和工程工作流程的用户而言，不失为一种理想的解决方案。凭借单插槽、半高外形特性以及低至70瓦的功耗，Tesla T4堪称为每个服务器节点实现最大GPU密度的绝佳之选。

#### NVIDIA TESLA P4


### GPU验证

① NVIDIA自带
- NVIDIA自带的驱动检测方法，能看到GPU的配置，可以看到Google Colab的GPU是Tesla T4，显存15G，强于上一版本K80 
- Kaggle GPU 性能对比：使用GPU后提速12倍
  - 用 ASL Alphabet 数据集训练模型为例，在 Kaggle Kernels 上用 GPU 的总训练时间为 994 秒，而此前用 CPU 的总训练时间达 13,419 秒。直接让你训练模型的时间缩短为原来的 12 分之一

#### nvidia-smi

```shell
#命令
nvidia-smi
# 结果如下：
Wed Jun  5 03:02:36 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   57C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

# 动态监控
watch -n 1 nvidia-smi

# ------ 检测GPU集群总体信息 -------
nvidia-smi --query-gpu=memory.used --format=csv # 检查使用的GPU
nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits
# 0, 10956, 11441                                                                                    
# 1, 0, 11441
nvidia-smi --query-gpu=index,name,uuid,serial --format=csv
# 0, Tesla K40m, GPU-d0e093a0-c3b3-f458-5a55-6eb69fxxxxxx, 0323913xxxxxx
# 1, Tesla K40m, GPU-d105b085-7239-3871-43ef-975ecaxxxxxx, 0324214xxxxxx
```

GPU监控
- 动态监控GPU使用率
- 按Ctrl+C退出监控
- [参考GPU进程查看管理](https://blog.csdn.net/Kaige_Zhao/article/details/79748079)

【2023-2-26】[PyTorch显存分配原理——以BERT为例](https://zhuanlan.zhihu.com/p/527143823)

nvidia-smi 看到的占用
- = CUDA 上下文 + pytorch 缓存区
- = CUDA 上下文 + 未使用缓存 + 已使用缓存

1. CUDA 上下文

PyTorch会在第一次cuda操作创建 CUDA 上下文，占用的大小和 CUDA 版本、硬件设备有关。
- 例如：V100，CUDA10.1上，CUDA 上下文共占用1053MB

2. PyTorch 缓存管理

Pytorch 内部有自己的缓存管理系统，能够加速显存分配。
- 使用 torch.cuda.<span style='color:blue'>memory_allocated()</span> 可以看到当前Tensor占用的显存
- 使用 torch.cuda.<span style='color:blue'>memory_reserved()</span> 可以看到总共占用的显存
- 使用 torch.cuda.<span style='color:blue'>empty_cache()</span> 清空未使用的缓存，但是已经使用的是不能释放的

只有一种情况需要使用 torch.cuda.empty_cache()，就是当你想要释放缓存以便让其他人也可以一起使用当前显卡，否则不需要调用这个方法。

这里用一个简单的例子解释一下缓存的概念：
- 定义一个4MB的 Tensor（一个long tensor占4byte）

```py
import torch
a = torch.zeros((1024,1024)).cuda() # 实际大小： 1024*1024=4M
print(torch.cuda.memory_allocated() / 1024 / 1024) # a 占用了 4m
print(torch.cuda.memory_reserved() / 1024 / 1024) # 缓存区大小 20m， 缓存区仍然占用了20M，nvidia-smi里也保持1073M的占用
del a # 清除张量
# --------
print(torch.cuda.empty_cache()) # 清空缓存
print(torch.cuda.memory_allocated()/1024/1024) 
print(torch.cuda.memory_reserved()/1024/1024)
```

3. 模型实际占用

这里分析一下模型训练，推理过程中，实际占用了多少显存。

训练阶段

前向传播：
>- 模型实际占用 = 模型参数 + 输入 + 输出 + 前向传播的中间变量
>- 模型参数：4.00390625MB = 1024*1024*4(weight)+1024*4(bias) = 4,198,400
>- 输入：40MB
>- 输出和前向传播的中间变量：4000MB

为了方便观察显存的变化，这里定义两个辅助函数

反向传播：
>- 模型实际占用 = 模型参数*2 + 输入 + 输出
>- 模型参数*2：4.00390625MB * 2 = 8.0078125 MB
>- 输入：40MB
>- 输出：40MB

这里将输出求和作为loss，然后求梯度

测试阶段
>- 模型实际占用 = 模型参数 + 输入向量 + 输出向量

1. torch.cuda.memory_allocated() 返回当前模型实际占用的显存。
2. torch.cuda.memory_reserved() 返回Pytorch占用的显存。
3. torch.cuda.memory_summary() 返回当前显存占用详细情况。

使用 torchinfo 可以查看模型共有多少参数，比直接print(model)的信息更全，可视化效果更好。
- 安装 pip install torchinfo

```py
from torchinfo import summary
from transformers import BertModel, BertConfig

model = BertModel(BertConfig()).to("cuda:0")
summary(model)
```

② GPU监控工具-[gpustat](https://www.ctolib.com/wookayin-gpustat.html)

- 仅适用于N卡（NVIDIA ），A卡不行（AMD）
- 效果示例：[图](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - ![](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - []里的数字是GPU编号，即常见的多机多卡里的“卡”

```shell
#pip install gpustat # 安装
echo "gpustat工具统计-`hostname`"
gpustat -cp # 检测
# 结果如下
528e504a63a3  Wed Jun  5 03:16:16 2019
#[0] Tesla T4         | 48'C,   0 % |     0 / 15079 MB |
# Google的Colab配置了Tesla T4的显卡一个，显存15G
# 动态监控
watch --color -n1 gpustat -cpu
```

③ Pytorch的GPU测试方法

```python
import torch

torch.cuda.is_available() # true
torch.backends.cudnn.enabled # true
```

④ Tensorflow的GPU测试代码

```python
import tensorflow as tf
device_name = tf.test.gpu_device_name()
print('Detect GPU:', device_name) # ('Detect GPU:', '/device:GPU:0')
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name)) # Found GPU at: /device:GPU:0
```



### GPU参数

GPU与CPU的不同
- GPU: **计算**密集型任务
- CPU: **I/O,** 逻辑运算

如果是多个GPU，需要注意参数：CUDA_VISIBLE_DEVICES, 限定CUDA程序所能使用的GPU设备

CUDA_VISIBLE_DEVICES [设置方法](https://www.jianshu.com/p/0816c3a5fa5c)
- （1）**永久**设置, 在环境变量中设置
  - linux环境：vim ~/.bash.rc，在最后添加上CUDA_VISIBLE_DEVICES=0，2，然后source ~/.bash.rc使之生效即可。
  - windows环境：直接添加CUDA_VISIBLE_DEVICES=0，2进环境变量
- （2）**临时**设置，在程序启动脚本中添加：
  - linux 环境：export CUDA_VISIBLE_DEVICES=0，2
  - windows环境: set CUDA_VISIBLE_DEVICES=0，2

```sh
# CUDA_VISIBLE_DEVICES设置说明，设置device对程序可见
CUDA_VISIBLE_DEVICES=1       # 仅使用device1 (即卡一)
CUDA_VISIBLE_DEVICES=0,1     # 仅使用device 0和 device1
CUDA_VISIBLE_DEVICES="0,1"	 # 同上, 仅使用device 0和 device1
CUDA_VISIBLE_DEVICES=0,2,3   # 仅使用device0, device2和device3
CUDA_VISIBLE_DEVICES=2,0,3   # 仅使用device0, device2和device3, 调整GPU序号映射关系
```

最后两条的区别是什么呢？
- CUDA_VISIBLE_DEVICES 后面的参数依次是设置 gpu\[0]，gpu\[1], gpu\[2]...等的device编号。
- 0,2,3 是gpu\[0]指向device0, gpu\[1]指向devcie2, gpu\[2]指向device3；
- 2,0,3 是gpu\[0]指向device2, gpu\[1], 指向devcie0, gpu\[2]指向device3；

再举例说明，如果当前主机有5张显卡，默认情况下5个device对程序都可以见，默认排序device0 - 4。
如果现在我们只希望使用第一张和第三张显卡，并且程序代码里看到的分别对应0，1。
那么设置应该如下：
CUDA_VISIBLE_DEVICES=0，2

- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)
- 通过tf.device指定运行设备，不管CPU多少个，一律标记为/cpu0，而GPU不同，分别是/gpu:1-n
- Tensorflow使用GPU时默认占满所有可用GPU的显存，但只在第一个GPU上进行计算
  - 多GPU时，只有一块GPU真正在工作，如果不加以利用，另一块GPU就白白浪费了
  - GPU是一种相对昂贵的计算资源，虽然正值矿难，相比之前动辄八九千一块1080Ti的价格低了不少，但也不是一般人能浪费的起的
- 如何有效提高GPU特别是tensorflow上的利用率就成为了一项重要的考量。
- 解决一：设置可见的GPU，CUDA_VISIBLE_DEVICES, 0表示GPU编号(0~n-1)

Shell代码：

```sh
export CUDA_VISIBLE_DEVICES=0
```
Python代码：

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
```

- 缓解tf占满整个GPU

```python
config = tf.ConfigProto(allow_soft_placement=True,allow_grouth=True)
config.gpu_options.per_process_gpu_memory_fraction = 0.9 #占用90%显存
sess = tf.Session(config=config)
```

- 解决二：有时候我们更需要利用好已有的卡，来或得线性的加速比，以便在更短的时候获取参考结果，上面的方法就无能无力，需要自己写代码实现多GPU编程
- 参考：[tensorflow 多GPU编程 完全指南](https://blog.csdn.net/minstyrain/article/details/80986397)


### GPU 如何工作

- 为了获得比 CPU 更高的吞吐量，GPU 使用一种简单的策略：在单个处理器中使用成千上万个 ALU。现代 GPU 通常在单个处理器中拥有 2500-5000 个 ALU，意味着同时执行数千次乘法和加法运算。

#### GPU为什么快

CPU，即 中央处理器，处理用户向计算机提出的每一条指令。
- 一个程序里包含的指令是十分复杂，不仅包括计算，还包括逻辑；如不但要计算1+1=2，还要判断if和else，甚至还要统筹计算机里的内存，硬盘等其他硬件
- 最终，CPU内部的要留大量的空间给控制单元和缓存，单纯的计算单元反而就占很小一块
- ![](https://pica.zhimg.com/80/v2-25b9113edbe0a06fbbaf0f5cef9ef1c0_1440w.webp?source=1940ef5c)
- CPU处理复杂逻辑的程序，比起其他种类硬件优势显著，但如果单纯拿CPU来计算，那内部有很多东西浪费了。

GPU就抛弃了复杂的控制模块。
- 诞生之初为了计算图形数据，图形数据没有太复杂的逻辑，都是点的坐标和各种空间计算公式
- 跑3D图形不需要很多的if和else，只有各种点的计算。所以，在仅仅保留了必要的一小部分控制单元后，GPU将绝大多数区域都让给了计算单元。
- ![](https://picd.zhimg.com/80/v2-7d44cad02926e8c3b8ec83e54c93dd47_1440w.webp?source=1940ef5c)

结果就是，GPU判断复杂的逻辑，很可能吃不消，但计算上的速度就远远领先CPU。

巧合的是，深度学习就需要大量的计算; GPU在跑深度学习这件事上，有着CPU数倍乃至数十倍的速度。
- ![](https://picx.zhimg.com/80/v2-15c2e1cf6f440cd7de4cef7ae252e934_1440w.webp?source=1940ef5c)

深度学习主要有训练、推理两个阶段
- ![](https://picd.zhimg.com/80/v2-2e9de41cf4c4232fabf99434074cf0a5_1440w.webp?source=1940ef5c)
- 训练过程需要大量的计算，因为要先将实例输入，计算出结果与答案比较，得到结果与答案的差异，再返回去优化，计算出迭代后的网络参数，来来回回折腾几遍，这个运算量就非常巨大。因此训练过程不论如何都需要专精计算的GPU来加速。
- 但推理过程就不一定只能靠GPU了，CPU一样可以参与，甚至在实际应用中，由于其他复杂逻辑的加入，CPU更加不可或缺。

所以说只要算法优化得好，CPU也可以很好地处理人工智能的运算工作。实际上各大CPU厂家也在不断提升CPU的AI计算能力。

作为处理器巨头，英特尔已经在自家的至强服务器处理器里集成了多种深度学习加速技术。
- 比如DL Boost技术，核心是通过降低数据精度来提升单个时钟周期内可以进行的运算次数，来加速计算。对于深度学习来说，降低精度对于推理结果的影响很小，提升硬件性能和效率，由此降低延迟更加重要，所以这种优化方法能在不影响结果的同时大幅度加速计算
- 再比如 one API，里面集成了TensorFlow、Apache MXNet、PyTorch等多种 AI 框架和库，并且针对DL Boost和至强处理器做了专门的优化，有着更高的运行效率。
- 在一系列优化加速下，尽管速度还不能说反超了GPU，但相较于之前，英特尔至强处理器的AI处理能力已经有了质的飞跃。

详见：[CPU和GPU跑深度学习差别有多大？](https://www.zhihu.com/question/273812506/answer/2764350102)

### Parallelism-GPU并行

- There are two types of parallelism:
- **模型**并行，Model parallelism - Different GPUs run different part of the code. Batches of data pass through all GPUs.
    - 适用于大规模
- **数据**并行，Data parallelism - We use multiple GPUs to run the same TensorFlow code. Each GPU is feed with different batch of data. 每个节点是各有一份模型copy，称为tower
    - 同步+异步，同步适用于设备差异不大，小数据，异步抖动，适用大数据
    - 图内+图间

## TPU

资料
- 【2019-08-11】[TPU灵魂三问：What？Why？How？](https://www.sohu.com/a/333008988_505915)
- [TPU是如何超越GPU，成为深度学习首选处理器的](https://www.toutiao.com/a6596935089408442883/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1536065103&app=news_article&utm_source=mobile_qq&iid=42461222229&utm_medium=toutiao_android&group_id=6596935089408442883)

### 介绍

- **张量处理单元**（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载。TPU 为谷歌的主要产品提供了计算支持，包括翻译、照片、搜索助理和 Gmail 等。Cloud TPU 将 TPU 作为可扩展的云计算资源，并为所有在 Google Cloud 上运行尖端 ML 模型的开发者与数据科学家提供计算资源
- 在 Google Next’18 中，我们宣布 TPU v2 现在已经得到用户的广泛使用，包括那些免费试用用户，而 TPU v3 目前已经发布了内部测试版。

- 谷歌设计 TPU 时，构建了一种领域特定的架构。这意味着，没有设计一种通用的处理器，而是专用于神经网络工作负载的矩阵处理器。TPU 不能运行文本处理软件、控制火箭引擎或执行银行业务，但它们可以为神经网络处理大量的乘法和加法运算，同时 TPU 的速度非常快、能耗非常小且物理空间占用也更小。
- 其主要助因是对冯诺依曼瓶颈的大幅度简化。因为该处理器的主要任务是矩阵处理，TPU 的硬件设计者知道该运算过程的每个步骤。因此他们放置了成千上万的乘法器和加法器并将它们直接连接起来，以构建那些运算符的物理矩阵。这被称作脉动阵列（Systolic Array）架构。在 Cloud TPU v2 的例子中，有两个 128X128 的脉动阵列，在单个处理器中集成了 32768 个 ALU 的 16 位浮点值。
- 我们来看看一个脉动阵列如何执行神经网络计算。首先，TPU 从内存加载参数到乘法器和加法器的矩阵中。

### 实践

参考：[Colab提供了免费TPU，机器之心帮你试了试](https://www.jiqizhixin.com/articles/2018-10-11-5)

开启笔记本的GPU开关
- 操作：修改→笔记本设置→勾选GPU

Colab环境
- 查看配置信息
- 输出 TPU 地址及 TPU 设备列表 ，表名配置了TPU资源，否则会报错
- Colab 为「TPU 运行时」分配 CPU 和 TPU，其中分配的 TPU 工作站有八个核，后面配置的 TPU 策略会选择 8 条并行 shards

测试代码

```python
"""在TPU运行时下测试有没有分配TPU计算资源"""
import os
import pprint
import tensorflow as tf

print("Colab环境信息")
print(os.environ)
# {'COLAB_TPU_ADDR': '10.68.94.74:8470', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'CLOUDSDK_CONFIG': '/content/.config',
#  'CUDA_VERSION': '10.0.130', 'PATH': '/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin', 'HOME': '/root', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64',
#  'LANG': 'en_US.UTF-8', 'SHELL': '/bin/bash', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'CUDA_PKG_VERSION': '10-0=10.0.130-1',
#  'SHLVL': '1', 'NCCL_VERSION': '2.4.2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 
#  'DEBIAN_FRONTEND': 'noninteractive', 'CUDNN_VERSION': '7.5.0.56', 'JPY_PARENT_PID': '37', 'PYTHONPATH': '/env/python',
# 'DATALAB_SETTINGS_OVERRIDES': '{"kernelManagerProxyPort":6000,"kernelManagerProxyHost":"172.28.0.3","jupyterArgs":["--ip=\\"172.28.0.2\\""]}',
# 'NO_GCE_CHECK': 'True', 'GLIBCXX_FORCE_NEW': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', '_': '/tools/node/bin/forever', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.0 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=410,driver<411', 'OLDPWD': '/', 'HOSTNAME': '59881c4943e6', 'ENV': '/root/.bashrc', 'COLAB_GPU': '0', 'PWD': '/', 'XRT_TPU_CONFIG': 'tpu_worker;0;10.68.94.74:8470', 'GLIBCPP_FORCE_NEW': '1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'TPU_NAME': 'grpc://10.68.94.74:8470', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}

# 检查GPU是否启用
if 'COLAB_TPU_ADDR' not in os.environ:
  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')
else:
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print ('TPU address is', tpu_address)

  with tf.Session(tpu_address) as session:
    devices = session.list_devices()
    
  print('TPU devices:')
  pprint.pprint(devices)
```

TPU Hello world
- TPU在contrib中，tf.contrib.tpu, 从一个简单的代码开始

```python
import tensorflow as tf 
import numpy as np
import timeit
​
tf.reset_default_graph()
img = np.random.randn(128, 256, 256, 3).astype(np.float32)
w = np.random.randn(5, 5, 3, 256).astype(np.float32)
conv = tf.nn.conv2d(img, w, [1,2,2,1], padding='SAME')
#=======================
with tf.Session() as sess:
    # with tf.device("/gpu:0") as dev:
    %timeit sess.run(conv)
#输出：loop, best of 3: 2.32 s per loop
#========================   
# 指定gpu环境
# with tf.Session() as sess:
#     with tf.device("/gpu:0") as dev:
#         %timeit sess.run(conv)
#输出：loop, best of 3: 2.34 s per loop
```

### TPU对比

思考
- 分别选择 CPU、GPU 和 TPU 作为运行时状态，运行代码并迭代一次耗时为：2.44 s、280 ms、2.47 s

|设备|耗时|备注|
|---|---|---|
|CPU|2.44s|-|
|GPU|280ms|-|
|TPU|2.47s|-|

- 使用tpu和gpu都没有差异吗？
- 原因：以上代码并未启用TPU或GPU
   - 启用TPU 似乎需要特定的函数与运算，不像 CPU 和 GPU 可以共用代码。
   - 仅修改运行时状态，并不会真正调用 TPU 资源，实际运行的还是 CPU
-  TF 存在一个神奇的类 tf.contrib.tpu，调用 TPU 资源必须用它改写模型
-  tf.contrib.tpu 类提供了两种使用 TPU 的简单方法
   - 直接使用 Keras 接口
      - tf.contrib.tpu.keras_to_tpu_model 方法可以直接将 Keras 模型与对应的权重复制到 TPU，并返回 TPU 模型
   - 用 TPUEstimator 构建模型
       -  tf.contrib.tpu.TPUEstimator
       - TPUEstimator 类继承自 Estimator 类，因此它不仅支持在 TPU 上运算，同时还支持 CPU 和 GPU 的运算，更方便
修正TPU代码
- 结果意外，卷积运算每一次迭代只需要 1.22 ms。
- 如下图所示，很可能存在变量缓存等其它因素造成了一定程度的缓慢，但 TPU 的速度无可置疑地快，就是需要修改代码
- ![](https://image.jiqizhixin.com/uploads/editor/4b93c3b3-e368-473a-aa0f-f7c6eb62d8f7/1539240581969.png)

```python
import numpy as np

def add_op(x, y):
  return x + y
  
x = tf.placeholder(tf.float32, [10,])
y = tf.placeholder(tf.float32, [10,])
# tpu选项在contrib中
tpu_ops = tf.contrib.tpu.rewrite(add_op, [x, y])

session = tf.Session(tpu_address)
try:
  print('Initializing...')
  session.run(tf.contrib.tpu.initialize_system())
  print('Running ops')
  print(session.run(tpu_ops, {x: np.arange(10), y: np.arange(10)}))
finally:
  # For now, TPU sessions must be shutdown separately from
  # closing the session.
  session.run(tf.contrib.tpu.shutdown_system())
  session.close()
```

### 总结

- Colab 提供的 TPU 要比 GPU 快 3 倍左右，一般 TPU 训练 5 个 Epoch 只需要 40 多秒，而 GPU 需要 2 分多钟。
- Colab 确实提供了非常强劲的免费 TPU，而且使用 Keras 或 TPUEstimator 也很容易重新搭建或转换已有的 TensorFlow 模型

# GPU环境准备

## 测试：Colab实现

- 【2019-06-05】[colab笔记代码](https://colab.research.google.com/drive/1uqhBy1zNF7uU7EdrQqwlMSoykgxvubyg)
- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)

### tf 2 版本

参考：[Tensorflow如何使用GPU训练](https://blog.csdn.net/qq_31554953/article/details/107302404)

```python
import tensorflow as tf

tf.debugging.set_log_device_placement(True)

# Create some tensors, 自动选择gpu
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print('结果:', c)
# Place tensors on the CPU # 指定gpu
with tf.device('/CPU:0'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print('结果:', c)
# TensorFlow默认映射该进程可见的几乎所有GPU的所有GPU内存（取决于CUDA_VISIBLE_DEVICES）。 
# 这样做是为了通过减少内存碎片来更有效地使用设备上相对宝贵的GPU内存资源。 
# 要将TensorFlow限制为一组特定的GPU，用tf.config.experimental.set_visible_devices方法。
gpus = tf.config.experimental.list_physical_devices('GPU')
print('物理可用gpu: ', gpus)
if gpus:
  # Restrict TensorFlow to only use the first GPU
  try:
    tf.config.experimental.set_visible_devices(gpus[0], 'GPU') # 限制可用gpu
    # 限制各GPU内存增速
    # Currently, memory growth needs to be the same across GPUs
    # ① 方法一：
    #       使用set_memory_growth
    #       或：环境变量TF_FORCE_GPU_ALLOW_GROWTH设置为true
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    # ② 方法二：虚拟GPU设备，set_virtual_device_configuration
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
    # 显示逻辑可用GPU
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(f"物理Physical GPUs: {len(gpus)}, 逻辑Logical GPUs: {len(logical_gpus)}")
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)
```

### tf 1 版本

```python
import tensorflow as tf
# ---- tf 1.*版本 -----
import tensorflow.compat.v1 as tf 
tf.compat.v1.disable_eager_execution() # 关闭即时执行模式

import os

#GPU:计算密集型任务, CPU: I/O, 逻辑运算
if tf.test.is_gpu_available(): # 检测GPU是否可用
    print('GPU可用: %s'%(tf.test.gpu_device_name()))
# 环境变量设置，默认有限选择GPU，占用所有GPU的所有显存
config = tf.ConfigProto()
os.environ['TF_CPP_MIN_LOG_LEVEL']='5'#日志级别
#----自定义使用哪些GPU----
os.environ["CUDA_VISIBLE_DEVICES"] = "2"#只使用第三块GPU，默认选用id最小的gpu
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"#只使用第2-3块GPU
#----自定义显存分配----
config.gpu_options.allow_growth = True#刚开始分配较少内存，逐步按需分配，注意：系统不自动释放内存，以免可能导致更严重的内存碎片情况
config.gpu_options.per_process_gpu_memory_fraction = 0.4#按比例分配,40%，方便多任务并行

a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')
b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')
c = a + b
#通过tf.device指定运行设备。不管CPU多少个，一律标记为/cpu0,而GPU不同，分别是/gpu:1-n
#注意：不宜包含过多设备约束，不同设备实现方式不同，多了会限制移植能力
with tf.device('/gpu:0'):
    #将tf.Variable强制放在GPU上会报错,GPU只在部分数据类型上支持tf.Variable操作（variable_ops.cc）
    #除非启动自动更改设备的参数allow_soft_placement
    a_gpu = tf.Variable(0, name="a_gpu")
# 通过log_device_placement参数来输出运行每一个运算的设备。
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# 通过allow_soft_placement参数自动将无法放在GPU上的操作放回CPU上。
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
#sess.run(tf.initialize_all_variables())
sess.run(tf.global_variables_initializer())
print(sess.run(c))
#jupyter中显示不出来
```

没GPU时的结果：

```shell
2018-03-26 19:29:38.606679: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Device mapping: no known devices.
2018-03-26 19:29:38.606972: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
add: (Add): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610106: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610137: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610142: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:CPU:0
[ 2.  4.  6.]
```

有GPU时的结果：

```
2018-03-26 19:27:46.864593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 22.38GiB freeMemory: 745.94MiB
2018-03-26 19:27:46.864673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
2018-03-26 19:27:47.193611: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194462: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194498: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194526: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
[2. 4. 6.]
```

# 分布式

- 【2022-2-23】贝壳，onnxruntime优化过后的bert模型，cpu推理延迟能从300ms降到100ms以内
- 【2021-10-13】[OpenAI 研究员最新博客：如何在多GPU上训练真正的大模型？](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247504041&idx=1&sn=a6a8ceaf1cb091d7832351bcddae6ffb&chksm=febc936dc9cb1a7bbcdeef42f304107d7fe221e7999f2a1a508c6164267dc12dd12ee29ad0eb&mpshare=1&scene=23&srcid=1013pNjTo5fSHOxkjfW5JoFs&sharer_sharetime=1634137253709&sharer_shareid=b8d409494a5439418f4a89712efcd92a#rd)，[原文链接](lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)
- 单个GPU卡的内存有限，许多大模型的大小已经超过了单个GPU，为解决此类问题，训练深且大的神经网络的主要方法有训练**并行**加速、各种模型**架构**以及内存**节省**设计等。
  - （1）并行加速方法有以下几种：
    - **数据**并行性：将相同的模型权重复制到多个worker中，并将一部分数据分配给每个worker以同时进行处理。
    - **模型**并行性
    - **流水线**并行
    - **张量**并行
  - （2）模型架构方面主要有专家混合（MoE）方法。
  - （3）节省内存的设计方法，如：CPU卸载、激活重新计算、混合精度训练、压缩以及内存高效优化器等等。

## 分布式训练范式

大规模深度学习模型训练中有几个主要范式：
- `数据并行`(DP)：模型尺寸能够被单个GPU 内存容纳，模型的不同实例在不同的 GPU 和不同批数据上运行，模型的每个实例都使用相同的参数进行初始化，但在前向传递期间，不同批次的数据被发送到每个模型。 收集来自每个模型实例的梯度并计算梯度更新。，然后更新模型参数并将其作为更新发送到每个模型实例。
  - ![](https://pic4.zhimg.com/80/v2-de60ad9dffd68d827084d84772b06dbb_720w.webp)
  - ![](https://pic4.zhimg.com/80/v2-b508d84ba9c6a9c6ae2c5be70526da43_1440w.webp)
  - 数据并行通过在 N 台机器上复制模型来实现。拆分 minibatch ，分成 N 个块，让每台机器处理一个块。
  - ![](https://pic3.zhimg.com/80/v2-678f7d2c116f7528be27d6445b6c091a_1440w.webp)
- `模型并行`：当单个 GPU无法容纳模型尺寸时，**模型并行性**变得必要，有必要将模型拆分到多个 GPU 上进行训练。实现模型尺寸超过单个GPU显存的深度学习模型训练。 
  - 这种方法的问题是计算使用效率不高，因为在任何时间点只有一个 GPU 正在使用，而其他 GPU 处于空闲状态。
  - ![](https://pic3.zhimg.com/80/v2-6a4304b529130e86e4552b3d4ed58a4e_720w.webp)
  - 相对于流水线并行和数据并行，模型并行具有以下优点：
    - 支持更大的模型规模：流水线并行和数据并行的限制通常是 GPU 内存大小和 GPU 数量，而模型并行可以支持更大的模型规模，因为模型可以分割成多个子模型，并分配到多个 GPU 上运行。
    - 减少通信开销：流水线并行的模型划分通常会导致模型层之间的通信，而模型并行只需在每个子模型之间进行通信。相对于数据并行，模型并行在执行过程中通信量更少，因为每个 GPU 只需传递模型的一部分而不是全部。
    - 灵活的模型分配：模型并行可以更灵活地将模型分配给不同的 GPU 或计算节点，这意味着可以在不同的 GPU 上运行不同的模型子集，从而实现更好的负载平衡和性能优化。
- `流水线并行` (PP)
  - 朴素流水线并行（Naive Pipeline Parallelism）是将一组模型层分布在多个 GPU 上，并简单地将数据从 GPU 移动到 GPU，就好像它是一个大型复合 GPU 一样。
  - 流水线并行 (PP) 与上述朴素流水线并行几乎相同，但它解决了 GPU 闲置问题，方法是将传入的 batch 为 micro-batches 并人工创建流水线，从而允许不同的 GPU 同时参与计算过程。
  - 流水并行是将一个大型计算任务拆分成多个小的**子任务**，并将子任务在多个处理单元上同时执行。不同于数据并行和模型并行，流水并行不是将数据或模型分割成多个部分并在处理单元间并行处理，而是将一系列计算步骤分解成多个流水阶段，并在多个处理单元上同时执行，以减少总体计算时间。

### 数据并行

数据并行性（Data parallelism (DP)）最简单的方法是：将相同的**模型权重**复制到多个worker中，并将一部分数据分配给每个worker以同时进行处理。
- 如果模型规模大于单个GPU的内存，Naive DP无法正常工作时。GeePS（Cui 等人，2016 年）之类的方法将暂时未使用的参数卸载回 CPU，以使用有限的 GPU 内存。数据交换传输在后端进行，且不干扰训练计算。
 
在每个小批量结束时，workers需要同步梯度或权重，以替换旧参数。常见有两种主要的同步方法，它们都有明确的优缺点：
- 1）大容量**同步**并行（ Bulk synchronous parallels (BSP)）：workers在每个小批量结束时同步数据。这种方法可以防止模型权重过时，同时获得良好的学习效率，但每台机器都必须停止并**等待**其他机器发送梯度。
- 2）**异步**并行（Asynchronous parallel (ASP)）：每个GPU工作进程异步处理数据，无需等待或暂停。然而，这种方法很容易导致网络使用陈旧的权重参数，从而**降低**统计学习效率。即使它增加了计算时间，也可能不会加快收敛的训练时间。
 
中间的某个地方是在每次x迭代时，全局同步梯度（x＞1）。自Pytorch v1.5版（Li等人，2021年）以来，该特征在平行分布数据（DDP）中被称为“梯度累积”。Bucket 梯度计算方法避免了梯度的立即AllReduce，而是将多个梯度变化值存储到一个AllReduce中以提高吞吐量，可以基于计算图进行计算和通信调度优化。

### 模型并行（大模型）

模型并行性（Model parallelism: MP）目的是解决**模型权重不能适应单个节点**的情况，通过将计算和模型参数分布在多台机器上进行训练。在数据并行中，每个worker承载整个模型的**完整副本**，而在MP中，每个worker上只分配模型参数的一小部分，从而减少了内存使用和计算。

由于深度神经网络通常包含一堆垂直层，因此将一个大型模型逐层拆分感觉很简单，其中一组连续的小层被分组到一个工作层上的一个分区中。然而，通过多个具有顺序依赖性的工作线程来运行每个数据批，会导致大量的**等待时间**和计算资源**利用率低下**的问题。

### 流水线并行（综合模型+数据）

通道并行（Pipeline parallelism: PP）将模型并行与数据并行相结合，以减少部分训练过程中出现的空闲时间。其主要思想是将一个小批量拆分为多个**微批次**，并使worker在每个阶段中能够同时处理一个微批次。需要注意的是，每个微批次需要**两次传递**，一次向前，一次向后。worker之间的通信仅传输激活（向前）和梯度（向后）。这些通道的调度方式以及梯度的聚合方式在不同的方法中有所不同。分区（workers）的数量也称为通道深度。

### 张量并行

模型并行和流水线并行都将一个模型垂直分割，可以将一个张量操作的计算水平分割到多个设备上，称为**张量并行**（tensor parallelism，TP）。
 
以当下比较流行的transformer为例，transformer模型主要由多层MLP和自我注意块组成。Megatron-LM（Shoeybi et al.2020）等人采用了一种简单的方法来并行多层计算MLP和自我注意。变压器中的MLP层包含GEMM（通用矩阵乘法）和非线性GeLU传输，按列拆分权重矩阵A


## 分布式训练库

目前开源的 模型库 主要是 NVIDIA 的 `Megatron-LM` 和微软的 [DeepSpeed](https://www.deepspeed.ai/)。

`Megatron` 和 `DeepSpeed` 都是基于 `PyTorch` ，分别由 `NVIDIA` 和`微软`经过深度定制开发，专门为支持 PyTorch 分布式训练 GPT 而设计的。

### DeepSpeed -- 微软

- [DeepSpeed](https://www.deepspeed.ai/) is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- DeepSpeed trained the world’s most powerful language models (`MT-530B`, `BLOOM`)
- 微软的 `DeepSpeed` 模型并行等内核取自 `Megatron` ，且 DeepSpeed 主打在数据并行下如何以更少的机器去跑更大的模型 （ ZeRO 、 ZeRO-Offload 等都是用梯度切片、计算、内存/硬盘换入换出来省显存）

`NVIDIA` 的 `Megatron` 和 微软的 `DeepSpeed`：
- DeepSpeed 本质上是一种“节省显存”的数据并行，即：数据并行的优化版。
  - DeepSpeed 假设了单层参数量可以在单张显卡上放得下，如果不满足这个假设，那么仍然需要使用模型并行，而且 DeepSpeed 的模型并行是通过调用 Megatron 来实现的。
  - 根据 NVIDIA 最新的那篇[论文](https://arxiv.org/abs/2104.04473)，`Megatron` 在大规模训练的效率是超过 `DeepSpeed` 不少的。
  - DeepSpeed 的论文一直强调：可以用更少机器训练更大的模型，但没有突出过在效率上的优势。
  - DeepSpeed 后来又出了一篇论文：[ZeRO-Infinity](https://arxiv.org/abs/2104.07857)，当单层参数量在单张显卡上放不下的时候，它通过对这一层算子切片，一片一片来执行，使得单卡也能跑起来一个巨大的层，可以理解成一种 “时间”轴上展开的模型并行。

### Megatron-LM -- NVIDIA

[Megatron](https://github.com/NVIDIA/Megatron-LM) is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. This repository is for ongoing research on training large transformer language models at scale. We developed efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.

### Megatron-DeepSpeed

`Megatron-DeepSpeed` 结合了两种主要技术：
- `DeepSpeed` 是微软开发的深度学习优化库，它使分布式训练变得简单、高效和有效。
- `Megatron-LM` 是由 `NVIDIA` 的应用深度学习研究团队开发的大型、强大的 **Transformer 模型框架**。

DeepSpeed 团队通过将 `DeepSpeed` 库中的 `ZeRO 分片`（ZeRO sharding）和`管道并行`（pipeline parallelism）与 `Megatron-LM` 中的`张量并行`（Tensor Parallelism）相结合，开发了一种基于 3D 并行的实现。

`Megatron-DeepSpeed` 实施 3D 并行以可以让大型模型以非常有效的方式进行训练。
- `DataParallel` (DP) - 相同的初始化模型被复制多次，并且每次都被馈送 minibatch 的一部分。处理是并行完成的，所有设置在每个训练步骤结束时进行同步。
- `TensorParallel` (TP) - 每个张量都被分成多个块，因此不是让整个张量驻留在单个 GPU 上，而是张量的每个分片都驻留在其指定的 GPU 上。在处理过程中，每个分片在不同的 GPU 上分别并行处理，最终结果在步骤结束时同步。这也被称作横向并行。
- `PipelineParallel` (PP) - 模型在多个 GPU 上垂直（层级）拆分，因此只有模型的一个或多个层放置在单个 GPU 上。每个 GPU 并行处理管道的不同阶段，并处理一小部分批处理。
- `零冗余优化器` (ZeRO) - 也执行与 TP 有点类似的张量分片，除了整个张量会及时重建以进行前向或反向计算，因此不需要修改模型。它还支持各种卸载技术以补偿有限的 GPU 内存。

各个技术细节参考：[大型语言模型(LLM)训练指南](https://zhuanlan.zhihu.com/p/611325149)

### 总结

Megatron-DeepSpeed 实施 3D 并行以可以让大型模型以非常有效的方式进行训练。
- DataParallel (`DP`) - 相同的初始化模型被复制多次，并且每次都被馈送 minibatch 的一部分。处理是并行完成的，所有设置在每个训练步骤结束时进行同步。
- TensorParallel (`TP`) - 每个张量都被分成多个块，因此不是让整个张量驻留在单个 GPU 上，而是张量的每个分片都驻留在其指定的 GPU 上。在处理过程中，每个分片在不同的 GPU 上分别并行处理，最终结果在步骤结束时同步。这也被称作横向并行。
- PipelineParallel (`PP`) - 模型在多个 GPU 上垂直（层级）拆分，因此只有模型的一个或多个层放置在单个 GPU 上。每个 GPU 并行处理管道的不同阶段，并处理一小部分批处理。
- 零冗余优化器 (`ZeRO`) - 也执行与 TP 有点类似的张量分片，除了整个张量会及时重建以进行前向或反向计算，因此不需要修改模型。它还支持各种卸载技术以补偿有限的 GPU 内存。

训练超大规模语言模型主要有两条技术路线：
- TPU + XLA + TensorFlow/JAX
- GPU + PyTorch + Megatron-LM + DeepSpeed
- 前者由Google主导，由于TPU和自家云平台GCP深度绑定，对于非Googler来说， 只可远观而不可把玩
- 后者背后则有NVIDIA、Meta、MS大厂加持，社区氛围活跃，也更受到群众欢迎。

Deepspeed 是微软的大规模分布式训练工具。专门用于训练超大模型。
- [大模型的训练工具（1）---Deepspeed](https://zhuanlan.zhihu.com/p/609865550)
- `DP`+`PP`: DeepSpeed 将 DP 与 PP 结合起来
  - ![](https://pic1.zhimg.com/80/v2-127d807df8f6efc7b1f8cb6d5ff38620_1440w.webp)
- `DP`+`PP`+`TP`: 为了获得更高效的训练，PP 与 TP 和 DP 相结合，称为 3D 并行性
  - ![](https://pic1.zhimg.com/80/v2-7951815d9ab95beedf1d238bc58e73f0_1440w.webp)
- ZeRO DP+PP+TP: DeepSpeed 的主要功能之一是 ZeRO，它是 DP 的超级可扩展扩展。
- 【2023-3-16】[大型语言模型(LLM)训练指南](https://zhuanlan.zhihu.com/p/611325149)

增加的功能主要有：
- 3个维度并行化实现万亿参数模型训练
- ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型
- 通过 DeepSpeed Sparse Attention 用6倍速度执行10倍长的序列
- 1 比特 Adam 减少 5 倍通信量

3D 并行：扩展至万亿参数模型

3D 并行同时解决了训练万亿参数模型的两个基本挑战：显存效率和计算效率。因此，DeepSpeed 可以扩展至在显存中放下最巨大的模型，而不会牺牲速度。
- 显存效率：集群上所能训练的LLM的参数量。
- 计算效率：单纯计算占系统的开销的比例。

（1）**数据并行**是分布式训练普遍使用的技术。

在该技术中，每批输入的训练数据都在数据并行的 worker 之间平分。反向传播后需要通信并规约梯度，以保证优化器在各个 worker 上进行相同的更新。数据并行性具有几个明显的优势，包括计算效率高和实现起来工作量小。但是，数据并行的 batch 大小随 worker 数量提高，而我们往往无法在不影响收敛性的情况下一直增加 batch 大小。
- 显存效率：数据并行会在所有 worker 之间进行模型和优化器的复制，因此显存效率不高。DeepSpeed 开发了 ZeRO ，它是一系列用于提高数据并行的显存效率的优化器。 这项工作依赖于 ZeRO 的 1 阶段，该阶段在 worker 之间划分优化器状态量以减少冗余。
- 计算效率：随着我们提高并行度，每个 worker 执行的计算量是恒定的。数据并行可以在小规模上实现近乎线性扩展。但是，在 worker 之间规约梯度的通信开销跟模型大小成正相关，所以当模型很大或通信带宽很低时，计算效率会受限。。梯度累积是一种用来均摊通信成本的一种常用策略。它会进一步增加batch大小，在本地使用 micro-batch 多次进行正向和反向传播积累梯度后，再进行梯度规约和优化器更新。

（2）**模型并行**是包含范围很广的一类技术。

它会在多个 worker 之间划分模型的各个层。就其本质而言，模型并行性的计算和通信因模型结构而异，因此在实现上有很大的工作量。DeepSpeed 借用了英伟达的 Megatron-LM 来为基于 Transformer 的语言模型提供大规模模型并行功能。模型并行会根据 worker 数量成比例地减少显存使用量，也是这三种并行度中显存效率最高的。但是其代价是计算效率最低。
- 显存效率：模型并行会根据 worker 数量成比例地减少显存使用量。至关重要的是，这是减少单个网络层的激活显存的唯一方法。DeepSpeed 通过在模型并行 worker 之间划分激活显存来进一步提高显存效率。
- 计算效率：由于每次前向和反向传播中都需要额外通信激活值，模型并行的计算效率很低。模型并行需要高通信带宽，并且不能很好地扩展到通信带宽受限的节点。此外，每个模型并行worker 都会减少每个通信阶段之间执行的计算量，从而影响计算效率。模型并行性通常与数据并行性结合使用，以在内存和计算效率之间进行权衡。

（3）**流水线并行**训练引擎也被包含在了这次发布的DeepSpeed中

流水线并行将模型的各层划分为可以并行处理的阶段。当一个阶段完成一个 micro-batch 的正向传递时，激活内存将被通信至流水线的下一个阶段。类似地，当下一阶段完成反向传播时，将通过管道反向通信梯度。必须同时计算多个 micro-batch 以确保流水线的各个阶段能并行计算。目前已经开发出了几种用于权衡内存和计算效率以及收敛行为的方法，例如 PipeDream。DeepSpeed 采用的方法是通过梯度累积来实现并行，并保持与传统数据并行和模型并行训练在相同的总 batch 大小下收敛情况相同。
- 显存效率：流水线并行减少的显存与流水线的阶段数成正比，使模型的大小可以随 worker 的数量线性扩展。但是，流水线并行不会减少每一层的激活函数的显存占用量。此外，每个 worker 必须存储同时运行的各个 micro-batch 的激活值。这导致流水线第一阶段的激活内存与单个 mirco batch 的总激活内存大致相同。一个万亿参数模型将需要为一个 micro batch 提供大约 19 GB 的显存的激活内存，这几乎占到新推出的英伟达 A100 GPU 总显存的一半。
- 计算效率：流水线并行具有最低的通信量，因为它的通信量只和在各阶段边界的各层的激活值大小成正比。但是，它不能无限扩展。像模型并行一样，增加流水线大小会减少每个流水线阶段的计算量，这会降低计算与通信的比率。如果要实现好的计算效率，流水线并行还要求其每个阶段的计算负载完美的均衡。


## 模型架构

**专家混合**（MoE）方法最近吸引了很多关注，因为研究人员（主要来自谷歌）试图突破模型大小的限制。该想法的核心是整合学习：多个弱学习模型组合以后会形成能力出众的学习模型。

Shazeer 等人于2017年发表了名为“稀疏门控专家混合”（MoE）层的文章，提出了在一个深度神经网络中可以通过连接多个专家的门控机制来实现输出控制的方法。

## 内存优化

### CPU卸载
 
当GPU内存已满时，一种选择是将暂时未使用的数据卸载到CPU，并在以后需要时将其读回（Rhu等人，2016）。数据卸载到CPU 的想法很简单，但由于它会延长训练时间，所以近年来不太流行。
 
### 激活重新计算
 
激活重新计算（Activation recomputation (also known as “activation checkpointing” or “gradient checkpointing”，Chen等人，[2016年](https://arvix.org/abs/1604.06174)）是一个以计算时间为代价减少内存占用的聪明而简单的想法

### 混合精度训练
 
Narang&Micikevicius等人（2018年）介绍了一种使用半精度浮点（FP16）数字训练模型而不损失模型精度的方法。


三种避免以半精度丢失关键信息的技术：
- 1）全精度原始权重。维护累积梯度的模型权重的全精度 (FP32) 副本， 对于向前和向后传递，数字四舍五入到半精度。主要是为了防止每个梯度更新（即梯度乘以学习率）可能太小而无法完全包含在 FP16 范围内（即 2-24 在 FP16 中变为零）的情况。
- 2）损失缩放。扩大损失以更好地处理小幅度的梯度（见图 16）， 放大梯度有助于将权重移动到可表示范围的右侧部分（包含较大值）占据更大的部分，从而保留否则会丢失的值。
- 3）算术精度。对于常见的网络算法（例如向量点积，向量元素相加减少），可以将部分结果累加到 FP32 中，然后将最终输出保存为 FP16，然后再保存到内存中。可以在 FP16 或 FP32 中执行逐点操作。

### 压缩
 
中间结果通常会消耗大量内存，尽管它们只在一次向前传递和一次向后传递中需要。这两种使用之间存在明显的时间差距。因此Jain等人（2018年）提出了一种数据编码策略，将第一次使用后的中间结果在第一次传递中进行压缩，然后将其解码回来进行反向传播。

### 内存高效优化器
 
优化器内存消耗。以流行的 Adam 优化器为例，它内部需要保持动量和方差，两者都与梯度和模型参数处于同一规模，但是需要节省 4 倍的模型权重内存。

# 分布式机器学习

【2022-6-2】[分布式机器学习](https://zhuanlan.zhihu.com/p/365662727)
- ![](https://pic1.zhimg.com/v2-8e4eefe63cc256d4420a881a00f2851f_1440w.jpg)

在深度学习时代，训练数据特别大的时候想要**单卡**完成训练基本是不可能的。所以就需要进行**分布式**深度学习。

## 基本原理

无论哪种机器学习框架，分布式训练的基本原理都是相同的。可以从**并行模式**、**架构模式**、**同步范式**、**物理架构**、**通信技术**等五个不同的角度来分类。

更多信息见优质paper，把 DP(Data Parallel)、MP(Model Parallel)、PP(Pipeline Parallel)各个方面讲的很透彻
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://zhuanlan.zhihu.com/p/106783111)

### 并行模式

分布式训练的目的在于将原本巨大的训练任务拆解成**多个子任务**，每个子任务在独立的机器上单独执行。

大规模深度学习任务的难点在于：
- 1) 训练**数据量巨大**：将数据拆解成多个小模型分布到不同的node上。→ **数据并行**
- 2) 训练模型的**参数巨大**：将数据集拆解分布到不同的node上。→ **模型并行**
  - NLP的预训练模型实在太大了

#### 数据并行

数据并行相对简单，N个node(也称为worker)构成一个**分布式集群**，每个worker处理1/N的数据。
- 理论情况下能达到**线性**的加速效果。TF、torch、Horovod都可以在原生支持或者微小的改动实现数据并行模式。

数据并行是在每个worker上存储一个模型的备份，在各个worker 上处理不同的**数据子集**。然后需要**规约**(reduce)每个worker的结果，在各节点之间同步模型参数。
- 这一步会成为数据并行的瓶颈，因为如果worker很多的情况下，worker之间的数据传输会有很大的时间成本。

参数同步后，需要采用不同的方法进行参数更新：
- **参数平均法**：最简单的一种数据平均化
- **更新式方法**

若采用**参数平均法**，训练的过程如下所示：基于模型的配置随机初始化网络模型参数
- 将当前这组参数分发到各个工作节点
- 在每个工作节点，用数据集的一部分数据进行训练
- 将各个工作节点的参数的**均值**作为**全局参数值**
- 若还有训练数据没有参与训练，则继续从第二步开始

**更新式**方法与**参数平均化**类似，主要区别在于，在**参数**服务器和**工作**服务器之间传递参数时，更新式方法只传递**更新信息**(梯度和张量)。

#### 模型并行

模型并行相对复杂，原理是分布式系统中的不同worker负责网络模型的不同部分，例如说，神经网络的不同层被分布到不同worker或者同一层的不同参数被分配到不同worker上。对于TF这种框架，可以拆分计算图成多个最小依赖子图到不同的worker上。同时在多个子图之间通过通信算子来实现模型并行。

但是**模型并行**实现起来比较复杂。工业界还是以**数据并行**为主。

补充：
- Model Parallel主要分两种：**intra-layer**拆分 和 **inter-layer**拆分
  - inter-layer拆分：对模型做网络上的拆分。将每一层或者**某几层**放在一个worker上单独训练。
    - 缺点：模型训练串行，整个模型的效率取决于最慢的那一层，存在资源浪费
- intranet-layer拆分：深度学习的网络结构基本都是一层一层的。常规的卷积、池化、BN等等。如果对某一层进行了拆分，那么就是intra-layer拆分。对单层的拆分其实就是拆分这一层的matrix运算。参考论文：Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

#### 混合并行

随着训练设备的增加，多个worker之间的通信成本增加，模型Reduce的成本也越来越大，数据并行的瓶颈也随之出现。故有学者提出**混合并行**(数据并行+模型并行)


### 架构模式

分布式训练上会频繁用到**规约**(AllReduce)操作。主流的**分布式架构**主要分为`参数服务器`(ParameterServer) 和`基于规约`(Reduce)两种模式。早期还有基于`MPI`的方式，不过现在已经很少用了。

#### PS：参数服务器

ParameterServer模式是一种基于reduce和broadcat算法的经典架构。
- 其中一个/一组机器作为PS架构的**中心节点**，用来**存储参数和梯度**。
- 在更新梯度的时候，先全局reduce接受其他worker节点的数据，经过本地计算后(比如参数平均法)，再broadcast回所有其他worker。

PS架构的问题在于多个worker与ps通信，PS本身可能存在**瓶颈**。
- 随着worker数量的增加，整体通信量也线性增加，加速比也可能停滞在某个点位上。
- ![](https://pic3.zhimg.com/80/v2-eee6e2ad8aa00a8679298ff297508a16_1440w.jpg)

#### 基于规约 Reduce模式

基于规约的模式解决了上述的问题，最典型的是百度提出的 Ring-AllRuduce。
- 多个Worker节点连接成一个环，每个Worker依次把自己的梯度同步给下一个Worker，经过至多 2*(N-1) 轮同步，就可以完成所有Worker的梯度更新。
- 这种方式下所有节点的地位是平等的，因此不存在某个节点的**负载瓶颈**，随着Worker的增加，整体的通信量并不随着增加。加速比几乎可以跟机器数量成线性关系且不存在明显瓶颈。
- ![](https://pic1.zhimg.com/80/v2-5c777ca6d8ce4972d51f6ce73f3a044c_1440w.jpg)

目前，越来越多的分布式训练采用**Reduce**这种模式。Horovod中主要就是用的这种分布式架构。
- 更多资料参考: [兰瑞Frank：腾讯机智团队分享--AllReduce算法的前世今生](https://zhuanlan.zhihu.com/p/79030485)

### 同步范式

实际训练过程中可能遇到各种问题，比如：部分节点资源受限、卡顿、网络延时等等，因此梯度同步时就存在“**木桶**“效应，即集群中的某些worker比其他worker更慢，导致整个训练pipeline需要等待慢的worker，整个集群的训练速度受限于最慢机器的速度。

因此梯度的同步有“**同步**”(sync)、“**异步**”(Async)和**混合**三种范式。
- **同步**范式：只有所有worker完成当前的计算任务，整个集群才会开始下一次迭代。
  - TF中同步范式使用SyncReplicasOptimizer优化器
- **异步**模式刚好相反，每个worker只关心知己的进程，完成计算后就尝试更新，能与其他多个worker同步梯度完成取决于各worker当前时刻的状态。其过程不可控，有可能出现模型正确性问题。(可在训练时logging对比)
- **混合**范式结合以上两种情况，各个worker都会等待其他worker的完成，但不是永久等待，有timeout的机制。如果超时了，则此情况下相当于异步机制。并且没来得及完成计算的worker，其梯度则被标记为“stale”而抛弃或另做处理。

### 物理架构

物理架构主要是 **GPU架构**，即：单机单卡、单机多卡、多机单卡、多机多卡（最典型）
- 单机单卡：常规操作
- 单机**多卡**：利用一台GPU上的多块GPU进行分布式训练。数据并行和模型并行皆可。整个训练过程一般只有一个进程，多GPU之间的通信通过多线程的方式，模型参数和梯度在进程内是共享的(基于NCCL的可能不大一样)。这种情况下基于Reduce的架构比PS架构更合适一些，因为不需要一个显式的PS，通过进程内的Reduce即可完成梯度同步。
- **多机**单卡：操作上与多机多卡基本一致
- 多机**多卡**：多机多卡是最典型的分布式架构，所以它需要较好的进程间的通讯机制(多worker之间的通信)。

### 通信技术

分布式条件下的多进程、多worker之间的通信技术，常见的主要有：MPI、NCCL，GRPC等。
- **MPI**主要是被应用在超算等大规模计算领域，机器学习场景下使用较少。主要是openMPI原语等。
- **NCCL**是NVIDIA针对GPU设计的一种规约库，可以实现多GPU间的直接数据同步，避免内存和显存的，CPU和GPU间的数据拷贝成本。当在TensorFlow中选择单机多卡训练时，其默认采用的就是NCCL方式来通信。
- **GRPC**是比较成熟的通信技术了，spark等框架内也都有用到。


内容：
- 分布式训练的基本原理
- TensorFlow的分布式训练
- PyTorch的分布式训练框架
- Horovod分布式训练

## Tensorflow分布式实现

- 黄文坚的[Tensorflow分布式实战](https://blog.csdn.net/CodeMaster_/article/details/76223835)

### TF分布式训练方法

TensorFlow主要的分布式训练的方法有三种：
1. Customer Train Loop：最原始，由框架工程师自己开发
1. Estimator + Strategy：高级API，不用关心底层硬件
1. Keras + Strategy：最新出的keras的高级API

> - 实际的开发工作中，分布式的工作最好是交给框架，而工程师本身只需要关注任务模型的pipeline就行了。
> - 最经典的是Spark框架，工程师只需要关注数据处理的workflow，分布式的大部分工作都交给框架。深度学习的开发同样如此。

各种方式评价
- 第一种方式太过原生，整个分布式的训练过程完全交给工程师来处理，代码模块比较复杂，这里不做赘述。
- 第二种方式，Estimator是TF的一个高级API，在分布式场景下，其最大的特点是**单机和分布式代码一致**，且不需要考虑底层的硬件设施。Strategy是tensorflow根据分布式训练的复杂性，抽象出的多种分布式训练策略。TF1.x和TF2.x接口变化较大，不同版本名字可能不一样，以实际使用版本为准。用的比较多的是：
  - **MirroredStrategy**：适用于单机多卡、数据并行、同步更新的分布式训练，采用Reduce的更新范式，worker之间采用NCCL进行通信。
  - **MultiWorkerMirrored**Strategy：与上面的类似，不同的是这种策略支持多机多卡、数据并行、同步更新的分布式策略、Reduce范式。在TF 1.15版本里，这个策略叫CollectiveAllReduceStrategy。
  - **ParameterServer**Strategy：经典的PS架构，多机多卡、数据并行、同步/异步更新
  - 使用Estimator+Strategy 实现分布式训练，参考[代码](https://github.com/kubeflow/tf-operator/blob/master/examples/v1/distribution_strategy/estimator-API/keras_model_to_estimator.py)
- 第三种方式 Keras + Strategy 是Tensorflow最新官方推荐的方案。主要是利用keras的高级API，配合Strategy实现多模式的分布式训练。
  - [代码](https://github.com/kubeflow/tf-operator/blob/master/examples/v1/distribution_strategy/keras-API/multi_worker_strategy-with-keras.py)

后两种方法都需要传入TF_CONFIG参数，没有就是单机的训练方式。Strategy会自动读取环境变量并应用相关信息。

TF_CONFIG的配置如下
- ![](https://pic2.zhimg.com/80/v2-dc8c2f647b9e359661e2a6f288ac1525_1440w.jpg)

### 单机单卡

- 单机单卡是最普通的情况，当然也是最简单的，示例代码如下：

```python
#coding=utf-8
#单机单卡，对于单机单卡，可以把参数和计算都定义再gpu上，不过如果参数模型比较大，显存不足等情况，就得放在cpu上
import  tensorflow as tf
with tf.device('/cpu:0'):#也可以放在gpu上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
    mutwb=w*b
init=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)
    np1,np2=sess.run([addwb,mutwb])
    print np1,np2
```

### 单机多卡

- 单机多卡，只要用device直接指定设备，就可以进行训练，SGD采用各个卡的平均值
- 问题：除了取均值，还有别的方式吗？

```python
#coding=utf-8
#单机多卡：一般采用共享操作定义在cpu上，然后并行操作定义在各自的gpu上，比如对于深度学习来说，我们一般把参数定义、参数梯度更新统一放在cpu上，各个gpu通过各自计算各自batch数据的梯度值，然后统一传到cpu上，由cpu计算求取平均值，cpu更新参数。具体的深度学习多卡训练代码，请参考：https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py
import  tensorflow as tf
  
with tf.device('/cpu:0'):
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
with tf.device('/gpu:1'):
    mutwb=w*b
  
ini=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(ini)
    while 1:
        print sess.run([addwb,mutwb])
```
- 多个 GPU 上运行 TensorFlow，则可以采用多塔式方式构建模型，其中每个塔都会分配给不同 GPU。例如：

```python
# Creates a graph.
c = []
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(sum))
```

- 【2020-5-20】每个gpu的梯度要累加起来，单独计算

```python
        # train op def
        tower_grads = []
        for i in xrange(FLAGS.num_gpus):
            with tf.device('/gpu:{}'.format(i)):
                with tf.name_scope('tower_{}'.format(i)):
                    next_batch = dhs.get_next_batch()
                    cnn.inference(
                        next_batch[0], next_batch[1], next_batch[2],
                        dropout_keep_prob=FLAGS.dropout_keep_prob,
                        input_dropout_keep_prob=FLAGS.input_dropout_keep_prob,
                        phase_train=True)
                    grads = optimizer.compute_gradients(cnn.loss)
                    tower_grads.append(grads)
        grads = average_gradients(tower_grads)
        train_op = optimizer.apply_gradients(grads, global_step=global_step)

def average_gradients(tower_grads):
    """
    Calculate the average gradient for each shared variable across all towers.
    Note that this function provides a synchronization point across all towers.
    NOTE: This function is copied from cifar codes in tensorflow tutorial with minor
    modification.
    Args:
        tower_grads: List of lists of (gradient, variable) tuples. The outer list
            is over individual gradients. The inner list is over the gradient
            calculation for each tower.
    Returns:
       List of pairs of (gradient, variable) where the gradient has been averaged
       across all towers.
    """
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        # Note that each grad_and_vars looks like the following:
        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
        grads = []
        for g, _ in grad_and_vars:
            # Add 0 dimension to the gradients to represent the tower.
            # NOTE: if batch norm applied, the grad of conv-maxpool-n/b will be
            #       None
            if g is None:
                continue
            expanded_g = tf.expand_dims(g, 0)

            # Append on a 'tower' dimension which we will average over below.
            grads.append(expanded_g)

        # Average over the 'tower' dimension.
        if grads:
            grad = tf.concat(axis=0, values=grads)
            grad = tf.reduce_mean(grad, 0)
        else:
            grad = None

        # Keep in mind that the Variables are redundant because they are shared
        # across towers. So .. we will just return the first tower's pointer to
        # the Variable.
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads
```

- 参考官网：[TensorFlow with multiple GPUs](https://jhui.github.io/2017/03/07/TensorFlow-GPU/)

### 多机多卡

- 一、基本概念
    - Cluster、Job、task概念：三者可以简单的看成是层次关系
    - task相当于每台机器上的一个进程，多个task组成job；
    - job又有两种：ps参数服务、worker计算服务，组成cluster。
- 二、同步SGD与异步SGD
    - 1、同步更新：各个用于并行计算的电脑，计算完各自的batch 后，求取梯度值，把梯度值统一送到ps服务机器中，由ps服务机器求取梯度平均值，更新ps服务器上的参数。
        - 如下图所示，可以看成有四台电脑，第一台电脑用于存储参数、共享参数、共享计算，可以简单的理解成内存、计算共享专用的区域，也就是ps job；另外三台电脑用于并行计算的，也就是worker task。
        - 这种计算方法存在的缺陷是：每一轮的梯度更新，都要等到A、B、C三台电脑都计算完毕后，才能更新参数，也就是迭代更新速度取决与A、B、C三台中，最慢的那一台电脑，所以采用同步更新的方法，建议A、B、C三台的计算能力都不想。
    - 2、异步更新：ps服务器收到只要收到一台机器的梯度值，就直接进行参数更新，无需等待其它机器。这种迭代方法比较不稳定，收敛曲线震动比较厉害，因为当A机器计算完更新了ps中的参数，可能B机器还是在用上一次迭代的旧版参数值。

代码编写
- 1、定义集群
    - 比如假设上面的图所示，我们有四台电脑，名字假设为：A、B、C、D，那么集群可以定义如下

```python
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "A_IP:2222",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
        "B_IP:1234"#第二台机器的IP地址 /job:worker/task:1
        "C_IP:2222"#第三台机器的IP地址 /job:worker/task:2
    ],
    "ps": [
        "D_IP:2222",#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
```

然后需要写四分代码，这四分代码文件大部分相同，但是有几行代码是各不相同的。

- 2、在各台机器上，定义server
    - 比如A机器上的代码server要定义如下：
```python
server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
```

- 3、在代码中，指定device
```python
with tf.device('/job:ps/task:0'):#参数定义在机器D上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/job:worker/task:0/cpu:0'):#在机器A cpu上运行
    addwb=w+b
with tf.device('/job:worker/task:1/cpu:0'):#在机器B cpu上运行
    mutwb=w*b
with tf.device('/job:worker/task:2/cpu:0'):#在机器C cpu上运行
    divwb=w/b
```

在深度学习训练中，一般图的计算，对于每个worker task来说，都是相同的，所以我们会把所有图计算、变量定义等代码，都写到下面这个语句下：
```python
with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:indexi',cluster=cluster))
```

函数replica_deviec_setter会自动把变量参数定义部分定义到ps服务中(如果ps有多个任务，那么自动分配)。下面举个例子，假设现在有两台机器A、B，A用于计算服务，B用于参数服务，那么代码如下：

```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
import  tensorflow as tf
#现在假设我们有A、B台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=False
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b

saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
    while 1:
        print sess.run([addwb,mutwb,divwb])
```

把该代码在机器A上运行，你会发现，程序会进入等候状态，等候用于ps参数服务的机器启动，才会运行。

因此接着我们在机器B上运行如下代码：

```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=True
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b
  
saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
    while 1:
        print sess.run([addwb,mutwb,divwb])
```

- [Tensorflow官方指南](https://www.tensorflow.org/versions/master/how_tos/distributed/index.html)
- 分布式训练需要熟悉的函数：
    - tf.train.Server
    - tf.train.Supervisor
    - tf.train.SessionManager
    - tf.train.ClusterSpec
    - tf.train.replica_device_setter
    - tf.train.MonitoredTrainingSession
    - tf.train.MonitoredSession
    - tf.train.SingularMonitoredSession
    - tf.train.Scaffold
    - tf.train.SessionCreator
    - tf.train.ChiefSessionCreator
    - tf.train.WorkerSessionCreator

## Pytorch的分布式训练

【2023-3-2】[PyTorch 分布式训练实现(DP/DDP/torchrun/多机多卡)](https://zhuanlan.zhihu.com/p/489011749)

相对Tensorflow，Pytorch简单的多。分布式训练主要有两个API：
- DataParallel(`DP`): **PS模式**，会有一张卡为reduce（parame server），实现简单，就一行代码
  - 将数据分割到多个GPU上。这是典型的`数据并行`，将模型复制到每个GPU上，一旦GPU0计算出梯度，则需要同步梯度，这需要大量的GPU数据传输（类似PS模式）
- DistributedDataParallel(`DDP`): **All-Reduce模式**，单机多卡/多级多卡皆可。官方建议API
  - 每个GPU的进程中创建模型副本，并只让数据的一部分对改GPU可用。因为每个GPU中的模型是独立运行的，所以在所有的模型都计算出梯度后，才会在模型之间同步梯度（类似All-reduce）

分析
- `DDP`每个batch只需要一次数据传输；
- `DP`可能存在多次数据同步(不用worker之间可能快慢不一样)。

### 1、DataParallel

模型与变量必须存在于同一个设备上（CPU or GPU）

pytorch使用**to函数**实现变量或模型的存储转移
- to函数的对象: 数据Tensor，或 模型Module 
- 张量不执行inplace(即 执行之后重新构建一个新的张量)
- 模型执行inplace(执行之后不重新构建一个新的模型)

原理：
- 当给定model时，主要实现功能是将input数据依据batch的这个维度，将数据划分到指定的设备上。其他的对象(objects)复制到每个设备上。在前向传播的过程中，module被复制到每个设备上，每个复制的副本处理一部分输入数据。在反向传播过程中，每个副本module的梯度被汇聚到原始的module上计算(一般为第0块GPU)。

举例：
- 如果当前有4个GPU，batch_size=16，那么模型将被复制到每一个GPU上，在前向传播时，每一个gpu将分到4个batch，每个gpu独立计算依据分到的batch计算出结果的梯度，然后将梯度返回到第一个GPU上，第一个GPU再进行梯度融合、模型更新。在下一次前向传播的时候，将更新后的模型再复制给每一个GPU。

```py
###	第一步：构建模型
# module 需要分发的模型
# device_ids 可分发的gpu，默认分发到所有看见GPU（环境变量设置的）
# output_device 结果输出设备 通常设置成逻辑gpu的第一个
module = your_simple_net() #你的模型
Your_Parallel_Net = torch.nn.DataParallel(module, device_ids=None, output_device=None)
### 第二步：数据迁移
inputs=inputs.to(device)	
labels=labels.to(device)	
# device通常应为模型输出的output_device，否则无法计算loss
```

代码

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import os

input_size = 5
output_size = 2
batch_size = 30
data_size = 30

class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len

rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)

class Model(nn.Module):
    # Our model
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print("  In Model: input size", input.size(),
              "output size", output.size())
        return output
model = Model(input_size, output_size)

if torch.cuda.is_available():
    model.cuda()

if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    # 就这一行！将模型整体复制到每个GPU上，计算完成后各自汇总到ps节点
    model = nn.DataParallel(model)

for data in rand_loader:
    if torch.cuda.is_available():
        input_var = Variable(data.cuda())
    else:
        input_var = Variable(data)
    output = model(input_var)
    print("Outside: input size", input_var.size(), "output_size", output.size())
```

### 2、DDP（官方建议）

为什么要引入DDP（DistributedDataParallel）？
- 1、DP在每个训练批次（batch）中，因为模型权重都是在 一个进程上先算出来, 然后再把分发到每个GPU上，所以网络通信就成为了一个瓶颈，而GPU使用率也通常很低。
- 2、因为在每次前向传播时把模型也复制了（即每次更新都复制一遍模型），并且单进程多线程会造成`GIL` contention （全局解释器锁争用） 这里进程计算权重使通信成为瓶颈造成了大量的时间浪费，因此引入了DDP。

DDP采用**多进程**控制多GPU，共同训练模型，一份代码会被pytorch自动分配到n个进程并在n个GPU上运行。 
- DDP运用 `Ring-Reduce`通信算法在每个GPU间对梯度进行通讯，交换彼此的梯度，从而获得所有GPU的梯度。

对比DP，不需要在进行模型本体的通信，因此可以加速训练。

需要注意以下几点：
- 1、设置DistributedSampler来打乱数据，因为一个batch被分配到了好几个进程中，要确保不同的GPU拿到的不是同一份数据。
- 2、要告诉每个进程自己的id，即使用哪一块GPU。
- 3、如果需要做BatchNormalization，需要对数据进行同步（还待研究，挖坑）

DDP采用All-Reduce架构，单机多卡、多机多卡都能用。

注意：DDP并不会自动shard数据
1. 如果自己写数据流，得根据`torch.distributed.get_rank()`去shard数据，获取自己应用的一份
2. 如果用 Dataset API，则需要在定义Dataloader的时候用 DistributedSampler 去shard

使用方式(单机多卡环境)

```py
# 启动方式，shell中运行：
python -m torch.distributed.launch --nnodes 1 --nproc_per_node=4  YourScript.py
# nnodes: 表示有多少个节点，可以通俗的理解为有多少台机器
# nproc_per_node 表示每个节点上有多少个进程，每个进程一般独占一块GPU
########################## 	第1步	 ##########################
#初始化
'''
在启动器为我们启动python脚本后，在执行过程中，启动器会将当前进程的（其实就是 GPU的）index 通过参数传递给 python，
我们可以这样获得当前进程的 index：即通过参数 local_rank 来告诉我们当前进程使用的是哪个GPU，
用于我们在每个进程中指定不同的device
'''
parse.add_argument('--local_rank',type=int)
args=parser.parse_args()
local_rank=args.local_rank
torch.cuda.set_device(local_rank)
'''
init_process_group用于初始化GPU通信方式（NCCL）和参数的获取方式（env代表通过环境变量）
gpu使用nccl最快，gloo为cpu分布式训练，mpu则需要重新编码
init_method 指定如何初始化进程组的 URL。 
默认及推荐为'env://' 其他初始化方式与多机多卡有关（not sure，挖个坑）
'''
torch.distributed.init_process_group('nccl'，init_method='env://')
device = torch.device(f'cuda:{args.local_rank}')
##########################	第2步  ##########################
#处理Dataloader
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,shuffle=True)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)
#torch.utils.data.DataLoader中的shuffle应该设置为False（默认），因为打乱的任务交给了sampler
##########################	第3步  ##########################
#模型的初始化
model=torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])
'''
使用 DistributedDataParallel 包装模型，
它能帮助我们为不同 GPU 上求得的梯度进行allreduce（即汇总不同 GPU 计算所得的梯度，并同步计算结果）。
allreduce 后不同 GPU 中模型的梯度均为 allreduce 之前各 GPU 梯度的均值。
''''
##########################	第4步  ##########################
#同DP，进行inputs、labels的设备转移
```


```py
sampler = DistributedSampler(dataset) # 这个sampler会自动分配数据到各个gpu上
DataLoader(dataset, batch_size=batch_size, sampler=sampler)
```

完整代码如下：

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import os
from torch.utils.data.distributed import DistributedSampler
# 1) 初始化
torch.distributed.init_process_group(backend="nccl")

input_size = 5
output_size = 2
batch_size = 30
data_size = 90

# 2） 配置每个进程的gpu
local_rank = torch.distributed.get_rank()
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)

class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size).to('cuda')

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len

dataset = RandomDataset(input_size, data_size)
# 3）使用DistributedSampler
rand_loader = DataLoader(dataset=dataset,
                         batch_size=batch_size,
                         sampler=DistributedSampler(dataset))

class Model(nn.Module):
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print("  In Model: input size", input.size(),
              "output size", output.size())
        return output

model = Model(input_size, output_size)

# 4) 封装之前要把模型移到对应的gpu
model.to(device)

if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    # 5) 封装：
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)

for data in rand_loader:
    if torch.cuda.is_available():
        input_var = data
    else:
        input_var = data
    output = model(input_var)
    print("Outside: input size", input_var.size(), "output_size", output.size())
```

执行脚本：

```shell
# 启用 DDP 模式
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 torch_ddp.py
```

apex加速(混合精度训练、并行训练、同步BN)可[参考](https://zhuanlan.zhihu.com/p/158375055)

### torchrun (更新)

最新版本的PyTorch实现
- 替换 torch.distributed.launch

根据PyTorch官网介绍
- This module（torch.distributed.launch） is going to be deprecated in favor of torchrun.

torchrun 包含了 torch.distributed.launch 的所有功能，还有以下三点额外的功能：
- 1、worker的rank和world_size将被自动分配
- 2、通过重新启动所有workers来处理workers的故障
- 3、允许节点数目在最大最小值之间有所改变 即具备弹性

```py
# local_rank参数应当从环境变量中读取，而不是通过参数传递。
### ------ BEFORE ------
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--local_rank", type=int)
args = parser.parse_args()

local_rank = args.local_rank
### ------ NOW -------
import os
local_rank = int(os.environ["LOCAL_RANK"])

#运行脚本
torchrun train_script.py #除了--use_env参数，其他torch.distributed.launch所使用的参数均可使用，
			 #如nnodes、nproc_per_node
```

### 多机多卡 DDP

概念理解
- `group`: `进程组`，通常DDP的各个进程都是在同一个进程组下 
- `world_size`: 总的进程数量（原则上，一个进程占用一个GPU） 
- `rank`：当前进程的序号，用于进程间通信，rank=0表示主机为master节点 
- `local_rank`：当前进程对应的GPU号

举个栗子 ： 
- 4台机器 (每台机器8张卡) 进行分布式训练。
- 通过 init_process_group() 对进程组进行初始化。 
- 初始化后 可以通过 get_world_size() 获取到 world size = 32。
- 在该例中为32， 即有32个进程，其编号为0-31 通过 get_rank() 函数可以进行获取 在每台机器上，local rank均为0-8， 这是 local rank 与 rank 的区别， local rank 会对应到实际的 GPU ID 上。

三种启动方法：
- torch.distributed.launch
- torch.multiprocessing
- Slurm Workload Manager: slurm启动应该会这几天更新掉

```py
########################## 	第1步	 ##########################
#初始化
rank = int(os.environ["RANK"])
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(rank % torch.cuda.device_count())
dist.init_process_group(backend="nccl")
device = torch.device("cuda", local_rank)
########################## 	第2步	 ##########################
#模型定义
model = model.to(device)
model = DDP(model, device_ids=[local_rank], output_device=local_rank)

#数据集操作与DDP一致

#####运行
'''
exmaple: 2 node, 8 GPUs per node (16GPUs)
需要在两台机器上分别运行脚本
注意细节：node_rank master 为 0 
机器1
>>> python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="master的ip" \
    --master_port=xxxxx \
    YourScript.py
机器2
>>> python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr="master的ip" \
    --master_port=xxxxx \
    YourScript.py

'''
```


## Horovod分布式训练

Horovod是Uber开源的跨平台的分布式训练工具，名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与Horovod设备之间的通信模式很像，有以下几个特点：
- 兼容TensorFlow、Keras和PyTorch机器学习框架。
- 使用Ring-AllReduce算法，对比Parameter Server算法，有着无需等待，负载均衡的优点。
- 实现简单，五分钟包教包会。

Horovod环境准备以及示例代码，可参考[上一篇](https://zhuanlan.zhihu.com/p/351693076)

# 推理加速

![](https://pica.zhimg.com/v2-4722a5639a0dafc705be6199c5920a08_1440w.jpg)


## ONNX

- 【2022-5-17】[ONNX 和 Azure 机器学习：创建和加速 ML 模型](https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-onnx)
- 【2022-6-9】[ONNX推理加速技术文档](https://zhuanlan.zhihu.com/p/524023964)

推理或模型评分是将部署的模型用于**预测**（通常针对生产数据）的阶段，ONNX 来帮助优化机器学习模型的推理

pytorch怎么使用c++调用部署模型？[参考](https://zhuanlan.zhihu.com/p/589562702)
- **PyTorch模型** --> `ONNX格式` --> **C++推理框架**

### 什么是ONNX

开放神经网络交换（Open Neural Network Exchange） `ONNX` 是**微软**和**Facebook**提出，用来表示深度学习模型的**开放格式**。
- [ONNX](https://onnx.ai/)定义了一组和环境平台均无关的标准格式，来增强各种AI模型的可交互性。
- 优化用于**推理**（或模型评分）的机器学习模型非常困难，因为需要调整模型和推理库，充分利用硬件功能。 在不同类型的平台（云/Edge、CPU/GPU 等）上获得最佳性能，异常困难，因为每个平台都有不同的功能和特性。 如果模型来自需要在各种平台上运行的多种框架，会极大增加复杂性。 优化框架和硬件的所有不同组合非常耗时。 

ONNX就是解决方法，在首选框架中训练一次后能在云或 Edge 上的**任意**位置运行。
- ![ONNX](https://pic3.zhimg.com/80/v2-b78c389b742875193877e806a661da6e_1440w.webp)

许多框架中的模型都可以导出或转换为标准 **ONNX 格式**。 模型采用 ONNX 格式后，可在各种平台和设备上运行。
- 各种平台包括 TensorFlow、PyTorch、SciKit-Learn、Keras、Chainer、MXNet、MATLAB 和 SparkML
**ONNX 运行时**是一种用于将 ONNX 模型部署到生产环境的**高性能推理引擎**。 它针对云和 Edge 进行了优化，适用于 Linux、Windows 和 Mac。
- ONNX文件不仅仅存储了神经网络模型的权重(Protobuf格式)，同时也存储了模型的结构信息以及网络中每一层的输入输出和一些其它的辅助信息。
- 用 C++ 编写，还包含 C、Python、C#、Java 和 JavaScript (Node.js) API，可在各种环境中使用。 
- ONNX 运行时同时支持 DNN 和传统 ML 模型，并与不同硬件上的**加速器**（例如，NVidia GPU 上的 `TensorRT`、Intel 处理器上的 `OpenVINO`、Windows 上的 `DirectML` 等）集成。 
通过使用 ONNX 运行时，可以从大量的生产级优化、测试和不断改进中受益。
- ![](https://docs.microsoft.com/zh-cn/azure/machine-learning/media/concept-onnx/onnx.png)

### 示例

yolov3-tiny的[onnx模型](https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/tiny-yolov3/model),Netron可视化
- ![](https://pic2.zhimg.com/80/v2-d21635b818ed67dfdb4c834324fa1555_1440w.jpg)

更多使用方法见: [ONNX学习笔记](https://zhuanlan.zhihu.com/p/346511883)


### 二、直接使用onnx进行推理
 
onnx文件可以直接进行推理，这时的代码就已经与框架无关了，可以与训练阶段解耦。但是，为了推理的顺利进行，你依然需要为onnx选择一个后端，以TensorFlow为例。
 
```python
import onnx
import tensorflow as tf
from onnx_tf.backend import prepare
import onnx_tf...# 包装一个TF后端
predictor = onnx.load(onnx_path)
onnx.checker.check_model(predictor)
onnx.helper.printable_graph(predictor.graph)
tf_rep = prepare(predictor, device="CUDA:0")  # default CPU
# 使用TF进行预测
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)  # defalut 0.5
tfconfig = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)... 
with tf.Session(config=tfconfig) as persisted_sess:
    persisted_sess.graph.as_default()
    tf.import_graph_def(tf_rep.graph.as_graph_def(), name='')
    tf_input = persisted_sess.graph.get_tensor_by_name(
        tf_rep.tensor_dict[tf_rep.inputs[0]].name
    )
    tf_scores = persisted_sess.graph.get_tensor_by_name(
        tf_rep.tensor_dict[tf_rep.outputs[0]].name
    )
    tf_boxes = persisted_sess.graph.get_tensor_by_name(
        tf_rep.tensor_dict[tf_rep.outputs[1]].name
    )
 
    for file_path in listdir:
        ...
        confidences, boxes = persisted_sess.run([tf_scores, tf_boxes], {tf_input: image})
        ...
```
 
### 三、使用onnxruntime加速推理
 
事实上，可以更高效地使用onnx。onnxruntime是一个对onnx模型提供推理加速的库，支持CPU和GPU加速，GPU加速版本为onnxruntime-gpu，默认版本为CPU加速。安装:
 
```shell
pip install onnxruntime  # CPU
pip install onnxruntime-gpu # GPU
```
 
使用onnxruntime对onnx模型加速非常简单，只需要几行代码。这里给出一个示例：
 
```python
import onnxruntime as ort

class NLFDOnnxCpuInferBase:
    """only support in CPU and accelerate with onnxruntime."""
 
    __metaclass__ = ABCMeta
   ...
   def __init__(self,
                 onnx_path=ONNX_PATH):
        """pytorch和onnx可以很好地结合        :param onnx_path: .onnx文件路径        """
        self._onnx_path = onnx_path
        # 使用onnx模型初始化ort的session
        self._ort_session = ort.InferenceSession(self._onnx_path)
        self._input_img = self._ort_session.get_inputs()[0].name
   ...
 
   # 使用run推理
   def _detect_img_utils(self, img: np.ndarray):
        """batch is ok."""
        feed_dict = {self._input_img: img}
        scores_before_nms, rois_before_nms = self._ort_session.run(None,input_feed=feed_dict)
        return rois_before_nms, scores_before_nms
```
 
onnxruntime会自动帮你检查onnx中的无关节点并删除，也利用了一些加速库优化推理图，从而加速推理。一些log:
 
```shell
python3 inference/ulfd/onnx_cpu_infer.py
# 2020-01-16 12:03:49.259044 [W:onnxruntime:, graph.cc:2412 CleanUnusedInitializers] Removing initializer 'base_net.9.4.num_batches_tracked'. It is not used by any node and should be removed from the model.
# 2020-01-16 12:03:49.259478 [W:onnxruntime:, graph.cc:2412 CleanUnusedInitializers] Removing initializer 'base_net.9.1.num_batches_tracked'. It is not used by any node and should be removed from the model.
# 2020-01-16 12:03:49.259492 [W:onnxruntime:, graph.cc:2412 CleanUnusedInitializers] Removing initializer 'base_net.8.4.num_batches_tracked'. It is not used by any node and should be removed from the model.
# 2020-01-16 12:03:49.259501 [W:onnxruntime:, graph.cc:2412 CleanUnusedInitializers] Removing initializer 'base_net.8.1.num_batches_tracked'. It is not used by any node and should be removed from the model.
```
 
### 四、实验结果
 
在人脸检测ULFD模型上，未使用onnxruntime加速，对于320x240分辨率的图片，在CPU上需要跑要50~60ms;使用onnxruntime加速后，在CPU需要8~11ms.
 
### 五、请优雅地使用Numpy
 
在图像处理中经常会出现归一化处理，即使在推理的时候也需要。而在推理时需要考虑性能问题，最近发现numpy的张量计算的不同方式，会对性能有很大影响。如果你的均值化处理中的每个通道减去的均值都是一样的比如127.
 
```python
# 普通的做法是：(请不要使用这种做法)
image_mean = np.array([127, 127, 127])
image = (image - image_mean) / 128 # 实际上会由于numpy的广播运算消耗更多的时间
# 你应该采用：(保证数据类型的一致以及减去一个常量的效率更高)
image = (image - 127.) / 128.
```
 
*   _实验代码 相同均值_
    
 
```python
# coding: utf-8
import cv2
import time
import numpy as np

if __name__ == '__main__':
    test_w, test_h = 500, 500
 
    test_path = 'logs/test0.jpg'
    test_img = cv2.imread(test_path)
    resize_img = cv2.resize(test_img, (test_w, test_h))

    test_count = 1000
    print('width: {0}, height: {1}, test_count: {2}'.format(test_w, test_h, test_count))
 
    t1 = time.time()
    image_mean = np.array([127, 127, 127])
    for _ in range(test_count):
        image = (resize_img - image_mean) / 128
    t2 = time.time()
    print('total_time_ugly: {0}s, mean_time_ugly: {1}ms'.format(
        (t2-t1), (t2-t1)*1000/test_count
    ))
    t3 = time.time()
    for _ in range(test_count):
        image = (resize_img - 127.) / 128.
    t4 = time.time()
    print('total_time_elegant: {0}s, mean_time_elegant: {1}ms'.format(
        (t4 - t3), (t4 - t3) * 1000 / test_count
    ))
```
 
实验结果：
- ![](https://pic2.zhimg.com/80/v2-46ca71eacd67ea8f9b279e6dc83289fd_1440w.jpg)
 
但是当你确实要对不同的通道用到不同的均值时呢？ 也请你这样做，以下是另一个测试结果。
 
* _实验代码 不同均值_
    
 
```python
# coding: utf-8
import cv2
import time
import numpy as np
 
if __name__ == '__main__':
    test_w, test_h = 100, 100
 
    test_path = 'logs/test0.jpg'
    test_img = cv2.imread(test_path)
    resize_img = cv2.resize(test_img, (test_w, test_h))
 
 
    test_count = 100
    print('width: {0}, height: {1}, test_count: {2}'.format(test_w, test_h, test_count))
    print('-'*100)
    t1 = time.time()
    image_mean = np.array([127, 120, 107])
    for _ in range(test_count):
        image = (resize_img - image_mean) / 128
    t2 = time.time()
    print('total_time_ugly: {0}s, mean_time_ugly: {1}ms'.format(
        (t2 - t1), (t2 - t1) * 1000 / test_count
    ))
    t3 = time.time()
    image = np.zeros_like(resize_img)
    for _ in range(test_count):
        image[:, :, 0] = (resize_img[:, :, 0] - 127.) / 128.
        image[:, :, 1] = (resize_img[:, :, 1] - 120.) / 128.
        image[:, :, 2] = (resize_img[:, :, 2] - 107.) / 128.
    t4 = time.time()
    print('total_time_elegant: {0}s, mean_time_elegant: {1}ms'.format(
        (t4 - t3), (t4 - t3) * 1000 / test_count
    ))
```
 
实验结果
- ![](https://pic2.zhimg.com/80/v2-1a79d361282b74a108592b1c48f24259_1440w.jpg)
 
简单来说就是，只要你愿意动手修改几行代码，就能带来5ms~15ms的性能提升。这比采用TensorRT/ONNX等各种加速工具要简单太多了。


## 模型文件转换

ONNX的目的是“通用”，所以难免出现算子不兼容的情况。
- 当把某个框架（例如PyTorch）的模型转成ONNX后，再将ONNX转成另一框架模型（例如ncnn）时，可能会报错（xxx算子不支持）。

解决方法：
- 使用 ONNXSIM 对 ONNX模型 进行精简，非常有效。
  - 建议：只要使用了ONNX，都用ONNXSIM对ONNX模型进行处理一次。[Github地址](https://github.com/daquexian/onnx-simplifier)。使用非常方便，使用“pip install onnxsim”安装，然后使用命令“onnxsim input_onnx_model_path output_onnx_model_path”即可。代码中调用也很简单，参考Git地址里的示例。
- 避免依赖于中间变量的尺寸来进行运算。
  - 在一些Image to Image的任务中，可能会根据中间tensor的尺寸来对另一些tensor进行resize。这时我们的做法是先去获取中间tensor的尺寸H、W，然后将它们作为参数送给其它方法。当遇到这种运算时，ONNX似乎会创建两个与H、W相关的变量，但它们的值会绑定为用dummy_input去forward一次时得到的H、W。这个值一旦绑定就不会改变。所以后续当使用不同尺寸输入时极大概率会报错（这点没有仔细验证过，但看中间结果很像是这种情况）。
- 另外, 强烈建议使用一些网络可视化工具。当遇到模型转换报错时可以用来方便定位出错的位置。个人比较喜欢的是[netron](https://github.com/lutzroeder/netron)

### 1.1 pth文件(Pytorch)转onnx
 
pytorch框架集成了**onnx模块**，属于官方支持，onnx也覆盖了pytorch框架中的大部分算子。因此将pth模型文件转换为onnx文件非常简单。

```py
import torch
# 指定输入尺寸，ONNX需要这个信息来确定输入大小
# 参数对应于 (batch_size, channels, H, W)
dummy_input = torch.randn(1, 3, 224, 224, device="cuda")
# model为模型自身
# dummy_input根据自己的需求更改其尺寸
# "model.onnx"为输出文件，更改为自己的路径即可
torch.onnx.export(model, dummy_input, "model.onnx")
```

以下是一个代码示例。需要注意的是，在转换之前，需要对pth模型的输入size进行冻结。比如：
 
```py
batch_size = 1
dummy_input = torch.randn(batch_size, 3, 240, 320).to(device)
```
 
输入一旦冻结后，就只会有固定的batch_size，在使用转换后的onnx文件进行模型推理时，推理时输入的batch_size必须和冻结时保持一致。对于这个示例，你只能batch_size=1进行推理。如果你需要在推理时采用不同的batch_size，比如10，你只能在保存onnx模型之前修改冻结的输入节点，代码如下：
 
```py
batch_size = 10
dummy_input = torch.randn(batch_size, 3, 240, 320).to(device)
```
 
这样，你就拥有了一个bacth_size=10的onnx模型。导出onnx文件，只需要使用torch.onnx.export()函数，代码如下：
 
```py
    model_name = model_path.split("/")[-1].split(".")[0]
    model_path = f"inference/ulfd/onnx/{model_name}-batch-{batch_size}.onnx"
 
    dummy_input = torch.randn(batch_size, 3, 240, 320).to(device)
    # dummy_input = torch.randn(1, 3, 480, 640).to("cuda") #if input size is 640*480
    torch.onnx.export(net, dummy_input, model_path,
                      verbose=False, input_names=['input'],
                      output_names=['scores', 'boxes'])
```
 
完整的转换代码：
 
```python
# -*- coding: utf-8 -*-
"""This code is used to convert the pytorch model into an onnx format model."""
import argparse
import sys
import torch.onnx
from models.ulfd.lib.ssd.config.fd_config import define_img_size

input_img_size = 320  # define input size ,default optional(128/160/320/480/640/1280)
define_img_size(input_img_size)
from models.ulfd.lib.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd
from models.ulfd.lib.ssd.mb_tiny_fd import create_mb_tiny_fd

def get_args():
    parser = argparse.ArgumentParser(description='convert model to onnx')
    parser.add_argument("--net", dest='net_type', default="RFB",
                        type=str, help='net type.')
    parser.add_argument('--batch', dest='batch_size', default=1,
                        type=int, help='batch size for input.')
    args_ = parser.parse_args()
 
    return args_if __name__ == '__main__':
 
    # net_type = "slim"  # inference faster,lower precision
    args = get_args()
 
    net_type = args.net_type  # inference lower,higher precision
    batch_size = args.batch_size
 
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
 
    label_path = "models/ulfd/voc-model-labels.txt"
    class_names = [name.strip() for name in open(label_path).readlines()]
    num_classes = len(class_names)
 
    if net_type == 'slim':
        model_path = "baseline/ulfd/version-slim-320.pth"
        # model_path = "models/pretrained/version-slim-640.pth"
        net = create_mb_tiny_fd(len(class_names), is_test=True, device=device)
    elif net_type == 'RFB':
        model_path = "baseline/ulfd/version-RFB-320.pth"
        # model_path = "models/pretrained/version-RFB-640.pth"
        net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device=device)
 
    else:
        print("unsupport network type.")
        sys.exit(1)
    net.load(model_path)
    net.eval()
    net.to(device)
 
    model_name = model_path.split("/")[-1].split(".")[0]
    model_path = f"inference/ulfd/onnx/{model_name}-batch-{batch_size}.onnx"
 
    dummy_input = torch.randn(batch_size, 3, 240, 320).to(device)
    # dummy_input = torch.randn(1, 3, 480, 640).to("cuda") #if input size is 640*480
    torch.onnx.export(net, dummy_input, model_path,
                      verbose=False, input_names=['input'],
                      output_names=['scores', 'boxes'])
    print('onnx model saved ', model_path)
 
    """    PYTHONPATH=. python3 inference/ulfd/pth_to_onnx.py --net RFB --batch 16    PYTHONPATH=. python inference/ulfd/pth_to_onnx.py --net RFB --batch 3    """
```
 
### 1.2 pb文件(TensorFlow)转onnx
 
pb文件转onnx可以使用**tf2onnx**库，但必须说明的是，TensorFlow并没有官方支持onnx，tf2onnx是一个第三方库。格式转化onnx格式文件将tensorflow的pb文件转化为onnx格式的文件 安装tf2onnx。 
- 参考：[tensorrt-cubelab-docs](https://dev.pandateacher.com/cube-lab/document/tutorial/tensorrt.html) tf2onnx安装
 
```shell
pip install tf2onnx
```

格式转化指令:
 
```shell
python -m tf2onnx.convert --input ./checkpoints/new_model.pb --inputs intent_network/inputs:0,intent_network/seq_len:0 --outputs logits:0 --output ./pb_models/model.onnx --fold_const # SAVE_MODEL保存为save_model
```

```python
from tensorflow.python.compiler.tensorrt import trt_convert as trt
converter = trt.TrtGraphConverter(input_saved_model_dir=input_saved_model_dir)
converter.convert()
converter.save(output_saved_model_dir)
```

```shell
python -m tf2onnx.convert --saved_model saved_model_dir --output model.onnx # .pb 文件
python -m tf2onnx.convert --input frozen_graph.pb  --inputs X:0,X1:0 --outputs output:0 --output model.onnx --fold_const # .ckpt 文件
python -m tf2onnx.convert --checkpoint checkpoint.meta  --inputs X:0 --outputs output:0 --output model.onnx --fold_const
```
 
### 1.3 onnx转pb文件(TensorFlow)
 
有时候，我们需要对模型进行**跨框架**的转换，比如用pytorch训练了一个模型，但需要集成到TensorFlow中以便和其他的模型保持一致，方便部署。

此时就可以通过将pth转换成onnx，然后再将onnx转换成pb文件，如果转换成功，那么就可以在TensorFlow使用pb文件进行推理了。之所以强调如果，是因为TensorFlow并没有官方支持onnx，有可能会因为一些算子不兼容的问题导致转换后的pb文件在TF推理时出问题。 将onnx转换pb文件可以使用onnx-tf库，安装

```
pip install onnx-tf
```
 
完整的转换代码：
 
```python
# -*- coding: utf-8 -*-
"""
    @File  : onnx_to_pb.py@Author: qiuyanjun@Date  : 2020-01-10 19:22@Desc  : 
"""
import cv2
import numpy as np
import onnx
import tensorflow as tf
from onnx_tf.backend import prepare
import onnx_tf
model = onnx.load('models/onnx/version-RFB-320.onnx')
tf_rep = prepare(model)
img = cv2.imread('imgs/1.jpg')
image = cv2.resize(img, (320, 240))# 测试是否能推理
image_mean = np.array([127, 127, 127])
image = (image - image_mean) / 128
image = np.transpose(image, [2, 0, 1])
image = np.expand_dims(image, axis=0)
image = image.astype(np.float32)
output = tf_rep.run(image)
print("output mat: \\n", output)
print("output type ", type(output))# 建立Session并获取输入输出节点信息
with tf.Session() as persisted_sess:
    print("load graph")
    persisted_sess.graph.as_default()
    tf.import_graph_def(tf_rep.graph.as_graph_def(), name='')
    inp = persisted_sess.graph.get_tensor_by_name(
        tf_rep.tensor_dict[tf_rep.inputs[0]].name
    )
    print('input_name: ', tf_rep.tensor_dict[tf_rep.inputs[0]].name)
    print('input_names: ', tf_rep.inputs)
    out = persisted_sess.graph.get_tensor_by_name(
        tf_rep.tensor_dict[tf_rep.outputs[0]].name
    )
    print('output_name_0: ', tf_rep.tensor_dict[tf_rep.outputs[0]].name)
    print('output_name_1: ', tf_rep.tensor_dict[tf_rep.outputs[1]].name)
    print('output_names: ', tf_rep.outputs)
    res = persisted_sess.run(out, {inp: image})
    print(res)
    print("result is ", res)# 保存成pb文件
    tf_rep.export_graph('version-RFB-320.pb')
    print('onnx to pb done.')
    
"""cmd
PYTHONPATH=. python3 onnx_to_pb.py
"""
```
 
### 1.4 ONNX转ncnn

[ncnn](https://github.com/Tencent/ncnn)是腾讯开源的轻量级推理框架, 简单易用, 但当功耗、时耗是主要考虑点的时候，需要多尝试其它框架，如TensorFlow Lite。


## TensorRT

- 【2021-5-21】[TensorRT入门指北](https://zhuanlan.zhihu.com/p/371239130)
[显卡算力查看](https://developer.nvidia.com/zh-cn/cuda-gpus)


### 什么是TensorRT

- TensorRT是由Nvidia推出的C++语言开发的高性能**神经网络推理库**，是一个用于生产部署的优化器和运行时引擎。其高性能计算能力依赖于Nvidia的图形处理单元。它专注于推理任务，与常用的神经网络学习框架形成互补，包括TensorFlow、Caffe、PyTorch、MXNet等。可以直接载入这些框架的已训练模型文件，也提供了API接口通过编程自行构建模型。
  - ![](https://img-blog.csdnimg.cn/20210425231146908.png)
- TensorRT是可以在NVIDIA各种GPU硬件平台下运行的一个C++推理框架。我们利用Pytorch、TF或者其他框架训练好的模型，可以转化为TensorRT的格式，然后利用TensorRT推理引擎去运行我们这个模型，从而提升这个模型在英伟达GPU上运行的速度。速度提升的比例是比较可观的。
- TensorRT是由C++、CUDA、python三种语言编写成的一个库，其中核心代码为C++和CUDA，Python端作为前端与用户交互。当然，TensorRT也是支持C++前端的，如果我们追求高性能，C++前端调用TensorRT是必不可少的。
- TensorRT是半开源的，除了核心部分其余的基本都开源了。
![](https://pic4.zhimg.com/80/v2-bc9b29cc831bb9793a0aeaaa3061e223_720w.jpg)

### TensorRT的使用场景

TensorRT的使用场景很多。服务端、嵌入式端、家用电脑端都是我们的使用场景。
- 服务端对应的显卡型号为A100、T4、V100等
- 嵌入式端对应的显卡为AGX Xavier、TX2、Nano等
- 家用电脑端对应的显卡为3080、2080TI、1080TI等

当然这不是固定的，只要我们显卡满足TensorRT的先决条件，用就对了。

### TensorRT安装

安装TensorRT的方式有很多，[官方](https://developer.nvidia.com/zh-cn/tensorrt)提供了多种方式：Debian or RPM packages, a pip wheel file, a tar file, or a zip file.
- 如下载TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.1.tar.gz
  - TensorRT的版本与CUDA还有CUDNN版本是密切相关的,不匹配版本的cuda以及cudnn是无法和TensorRT一起使用的
  - 查看本机驱动：nvidia-smi
- 下载好后，tar -zxvf解压即可。
- 解压之后我们需要添加环境变量，以便让我们的程序能够找到TensorRT的libs

```shell
vim ~/.bashrc
# 添加以下内容
export LD_LIBRARY_PATH=/path/to/TensorRT-7.2.3.4/lib:$LD_LIBRARY_PATH
export LIBRARY_PATH=/path/to/TensorRT-7.2.3.4/lib::$LIBRARY_PATH
```

### TensorRT 工作流

工作流主要分为两个阶段：建造阶段(build  phase)和执行阶段(compile phase)。
- 在建造阶段，TensorRT 接收外部提供的网络定义(也可包含权值 weights)和超参数，根据当前编译的设备进行网络运行的优化(optimization), 并生成推理引擎 inference  engine(可以以 PLAN 形式存在在硬盘上)；
- 在执行阶段，通过运行推理引擎调用 GPU 计算资源——整个流程如图
[原文链接](https://blog.csdn.net/weixin_39875161/article/details/99084743)

![](https://img-blog.csdnimg.cn/20190810162851400.png)

### TensorRT 接口

必备接口流程图
![](https://img-blog.csdnimg.cn/20210425232029160.png

TensorRT核心库中，最关键的几种接口类型有：
- IExecutionContext    推理引擎运行上下文
- ICudaEngine            推理引擎
- IRuntime                  CudaEngine反序列化
- INetWorkDefinition   网络定义
- IParser                     网络模型解析
- IOptimizationProfile 优化配置
- IBuilderConfig          CudaEngine的构造参数
- IBuilder                     构造器，主要用于构造CudaEngine
- ILogger                    日志接口，需要开发者实现

接口详情参考：[TensorRT入门](https://blog.csdn.net/Ango_/article/details/116140436)


### TensorRT的加速效果怎么样

加速效果取决于模型的类型和大小，也取决于所使用的显卡类型。

对于GPU来说，因为底层的硬件设计，更适合并行计算也更喜欢密集型计算。TensorRT所做的优化也是基于GPU进行优化，当然也是更喜欢那种一大块一大块的矩阵运算，尽量直通到底。因此对于通道数比较多的卷积层和反卷积层，优化力度是比较大的；如果是比较繁多复杂的各种细小op操作(例如reshape、gather、split等)，那么TensorRT的优化力度就没有那么夸张了。

为了更充分利用GPU的优势，我们在设计模型的时候，可以更加偏向于模型的并行性，因为同样的计算量，“大而整”的GPU运算效率远超“小而碎”的运算。

工业界更喜欢简单直接的模型和backbone。2020年的RepVGG，就是为GPU和专用硬件设计的高效模型，追求高速度、省内存，较少关注参数量和理论计算量。相比resnet系列，更加适合充当一些检测模型或者识别模型的backbone。

在实际应用中，老潘也简单总结了下TensorRT的加速效果：
- SSD检测模型，加速3倍(Caffe)
- CenterNet检测模型，加速3-5倍(Pytorch)
- LSTM、Transformer(细op)，加速0.5倍-1倍(TensorFlow)
- resnet系列的分类模型，加速3倍左右(Keras)
- GAN、分割模型系列比较大的模型，加速7-20倍左右(Pytorch)

### TensorRT有哪些黑科技

为什么TensorRT能够提升我们模型在英伟达GPU上运行的速度，当然是做了很多对提速有增益的优化：
- 算子融合(层与张量融合)：简单来说就是通过融合一些计算op或者去掉一些多余op来减少数据流通次数以及显存的频繁使用来提速
量化：量化即IN8量化或者FP16以及TF32等不同于常规FP32精度的使用，这些精度可以显著提升模型执行速度并且不会保持原先模型的精度
- 内核自动调整：根据不同的显卡构架、SM数量、内核频率等(例如1080TI和2080TI)，选择不同的优化策略以及计算方式，寻找最合适当前构架的计算方式
- 动态张量显存：我们都知道，显存的开辟和释放是比较耗时的，通过调整一些策略可以减少模型中这些操作的次数，从而可以减少模型运行的时间
- 多流执行：使用CUDA中的stream技术，最大化实现并行操作
TensorRT的这些优化策略代码虽然是闭源的，但是大部分的优化策略我们或许也可以猜到一些，也包括TensorRT官方公布出来的一些优化策略：

![](https://pic3.zhimg.com/80/v2-41d4cde8f1a25ffb0ed0ac22a4dcc782_720w.jpg)


### 什么模型可以转换为TensorRT

TensorRT官方支持Caffe、Tensorflow、Pytorch、ONNX等模型的转换(不过Caffe和Tensorflow的转换器Caffe-Parser和UFF-Parser已经有些落后了)，也提供了三种转换模型的方式：
- 使用TF-TRT，将TensorRT集成在TensorFlow中
- 使用ONNX2TensorRT，即ONNX转换trt的工具
- 手动构造模型结构，然后手动将权重信息挪过去，非常灵活但是时间成本略高，有大佬已经尝试过了：tensorrtx

不过目前TensorRT对ONNX的支持最好，TensorRT-8最新版ONNX转换器又支持了更多的op操作。而深度学习框架中，TensorRT对Pytorch的支持更为友好，除了Pytorch->ONNX->TensorRT这条路，还有：
- torch2trt
- torch2trt_dynamic
- TRTorch

总而言之，理论上95%的模型都可以转换为TensorRT，条条大路通罗马嘛。只不过有些模型可能转换的难度比较大。如果遇到一个无法转换的模型，先不要绝望，再想想，再想想，看看能不能通过其他方式绕过去。

### TensorRT支持哪几种权重精度

支持FP32、FP16、INT8、TF32等，这几种类型都比较常用。
- FP32：单精度浮点型，没什么好说的，深度学习中最常见的数据格式，训练推理都会用到；
- FP16：半精度浮点型，相比FP32占用内存减少一半，有相应的指令值，速度比FP32要快很多；
- TF32：第三代Tensor Core支持的一种数据类型，是一种截短的 Float32 数据格式，将FP32中23个尾数位截短为10bits，而指数位仍为8bits，总长度为19(=1+8 +10)。保持了与FP16同样的精度(尾数位都是 10 位），同时还保持了FP32的动态范围指数位都是8位)；
- INT8：整型，相比FP16占用内存减小一半，有相应的指令集，模型量化后可以利用INT8进行加速。
简单展示下各种精度的区别：
![](https://pic2.zhimg.com/80/v2-e86c8661901842ffaf960bb2abbe37e9_720w.jpg)

## ZeRO -- 突破显存限制

`ZeRO`（Zero Redundancy Optimizer）由NVIDIA开发的分布式深度学习训练技术，解决在大规模模型上训练时由于**显存限制**而导致的性能瓶颈问题。

在传统的分布式训练中，每个工作节点都必须存储完整的**模型参数副本**，使用大型模型时，每个工作节点需要拥有足够的显存才能存储这些参数。

而ZeRO技术通过将模型参数分成多个**分片**，让每个工作节点只需要存储部分参数，从而显著减少了显存占用量。
 
具体而言，ZeRO技术通过以下三个主要组件实现：
1.  ZeRO-Stage：将模型参数划分为**更小的分片**，每个工作节点只需存储自己所负责的参数分片。
2.  ZeRO-Offload：将一部分模型参数存储在**CPU内存**中，从而释放显存空间。
 
通过使用ZeRO技术，可以大幅度提高分布式深度学习训练的效率和规模。同时，由于减少了显存占用量，还可以使用更大的批量大小，从而加速训练过程。

除了ZeRO 外还有 Megatron，Gpip 和 Mesh-TensorFlow 等分布式深度计算方式

另外[FlagAI](https://github.com/FlagAI-Open/FlagAI)也集成了ZeRO的使用方式，具体实现方式使用的是利用[Deepspeed](https://github.com/FlagAI-Open/FlagAI/blob/master/examples/glm_seq2seq/train_deepspeed.py)实现ZeRO1和ZeRO2，利用[bmtrain](https://github.com/FlagAI-Open/FlagAI/blob/master/examples/glm_pretrain/train_bmtrain.py)实现了ZeRO3。
 
> FlagAI 由北京智源人工智能研究院于 2022 年 5 月开源，是大模型算法、模型及各种优化工具的一站式、高质量开源项目，旨在集成全球各种主流大模型算法技术，以及多种大模型并行处理和训练加速技术，支持高效训练和微调，降低大模型开发和应用的门槛，提高大模型的开发效率。项目地址：[https://github.com/FlagAI-Open/FlagAI](https://github.com/FlagAI-Open/FlagAI)

【2023-4-5】参考：[模型并行下利用ZeRO进行显存优化](https://zhuanlan.zhihu.com/p/619429610)
 
### ZeRO-Stage
 
ZeRO将模型训练阶段，每张卡中显存内容分为两类：
1.  模型状态（model states）: 模型参数（fp16）、模型梯度（fp16）和Adam状态（fp32的模型参数备份，fp32的momentum和fp32的variance）。假设模型参数量 Φ ，则共需要 2Φ+2Φ+(4Φ+4Φ+4Φ)=4Φ+12Φ=16Φ 字节存储，可以看到，Adam状态占比 75% 。
2.  剩余状态（residual states）: 除了模型状态之外的显存占用，包括激活值（activation）、各种临时缓冲区（buffer）以及无法使用的显存碎片（fragmentation）。
 
针对模型状态的存储优化（去除冗余），ZeRO使用的方法是分片（partition），即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态。
*   首先进行分片操作的是模型状态中的Adam，也就是图5中的 Pos ，这里os指的是optimizer states。模型参数（parameters）和梯度（gradients）仍旧是每张卡保持一份，此时，每张卡的模型状态所需显存是 4Φ+12Φ/N 字节，当 N 比较大时，趋向于 4ΦB ，也就是原来 16ΦB 的 1/4 。
*   如果继续对模型梯度进行分片，也就是图5中的 Pos+g ，模型参数仍旧是每张卡保持一份，此时，每张卡的模型状态所需显存是 2Φ+(2Φ+12Φ)/N 字节，当 N 比较大时，趋向于 2ΦB ，也即是原来 16ΦB 的 1/8 。
*   如果继续对模型参数进行分片，也就是图5中的 Pos+g+p ，此时每张卡的模型状态所需显存是 16Φ/N 字节，当 N 比较大时，趋向于 0 。

分析下通信数据量，先说结论：
- Pos 和 Pos+g 的通信量和传统数据并行相同，Pos+g+p 会增加通信量。

传统数据数据并行在每一步（step/iteration）计算梯度后，需要进行一次AllReduce操作来计算梯度均值，目前常用的是Ring AllReduce，分为ReduceScatter和AllGather两步，每张卡的通信数据量（发送+接收）近似为 2Φ (\[2\])。
 
直接分析 Pos+g ，每张卡只存储 1N 的优化器状态和梯度，对于 gpu0 来说，为了计算它这 1N 梯度的均值，需要进行一次Reduce操作，通信数据量是 1/N⋅Φ⋅N=Φ ，然后其余显卡则不需要保存这部分梯度值了。实现中使用了bucket策略，保证 1N 的梯度每张卡只发送一次。
 
当 gpu0 计算好梯度均值后，就可以更新局部的优化器状态（包括 1/N⋅Φ 的参数），当反向传播过程结束，进行一次Gather操作，更新 (1−1/N)Φ 的模型参数，通信数据量是 1/N⋅Φ⋅N=Φ 。
 
从全局来看，相当于用Reduce-Scatter和AllGather两步，和数据并行一致，使得每张卡只存了 1/N 的参数，不管是在前向计算还是反向传播，都涉及一次Broadcast操作。
- ![](https://pic2.zhimg.com/80/v2-e28e1ca08f66f19d45a98af3fbfa13e9_1440w.webp)

解决了模型状态，再来看剩余状态，也就是激活值（activation）、临时缓冲区（buffer）以及显存碎片（fragmentation）。
*   激活值同样使用分片方法，并且配合checkpointing
*   模型训练过程中经常会创建一些大小不等的临时缓冲区，比如对梯度进行AllReduce啥的，解决办法就是预先创建一个固定的缓冲区，训练过程中不再动态创建，如果要传输的数据较小，则多组数据bucket后再一次性传输，提高效率
*   显存出现碎片的一大原因是时候gradient checkpointing后，不断地创建和销毁那些不保存的激活值，解决方法是预先分配一块连续的显存，将常驻显存的模型状态和checkpointed activation存在里面，剩余显存用于动态创建和销毁discarded activation

### ZeRO-Offload
 
ZeRO-Offload 是 ZeRO（Zero Redundancy Optimizer）技术的一种**扩展**，它使用显存作为模型参数存储和通信的中间介质，以减少模型并行化训练中的通信和同步开销。
 
ZeRO-Offload技术使用显存缓存将模型参数存储在显存中，这可以减少网络带宽的使用，同时还可以加速参数访问和更新。为了最大限度地减少显存的使用，ZeRO-Offload技术使用了一种称为“按需加载”的策略。这种策略只在需要使用参数时才将其从磁盘或网络加载到显存中，而不是一次性将所有参数都加载到显存中。
- ![](https://pic4.zhimg.com/80/v2-e83a35c1d2a1db0738cc19770be60207_1440w.webp)
 

Offload策略
 
ZeRO-Offload技术的核心是使用显存缓存和显存内通信来降低通信开销。为了最大程度地利用显存并减少网络带宽的使用，ZeRO-Offload技术采用了一种称为“Offload策略”的技术。下面是ZeRO-Offload技术的Offload策略的几个关键点：
1.  按需加载
  - ZeRO-Offload技术使用“按需加载”策略，只在需要使用参数时才将其从磁盘或网络加载到显存中，而不是一次性将所有参数都加载到显存中。这种策略可以最大限度地减少显存的使用，并减少网络带宽的使用。
2. 数据流水线
  - ZeRO-Offload技术使用“数据流水线”策略，将数据流分成多个阶段，每个阶段都使用不同的计算资源进行处理。在模型训练期间，ZeRO-Offload技术将数据分为多个块，并将这些块分配给不同的GPU进行计算。每个GPU只对其分配的数据块进行计算，并将计算结果传递给下一个阶段的GPU，直到所有阶段都完成为止。
3. 显存原语
  - ZeRO-Offload技术使用一种称为“显存原语”的通信协议，在显存中直接进行通信和同步操作，而不需要通过网络或主机内存。这种协议可以显著减少通信延迟和数据传输时间，从而提高训练效率。
4. 数据切片
  - ZeRO-Offload技术使用“数据切片”策略来划分模型参数，并通过显存内通信来实现不同GPU上参数的同步。具体来说，ZeRO-Offload技术将模型参数划分为多个小块，并在每个GPU上存储一部分参数块。在训练过程中，每个GPU只对其分配的参数块进行计算，并通过显存内通信将参数块传输到其他GPU上进行同步。

总的来说，ZeRO-Offload技术的Offload策略通过按需加载、数据流水线、显存原语和数据切片等技术手段来最大化地利用显存，并降低通信开销，从而提高深度学习模型训练的效率和可扩展性。
 
为了找到最优的offload策略，ZeRO作者将模型训练过程看作数据流图（data-flow graph）。
*   圆形节点表示模型状态，比如参数、梯度和优化器状态
*   矩形节点表示计算操作，比如前向计算、后向计算和参数更新
*   边表示数据流向
 
下图是某一层的一次迭代过程（iteration/step），使用了混合精读训练，前向计算（FWD）需要用到上一次的激活值（activation）和本层的参数（parameter），反向传播（BWD）也需要用到激活值和参数计算梯度，
- ![](https://pic3.zhimg.com/80/v2-6a6ae5248f45f1d59fb28aa1369e5f06_1440w.webp)

如果用Adam优化器进行参数更新（Param update），流程如下：
- ![](https://pic1.zhimg.com/80/v2-fb39f875d3a73901b8501cea18630ff0_1440w.webp)

为边添加权重，物理含义是数据量大小（单位是字节），假设模型参数量是 M ，在混合精度训练的前提下，边的权重要么是2M（fp16），要么是4M（fp32）。
- ![](https://pic1.zhimg.com/80/v2-26950ad74ab06e57222caa18c1af3214_1440w.webp)

现在要做的就是沿着边把数据流图切分为两部分，分布对应GPU和CPU，计算节点（矩形节点）落在哪个设备，哪个设备就执行计算，数据节点（圆形）落在哪个设备，哪个设备就负责存储，将被切分的边权重加起来，就是CPU和GPU的通信数据量。
 
ZeRO-Offload的切分思路如图 10 所示：
- ![](https://pic1.zhimg.com/80/v2-a5ded60ef49d1c2e9a0b9f8cf7ae29a8_1440w.webp)

图10中有四个计算类节点：FWD、BWD、Param update和float2half，前两个计算复杂度大致是 O(MB) ， B 是batch size，后两个计算复杂度是 O(M) 。为了不降低计算效率，将前两个节点放在GPU，后两个节点不但计算量小还需要和Adam状态打交道，所以放在CPU上，Adam状态自然也放在内存中，为了简化数据图，将前两个节点融合成一个节点FWD-BWD Super Node，将后两个节点融合成一个节点Update Super Node。
 
所以，现在的计算流程是，在GPU上面进行前向和后向计算，将梯度传给CPU，进行参数更新，再将更新后的参数传给GPU。为了提高效率，可以将计算和通信并行起来，GPU在反向传播阶段，可以待梯度值填满bucket后，一遍计算新的梯度一遍将bucket传输给CPU，当反向传播结束，CPU基本上已经有最新的梯度值了，同样的，CPU在参数更新时也同步将已经计算好的参数传给GPU，如下图所示：
- ![](https://pic3.zhimg.com/80/v2-bac2b7d030141b2a146852a44d5c379a_1440w.webp)

## 大模型训练加速

### PEFT

Parameter-Efficient Fine-Tuning (`PEFT`) 是HuggingFace 开源的一个高效微调大模型库，支持在 LLM 上创建和微调适配器层。
- peft 与  🤗 Accelerate 无缝集成，用于利用了 DeepSpeed 和 Big Model Inference 的大规模模型。

目前包含LoRA，Prefix Tuning，Prompt Tuning，P-Tuning 四种算法
*   LoRA
*   [Prefix Tuning](https://arxiv.org/pdf/2110.07602.pdf)
  - Prefix Tuning 算法是根据 下游任务 "前缀指令文本" 的所有层的embeding表示，学习到的前缀指令文本向量可以挖掘大模型的潜力去引导模型完成特定任务。
  - ![](https://pic3.zhimg.com/80/v2-9a6b5792cf60079429d067fc629e65ae_1440w.webp)
*   [P-Tuning](https://arxiv.org/pdf/2103.10385.pdf)
  - P-Tuning 算法和 Prefix Tuning 的想法很相似，想通过微调"指令文本",让指令文本去挖掘大模型的潜力去完成特定的任务。但是 P-Tuning 只学习 "指令文本" 输入层embeding的的表示。 为了增强 "指令文本"的连续性，采用了一个 MLP(LSTM) 的结果去encoding "指令文本"。从微调参数量来看只有 0.65% 比 Prefix Tuning 和 LoRA 这些在所有层都增加参数的方法要少。
  - ![](https://pic3.zhimg.com/80/v2-7540fb5d913adcae8be308fce31befea_1440w.webp)
*   [Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
  - Prompt Tuning 算法和 P-Tuning 很像，且更简单，就是是根据 下游任务 "指令文本" 输入层embeding的的表示。 Prompt Tuning 没有增加任何的层，直接使用微调指令文本(prompt) 的embeding向量。
  - ![](https://pic3.zhimg.com/80/v2-b281f773be36787dddd0f06e782384b2_1440w.webp)

[详见](https://zhuanlan.zhihu.com/p/618695885)

[Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft) (PEFT)

单个 24GB GPU 上使用上述工具使用 RL 微调 20B 参数量的 LLM, 详见量化[quantization](https://hf.co/docs/transformers/main/en/main_classes/quantization)
- 与全精度模型相比，以 **8位**精度加载模型最多可节省 **4倍**的内存
- 调用 from_pretrained 方法时简单地添加标志 load_in_8bit=True

详见：[在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs](https://mp.weixin.qq.com/s/7nmegO1UYObO0-eUDTKnMg)

### LoRA 低秩适配

2021 年，[LoRA: Low-Rank Adaption of Large Language Models]() 论文表明，可以通过**冻结**预训练权重，并创建查询和值层的注意力矩阵的低秩版本来对大型语言模型进行微调。这些低秩矩阵的参数**远少于**原始模型，因此可以使用更少的 GPU 内存进行微调。作者证明，低阶适配器的微调取得了与微调完整预训练模型相当的结果。

这种技术允许使用小部分内存来微调 LLM。然而，也有缺点
- 由于适配器层中的额外矩阵乘法，前向和反向传递的速度大约是原来的**两倍**。

LoRA 是 Parameter Efficient 的方法之一。
- 过度参数化的模型其实是位于一个低的**内在维度**上，所以作者假设在模型适应过程中的权重变化也具有较低的“内在等级”。
- [LoRA](https://github.com/microsoft/LoRA)的主要方法为**冻结**一个预训练模型的矩阵参数，并选择用A和B矩阵来替代，在下游任务时只更新A和B。
- ![](https://pic4.zhimg.com/80/v2-67cd3e1e603a5bb674463ddc4db38d57_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-f56b07afc29ccad77a6faffa130ab24d_1440w.webp)

LoRA 已经被作者打包到了loralib中。
- pip install loralib

可以选择用loralib中实现的对应层来替换一些层。
- 目前loralib只支持 nn.Linear、nn.Embedding 和 nn.Conv2d。
- loralib还支持一个 MergedLinear，用于单个 nn.Linear 代表一个以上的层的情况，比如在一些关注 qkv 投影的实现中（self- attention）
- ![](https://pic2.zhimg.com/80/v2-bcef352dc1adf7d6f2fad86e1fe892fd_1440w.webp)

```py
# ===== Before =====
layer = nn.Linear(in_features, out_features)

# ===== After ======
import loralib as lora
# Add a pair of low-rank adaptation matrices with rank r=16
layer = lora.Linear(in_features, out_features, r=16)
```

详见原文：[微软LoRA: Low-Rank Adaptation of Large Language Models 代码解读](https://zhuanlan.zhihu.com/p/515954218)



# 结束


