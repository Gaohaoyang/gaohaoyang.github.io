---
layout: post
title:  æç¤ºå­¦ä¹  Prompt learning
date:   2021-11-10 19:58:00
categories: æ·±åº¦å­¦ä¹  è‡ªç„¶è¯­è¨€å¤„ç†
tags: è‡ªç›‘ç£ prompt æ€ç»´é“¾
excerpt: NLPæ–°èŒƒå¼ï¼šPromptï¼ˆæç¤ºå­¦ä¹ ï¼‰
mathjax: true
permalink: /prompt
---

* content
{:toc}

# NLPæ–°èŒƒå¼ï¼šPrompt


## èµ„æ–™

- ã€2023-2-26ã€‘[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide), Motivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest **papers**, learning **guides**, **lectures**, references, and **tools** related to prompt engineering.
- ã€2023-2-16ã€‘æ¸…åå¼€æºçš„ [OpenPrompt](https://github.com/thunlp/OpenPrompt)ï¼šAn Open-Source Framework for Prompt-learning. åŒ…å«promptå„ä¸ªç®—æ³•ã€æ¨¡å—çš„ä»£ç åŠç¤ºä¾‹
  - [PromptPapers](https://github.com/thunlp/PromptPapers)
  - CMUçš„Neubigè€å¸ˆçš„[Advanced NLPè¯¾ç¨‹](http://www.phontron.com/class/anlp2021/schedule/prompting.html)
- ã€2021-8-3ã€‘[Fine-tuneä¹‹åçš„NLPæ–°èŒƒå¼ï¼šPromptè¶Šæ¥è¶Šç«ï¼ŒCMUåäººåšå£«åå‡ºäº†ç¯‡ç»¼è¿°æ–‡ç« ](https://blog.csdn.net/xixiaoyaoww/article/details/119363189)ï¼ŒCMU åšå£«åç ”ç©¶å‘˜åˆ˜é¹é£ï¼šè¿‘ä»£è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•çš„ç¬¬å››èŒƒå¼å¯èƒ½æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åŠ æŒä¸‹çš„ Prompt Learningã€‚
- ä» BERT å¼€å§‹ï¼Œ**é¢„è®­ç»ƒ+finetune** å·²ç»æˆä¸ºäº†æ•´ä¸ªé¢†åŸŸçš„å¸¸è§„èŒƒå¼ã€‚ä½†æ˜¯ä» GPT-3 å¼€å§‹ï¼Œä¸€ç§æ–°èŒƒå¼å¼€å§‹å¼•èµ·å¤§å®¶çš„å…³æ³¨å¹¶è¶Šæ¥è¶Šæµè¡Œï¼š**prompting**ã€‚
- [è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2107.13586.pdf)ï¼Œæ›´å¤šç ”ç©¶è§ï¼šæ¸…åå¤§å­¦å¼€æºçš„è®ºæ–‡åˆ—è¡¨ [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers)
- ![img](https://img-blog.csdnimg.cn/img_convert/e538ffef7d05deaf84a5a66225c7f4bc.png)

promptè®²è§£
- [CMU](https://blender.cs.illinois.edu/course/fall22/lecture9.pdf)

<object type="application/pdf" data="https://blender.cs.illinois.edu/course/fall22/lecture9.pdf"
           id="review" style="width:100%;  height:800px; margin-top:0px;  margin-left:0px" >
</object>

## NLP èŒƒå¼

å…¨ç›‘ç£å­¦ä¹ åœ¨ NLP é¢†åŸŸä¹Ÿéå¸¸é‡è¦ã€‚ä½†æ˜¯å…¨ç›‘ç£çš„æ•°æ®é›†å¯¹äºå­¦ä¹ é«˜è´¨é‡çš„æ¨¡å‹æ¥è¯´æ˜¯ä¸å……è¶³çš„ï¼Œ<span style='color:red'>æ—©æœŸçš„ NLP æ¨¡å‹ä¸¥é‡ä¾èµ–ç‰¹å¾å·¥ç¨‹</span>ã€‚
- éšç€ç”¨äº NLP ä»»åŠ¡çš„ç¥ç»ç½‘ç»œå‡ºç°ï¼Œä½¿å¾—ç‰¹å¾å­¦ä¹ ä¸æ¨¡å‹è®­ç»ƒç›¸ç»“åˆï¼Œç ”ç©¶è€…å°†ç ”ç©¶é‡ç‚¹è½¬å‘äº†**æ¶æ„å·¥ç¨‹**ï¼Œå³é€šè¿‡è®¾è®¡ä¸€ä¸ªç½‘ç»œæ¶æ„èƒ½å¤Ÿå­¦ä¹ æ•°æ®ç‰¹å¾ã€‚

å„ç§æ¨¡å¼çš„å¯¹æ¯”å¦‚ä¸‹ï¼š

|æ¨¡å¼paradigm|å·¥ç¨‹é‡å¿ƒengineering|ç¤ºä¾‹|ä»»åŠ¡å…³ç³»task relation|
|---|---|---|---|
|â‘ å…¨ç›‘ç£ï¼ˆéç¥ç»ç½‘ç»œï¼‰|ç‰¹å¾|å¦‚å•è¯ï¼Œè¯æ€§ï¼Œå¥å­é•¿åº¦ç­‰|åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€è¯­è¨€æ¨¡å‹ï¼ˆæ— ç›‘ç£ï¼‰ã€ç”Ÿæˆ|
|â‘¡å…¨ç›‘ç£ï¼ˆç¥ç»ç½‘ç»œï¼‰|ç»“æ„|å¦‚å·ç§¯ã€å¾ªç¯ã€è‡ªæ³¨æ„åŠ›|åŒä¸Š|
|â‘¢pre-trainä¸fine-tune|ç›®æ ‡|æ©ç è¯­è¨€æ¨¡å‹ã€NSPä¸‹ä¸€å¥é¢„æµ‹|ä»¥è¯­è¨€æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œå«æ— ç›‘ç£è®­ç»ƒ|
|â‘£pre-trainã€promptä¸predict|æç¤º|å®Œå½¢å¡«ç©ºã€å‰ç¼€|è¯­è¨€æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œå«æ–‡æœ¬æç¤º|

- ![img](https://img-blog.csdnimg.cn/img_convert/e7be4976c76f47cf54a95a7dcd2150b9.png)
- ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼ˆä¸éœ€è¦ç¥ç»ç½‘ç»œï¼‰
- ç¥ç»ç½‘ç»œ-ç›‘ç£å­¦ä¹ ï¼šä¸åŒNLPä»»åŠ¡éœ€è¦å•ç‹¬è®­ç»ƒ
- pre-train + fine-tuneï¼šç›®å‰æµè¡Œçš„èŒƒå¼ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„åœºæ™¯ä»»åŠ¡
- pre-train + prompt + predictï¼šæ¨¡æ¿promptèŒƒå¼ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„åœºæ™¯ä»»åŠ¡

ã€2023-2-12ã€‘[ã€NLPã€‘Prompt Learning è¶…å¼ºå…¥é—¨æ•™ç¨‹](https://zhuanlan.zhihu.com/p/442486331), åˆ˜é¹é£åœ¨[åŒ—äº¬æ™ºæºå¤§ä¼š](https://event.baai.ac.cn/activities/172)ä¸Šå…³äº Prompt çš„åˆ†äº«

ã€2023-2-9ã€‘
- [ã€NLPã€‘Prompt Learning è¶…å¼ºå…¥é—¨æ•™ç¨‹](https://zhuanlan.zhihu.com/p/442486331)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)

å¾ˆé•¿ä¸€æ®µæ—¶é—´å†…ï¼ŒNLPä»»åŠ¡é‡‡ç”¨çš„éƒ½æ˜¯ `Pretrain` + `Fine-tuning`ï¼ˆModel Tuningï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œéœ€è¦å¯¹äºæ¯ä¸ªä»»åŠ¡éƒ½é‡æ–° fine-tune ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œä¸”**ä¸èƒ½å…±ç”¨**ã€‚
- ä½†æ˜¯å¯¹äºä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œè¿™å°±ä»¿ä½›å¯¹äºæ¯ä¸ªä»»åŠ¡éƒ½è¿›è¡Œäº†**å®šåˆ¶åŒ–**ï¼Œååˆ†ä¸é«˜æ•ˆã€‚
- æ˜¯å¦å­˜åœ¨ä¸€ç§æ–¹å¼ï¼Œå¯ä»¥å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸º**ç”µæº**ï¼Œä¸åŒçš„ä»»åŠ¡å½“ä½œ**ç”µå™¨**ï¼Œä»…éœ€è¦æ ¹æ®ä¸åŒçš„ç”µå™¨ï¼ˆä»»åŠ¡ï¼‰ï¼Œé€‰æ‹©ä¸åŒçš„**æ’åº§**ï¼Œå¯¹äºæ¨¡å‹æ¥è¯´ï¼Œå³æ’å…¥ä¸åŒçš„ä»»åŠ¡ç‰¹å®šçš„å‚æ•°ï¼Œå°±å¯ä»¥ä½¿å¾—æ¨¡å‹é€‚é…è¯¥ä¸‹æ¸¸ä»»åŠ¡ã€‚
- `Prompt Learning` å°±æ˜¯è¿™ä¸ª**é€‚é…å™¨**ï¼Œå®ƒèƒ½é«˜æ•ˆå¾—è¿›è¡Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä½¿ç”¨ã€‚

è¿™ç§æ–¹å¼å¤§å¤§åœ°æå‡äº†é¢„è®­ç»ƒæ¨¡å‹çš„**ä½¿ç”¨æ•ˆç‡**ï¼Œå¦‚ä¸‹å›¾ï¼š
- ![](https://pic2.zhimg.com/80/v2-ffa9e652a07961216d1ed260dfdea95d_1440w.webp)
*   å·¦è¾¹æ˜¯ä¼ ç»Ÿçš„ `Model Tuning` çš„èŒƒå¼ï¼šå¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œéƒ½éœ€è¦å°†æ•´ä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œ**ç²¾è°ƒ**ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰è‡ªå·±çš„ä¸€æ•´å¥—å‚æ•°ã€‚
*   å³è¾¹æ˜¯`Prompt Tuning`ï¼Œå¯¹äºä¸åŒä»»åŠ¡ï¼Œä»…éœ€è¦æ’å…¥ä¸åŒçš„ promptå‚æ•°ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½å•ç‹¬è®­ç»ƒ Prompt å‚æ•°ï¼Œä¸è®­ç»ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¿™æ ·å­å¯ä»¥å¤§å¤§ç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œä¹Ÿæå¤§çš„æå‡äº†æ¨¡å‹çš„ä½¿ç”¨ç‡ã€‚


### NLPå››å¤§èŒƒå¼

NLPå››å¤§èŒƒå¼
- ç¬¬ä¸€èŒƒå¼ï¼š**éç¥ç»ç½‘ç»œæ—¶ä»£çš„å®Œå…¨ç›‘ç£å­¦ä¹ **ï¼ˆ`ç‰¹å¾å·¥ç¨‹`ï¼‰ã€‚è¯¥é˜¶æ®µéœ€è¦å¤§é‡ä»»åŠ¡ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ç‰¹å¾å·¥ç¨‹å’Œç®—æ³•ï¼Œæ¯”è¾ƒæœ‰ä»£è¡¨çš„ç®—æ³•æ˜¯æœ´ç´ è´å¶æ–¯NaÃ¯ve Bayesã€æ”¯æŒå‘é‡æœºSVMã€é€»è¾‘å›å½’LRç­‰ï¼›
- ç¬¬äºŒèŒƒå¼ï¼š**åŸºäºç¥ç»ç½‘ç»œçš„å®Œå…¨ç›‘ç£å­¦ä¹ **ï¼ˆ`æ¶æ„å·¥ç¨‹`ï¼‰ã€‚è¯¥é˜¶æ®µä¹Ÿéœ€è¦å¤§é‡ä»»åŠ¡ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œè‡ªåŠ¨è·å–ç‰¹å¾(è¡¨ç¤ºå­¦ä¹ )è¿›è¡Œç«¯åˆ°ç«¯åˆ†ç±»å­¦ä¹ ï¼›
- ç¬¬ä¸‰èŒƒå¼ï¼š**é¢„è®­ç»ƒï¼Œç²¾è°ƒèŒƒå¼**ï¼ˆ`ç›®æ ‡å·¥ç¨‹`ï¼‰ï¼šè¯¥é˜¶æ®µæ˜¯å½“å‰ä½¿ç”¨æ¯”è¾ƒå¤šçš„é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„æ–¹å¼(æ¯”å¦‚æ©ç è¯­è¨€æ¨¡å‹Masked Language Model)æ¥å­¦ä¹ æµ·é‡çš„è¯­è¨€å­¦çŸ¥è¯†ï¼Œç„¶åä¸‹æ¸¸ä½¿ç”¨å°‘é‡çš„ä»»åŠ¡ç›¸å…³çš„æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒå³å¯å®Œæˆç›¸å…³ä»»åŠ¡ï¼›
- ç¬¬å››èŒƒå¼ï¼š**é¢„è®­ç»ƒï¼Œæç¤ºï¼Œé¢„æµ‹èŒƒå¼**ï¼ˆ`Promptå·¥ç¨‹`ï¼‰ï¼šå½“å‰è¿›å…¥äº†Prompt Learningæç¤ºå­¦ä¹ çš„æ–°èŒƒå¼ï¼Œä½¿ç”¨Few shotæˆ–è€…Zero shotå³å¯å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚
- ![img](https://pic3.zhimg.com/80/v2-c4227aa430fb3be39172a08274f0c5da_1440w.webp)

é¢„è®­ç»ƒ+å¾®è°ƒå’ŒPrompt Learningæµç¨‹å¯¹æ¯”
- ![](https://pic1.zhimg.com/80/v2-bc80e4117e9cb014decdde51f5685180_1440w.webp)

### NLP èŒƒå¼å‘å±•å†å²

| å¹´ä»½	| NLPæ¨¡å‹èŒƒå¼å˜åŒ– |
|---|---|
| 2017å¹´ä»¥å‰ |	ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ã€ç¥ç»ç½‘ç»œ |
| 2017-2019	| é¢„è®­ç»ƒ + å¾®è°ƒï¼ˆpre-train + fine-tuneï¼‰ |
| 2019-è‡³ä»Š	| â€é¢„è®­ç»ƒï¼Œpromptå’Œé¢„æµ‹â€œï¼ˆpre-trainï¼Œprompt and predictï¼‰èŒƒå¼ |

NLPå‘å±•å†å²ä¸Šçš„ä¸‰ç§èŒƒå¼ï¼ˆæ—¶é—´é¡ºåºï¼‰
*   å¾ˆä¹…ä»¥å‰å‘å±•èµ·æ¥çš„`å…¨ç›‘ç£å­¦ä¹ ` Fully Supervised Learning
  - Fully Supervised Learningï¼Œå³ä»…åœ¨ç›®æ ‡ä»»åŠ¡çš„è¾“å…¥è¾“å‡ºæ ·æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Œé•¿æœŸä»¥æ¥åœ¨è®¸å¤šæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ï¼ŒåŒæ ·çš„ï¼Œå…¨ç›‘ç£å­¦ä¹ åœ¨ NLP é¢†åŸŸä¹Ÿéå¸¸é‡è¦ã€‚
  - ![åŸºäºç¥ç»ç½‘ç»œçš„ç›‘ç£å­¦ä¹ ](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
  - ä½†æ˜¯å…¨ç›‘ç£æ•°æ®é›†å¯¹äºå­¦ä¹ é«˜è´¨é‡çš„æ¨¡å‹æ¥è¯´æ˜¯ä¸å……è¶³çš„ï¼Œ**æ—©æœŸçš„ NLP æ¨¡å‹ä¸¥é‡ä¾èµ–ç‰¹å¾å·¥ç¨‹**ã€‚éšç€ç”¨äº NLP ä»»åŠ¡çš„ç¥ç»ç½‘ç»œå‡ºç°ï¼Œä½¿å¾—ç‰¹å¾å­¦ä¹ ä¸æ¨¡å‹è®­ç»ƒç›¸ç»“åˆï¼Œç ”ç©¶è€…å°†ç ”ç©¶é‡ç‚¹è½¬å‘äº†æ¶æ„å·¥ç¨‹ï¼Œå³é€šè¿‡è®¾è®¡ä¸€ä¸ªç½‘ç»œæ¶æ„èƒ½å¤Ÿå­¦ä¹ æ•°æ®ç‰¹å¾ã€‚
  - ![åŸºäºç¥ç»ç½‘ç»œçš„ç›‘ç£å­¦ä¹ ](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
*   å‰ä¸‰å¹´ç«çˆ†çš„`é¢„è®­ç»ƒ+å¾®è°ƒ`  Pre-train, Fine-tune
  - ç„¶è€Œï¼Œä» 2017-2019 å¹´å¼€å§‹ï¼ŒNLP æ¨¡å‹å‘ç”Ÿäº†ç¿»å¤©è¦†åœ°çš„å˜åŒ–ï¼Œè¿™ç§`å…¨ç›‘ç£èŒƒå¼`å‘æŒ¥çš„ä½œç”¨è¶Šæ¥è¶Šå°ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶é‡ç‚¹å¼€å§‹è½¬å‘é¢„è®­ç»ƒã€Fine-tuningèŒƒå¼ã€‚ä¸€ä¸ªå…·æœ‰å›ºå®šæ¶æ„çš„æ¨¡å‹é€šè¿‡**é¢„è®­ç»ƒ**ä½œä¸º`è¯­è¨€æ¨¡å‹`ï¼ˆLMï¼‰ï¼Œç”¨æ¥é¢„æµ‹è§‚æµ‹åˆ°çš„æ–‡æœ¬æ•°æ®çš„æ¦‚ç‡ã€‚
  - ![é¢„è®­ç»ƒ+å¾®è°ƒ](https://pic1.zhimg.com/80/v2-9b02ff7acb3ac010d683c76dd53a4c14_1440w.webp)
*   æœ€æ–°çš„ `é¢„è®­ç»ƒ+æç¤º+é¢„æµ‹ `  Pre-train, Prompt, Predict  
  - å½“å‰æ­£å¤„äºç¬¬äºŒæ¬¡å·¨å˜ä¸­ï¼Œã€Œé¢„è®­ç»ƒã€Fine-tuningã€è¿‡ç¨‹è¢«ã€Œé¢„è®­ç»ƒã€prompt å’Œé¢„æµ‹ã€è¿‡ç¨‹æ‰€å–ä»£ã€‚
  - åœ¨è¿™ç§èŒƒå¼ä¸­ï¼Œä¸æ˜¯é€šè¿‡ç›®æ ‡å·¥ç¨‹ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œæ˜¯**é‡æ–°å½¢å¼åŒ–**ï¼ˆReformulateï¼‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åƒæ˜¯åœ¨æ–‡æœ¬ prompt çš„å¸®åŠ©ä¸‹åœ¨åŸå§‹ LM è®­ç»ƒæœŸé—´è§£å†³çš„ä»»åŠ¡ã€‚
  - é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé€‰æ‹©é€‚å½“çš„ promptï¼Œè¯¥æ–¹æ³•å¯ä»¥æ“çºµæ¨¡å‹çš„è¡Œä¸ºï¼Œä»¥ä¾¿é¢„è®­ç»ƒçš„ LM æœ¬èº«å¯ä»¥ç”¨äºé¢„æµ‹æ‰€éœ€çš„è¾“å‡ºï¼Œæœ‰æ—¶ç”šè‡³æ— éœ€ä»»ä½•é¢å¤–çš„ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚
  - ä¼˜ç‚¹æ˜¯ç»™å®šä¸€ç»„åˆé€‚çš„ promptï¼Œä»¥**å®Œå…¨æ— ç›‘ç£**æ–¹å¼è®­ç»ƒçš„å•ä¸ª LM å°±èƒ½å¤Ÿç”¨äºè§£å†³å¤§é‡ä»»åŠ¡ã€‚
  - ç„¶è€Œè¯¥æ–¹æ³•ä¹Ÿå­˜åœ¨ä¸€ä¸ªé—®é¢˜ â€”â€” è¿™ç§æ–¹æ³•å¼•å…¥äº† **prompt æŒ–æ˜å·¥ç¨‹**çš„å¿…è¦æ€§ï¼Œå³éœ€è¦æ‰¾å‡ºæœ€åˆé€‚çš„ prompt æ¥è®© LM è§£å†³é¢ä¸´çš„é—®é¢˜

Promptåˆšåˆšå‡ºç°æ—¶ï¼Œè¿˜æ²¡æœ‰è¢«å«åšPromptï¼Œæ˜¯ç ”ç©¶è€…ä»¬ä¸ºäº†ä¸‹æ¸¸ä»»åŠ¡è®¾è®¡å‡ºæ¥çš„ä¸€ç§è¾“å…¥**å½¢å¼**æˆ–**æ¨¡æ¿**ï¼Œå®ƒèƒ½å¤Ÿå¸®åŠ©PLMâ€œå›å¿†â€èµ·è‡ªå·±åœ¨é¢„è®­ç»ƒæ—¶â€œå­¦ä¹ â€åˆ°çš„ä¸œè¥¿ï¼Œå› æ­¤åæ¥æ…¢æ…¢åœ°è¢«å«åšPromptäº†ã€‚
- ![prompt](https://pic1.zhimg.com/80/v2-f152c022e31126bd8520e453899aaadc_1440w.webp)

ä¸åŒäºfine-tuningæ–¹æ³•ï¼Œprompt èŒƒå¼éœ€è¦ç»™å‡ºä¸€ä¸ªå®šä¹‰å¥½çš„æ¨¡æ¿ï¼Œè¿™ä¸ªæ¨¡æ¿å¯ä»¥æ˜¯ç¦»æ•£çš„æˆ–è€…æ˜¯è¿ç»­çš„ï¼Œæ¥æé†’æ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™å­¦ä¹ çš„çŸ¥è¯†ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒçš„ä»»åŠ¡å’Œä¸‹æ¸¸ä»»åŠ¡å¾€å¾€å·®åˆ«è¾ƒå¤§ï¼Œæ¨¡å‹å¯èƒ½ä¼šå­˜åœ¨ç‰¹å®šæ€§é—å¿˜ã€‚

## prompt ä»‹ç»

ä»€ä¹ˆæ˜¯ `Prompt`, Prompt å°±æ˜¯ **æç¤º**ï¼š  
- æœ‰äººå¿˜è®°äº†æŸä¸ªäº‹æƒ…ï¼Œç»™äºˆç‰¹å®šæç¤ºï¼Œä»–å°±å¯ä»¥æƒ³èµ·æ¥
- **ç™½æ—¥ä¾å±±å°½**, å¤§å®¶è‡ªç„¶è€Œç„¶åœ°ä¼šæƒ³èµ·æ¥ä¸‹ä¸€å¥è¯—ï¼š**é»„æ²³å…¥æµ·æµ**ã€‚
- æœç´¢å¼•æ“ï¼Œå¯ä»¥æ ¹æ®è¾“å…¥ï¼Œè¿›è¡Œè¾“å‡ºæç¤ºï¼š
  - ![](https://pic3.zhimg.com/80/v2-81d4c4b774e7910935d97fcde34b3842_1440w.webp)
 
æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼š
- â€œI missed the bus today.â€ å¥å­åç´§è·Ÿç€ç»™å‡ºè¿™æ ·ä¸€ä¸ªpromptï¼šâ€œI felt so _____â€
- â€œEnglish: I missed the bus today. French: ______â€

PLMå°†è‡ªåŠ¨è¡¥å……å•è¯sadï¼Œæˆ–ä½¿ç”¨æ³•è¯­æ¥è¿›è¡Œå¡«ç©ºã€‚

### promptæ¼”å˜è¿‡ç¨‹

ã€2023-2-16ã€‘[ä¸€ä¸ªå°ç™½å¦‚ä½•å­¦å¥½prompt tuning?](https://www.zhihu.com/question/509079916/answer/2757278140)

ä»»åŠ¡ï¼š
- å¯¹æè¿°çš„å•†å“ç±»å‹è¿›è¡Œåˆ†ç±»
- ç¬¬ä¸€å¥éœ€è¦è¢«åˆ†ç±»åˆ°ã€Œæ°´æœã€ç±»åˆ«ä¸­ï¼›
- ç¬¬äºŒå¥åˆ™éœ€è¦åˆ†ç±»åˆ°ã€Œç”µè„‘ã€ç±»åˆ«ä¸­ã€‚

æ ‡æ³¨æ•°æ®é›†ï¼š

```sh
ä»€ä¹ˆè‹¹æœå•Šï¼Œéƒ½æ²¡æœ‰è‹¹æœå‘³ï¼Œæ€ªæ€ªçš„å‘³é“ï¼Œè€Œä¸”ä¸€ç‚¹éƒ½ä¸ç”œï¼Œè¶…çº§éš¾åƒï¼  1
è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚    0
```

#### ï¼ˆ1ï¼‰æ—©æœŸåˆ†ç±»æ–¹æ³•

ç›´è§‰ä¸Šåˆ†ç±»æ–¹å¼ï¼š
- å°†è¯¥é—®é¢˜å»ºæ¨¡æˆä¸€ä¸ª**ä¼ ç»Ÿæ–‡æœ¬åˆ†ç±»**ä»»åŠ¡ï¼Œé€šè¿‡äººå·¥æ ‡æ³¨ï¼Œä¸ºæ¯ä¸€ä¸ªç±»åˆ«è®¾ç½®ä¸€ä¸ªidï¼Œä¾‹å¦‚ï¼š

```json
{
    'ç”µè„‘': 0,
    'æ°´æœ': 1,
    ....
}
```

è¿™ç§æ–¹æ³•å¯è¡Œï¼Œä½†éœ€è¦ã€Œè¾ƒå¤šçš„æ ‡æ³¨æ•°æ®ã€æ‰èƒ½å–å¾—ä¸é”™çš„æ•ˆæœã€‚

#### ï¼ˆ2ï¼‰BERT é¢„è®­ç»ƒ+åˆ†ç±»

ç”±äºå¤§å¤šæ•°**é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆå¦‚BRETï¼‰åœ¨ pretrain çš„æ—¶å€™éƒ½ä½¿ç”¨äº† \[MASK\] token åš MLM ä»»åŠ¡ï¼Œè€Œå®é™…ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¸ä¼šä½¿ç”¨åˆ° \[MASK\] è¿™ä¸ª tokenï¼Œè¿™å°±æ„å‘³ç€ä»Šå¤©è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡æ—¶éœ€è¦è¾ƒå¤šçš„æ•°æ®é›†å»æŠ¹å¹³ä¸Šä¸‹æ¸¸ä»»åŠ¡ä¸ä¸€è‡´çš„ gapã€‚

#### ï¼ˆ3ï¼‰Prompt

å¦‚æœæ²¡æœ‰è¶³å¤Ÿå¤šçš„è®­ç»ƒæ•°æ®å‘¢ï¼Ÿ
- prompt learning å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå°† \[MASK\] çš„ token å¼•å…¥åˆ°äº†ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡æ„é€ æˆå’Œ MLM ç±»ä¼¼çš„ä»»åŠ¡ã€‚

ä¸Šè¿°è¯„è®ºæ”¹å†™ä¸ºï¼š

```sh
è¿™æ˜¯ä¸€æ¡[MASK][MASK]è¯„è®ºï¼šè¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚
```

ç„¶åè®©æ¨¡å‹å»é¢„æµ‹ä¸¤ä¸ª \[MASK\] token çš„çœŸå®å€¼æ˜¯ä»€ä¹ˆï¼Œé‚£æ¨¡å‹æ ¹æ®**ä¸Šä¸‹æ–‡**èƒ½æ¨æµ‹å‡ºè¢«æ©ç ä½çš„è¯åº”è¯¥ä¸ºã€Œç”µè„‘ã€ã€‚ç”±äºä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿä½¿ç”¨äº†å’Œé¢„è®­ç»ƒä»»åŠ¡ä¸­åŒæ ·çš„ MLM ä»»åŠ¡ï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ•°æ®æ¥è¿›è¡Œå¾®è°ƒäº†ã€‚

#### ï¼ˆ4ï¼‰p-tuning

ä½†ï¼Œè¿™è¿˜ä¸æ˜¯ `P-tuning`ã€‚

å¯è§ï¼Œæ„å»ºå¥å­æœ€å…³é”®çš„éƒ¨åˆ†æ˜¯åœ¨äº **promptç”Ÿæˆ**ï¼Œå³ï¼š

```sh
ã€Œè¿™æ˜¯ä¸€æ¡[MASK][MASK]è¯„è®ºï¼šã€(prompt) + è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚(content)
```

è¢«æ‹¬å·æ‹¬èµ·æ¥çš„å‰ç¼€ï¼ˆpromptï¼‰çš„ç”Ÿæˆéå¸¸é‡è¦ï¼Œä¸åŒ prompt ä¼šæå¤§å½±å“æ¨¡å‹å¯¹ \[MASK\] é¢„æµ‹çš„æ­£ç¡®ç‡ã€‚

é‚£ä¹ˆè¿™ä¸ª prompt æ€ä¹ˆç”Ÿæˆå‘¢ï¼Ÿ
- å½“ç„¶å¯ä»¥é€šè¿‡**äººå·¥å»è®¾è®¡**å¾ˆå¤šä¸åŒç±»å‹çš„å‰ç¼€ promptï¼Œå³ `prompt pattern`ï¼Œä¾‹å¦‚ï¼š

```sh
è¿™æ˜¯ä¸€æ¡[MASK][MASK]è¯„è®ºï¼š
ä¸‹é¢æ˜¯ä¸€æ¡æè¿°[MASK][MASK]çš„è¯„è®ºï¼š
[MASK][MASK]ï¼š
...
```

ä½†æ˜¯äººå·¥åˆ—è¿™ç§ `prompt pattern` éå¸¸éº»çƒ¦ï¼Œä¸åŒæ•°æ®é›†æ‰€éœ€è¦çš„ prompt pattern ä¹Ÿä¸åŒï¼Œå¯å¤ç”¨æ€§å¾ˆä½ã€‚

é‚£ä¹ˆï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½é€šè¿‡æœºå™¨è‡ªå·±å»å­¦ä¹  prompt pattern å‘¢ï¼Ÿå¯ä»¥ï¼Œ`P-tuning`ã€‚

ä½œè€…ï¼š[ä½•æ](https://www.zhihu.com/question/509079916/answer/2757278140)

### ä»€ä¹ˆæ˜¯ promptï¼Ÿ

NLPä¸­ `Prompt` ä»£è¡¨çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
*   prompt å°±æ˜¯ç»™ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ çš„ä¸€ä¸ª**çº¿ç´¢**/**æç¤º**ï¼Œæ›´å¥½çš„ç†è§£ äººç±»çš„é—®é¢˜ã€‚

ä¸‹å›¾çš„ BERT/BART/ERNIE å‡ä¸ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¯¹äºäººç±»æå‡ºçš„é—®é¢˜ä»¥åŠçº¿ç´¢ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥ç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚
- ![](https://pic4.zhimg.com/80/v2-f09af919f520b65363e38d8340ac4e4f_1440w.webp)
*   æ ¹æ®æç¤ºï¼ŒBERTèƒ½å›ç­”ï¼ŒJDK æ˜¯ _Oracle_ ç ”å‘çš„
*   æ ¹æ® `TL;DR:` çš„æç¤ºï¼ŒBARTçŸ¥é“äººç±»æƒ³è¦é—®çš„æ˜¯æ–‡ç« çš„æ‘˜è¦
*   æ ¹æ®æç¤ºï¼ŒERNIE çŸ¥é“äººç±»æƒ³è¦é—®é¸Ÿç±»çš„èƒ½åŠ›--é£è¡Œ

Prompt æ›´ä¸¥è°¨çš„å®šä¹‰å¦‚ä¸‹ï¼š
> - Prompt is the technique of making better use of the knowledge from the pre-trained model by adding additional texts to the input.  
> - Prompt æ˜¯ä¸€ç§ä¸ºäº†æ›´å¥½çš„ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œé‡‡ç”¨åœ¨è¾“å…¥æ®µæ·»åŠ **é¢å¤–æ–‡æœ¬**çš„æŠ€æœ¯ã€‚
 
*   ç›®çš„ï¼šæ›´å¥½æŒ–æ˜é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›
*   æ‰‹æ®µï¼šåœ¨è¾“å…¥ç«¯æ·»åŠ æ–‡æœ¬ï¼Œå³é‡æ–°å®šä¹‰ä»»åŠ¡ï¼ˆtask reformulationï¼‰

### Prompt æ¦‚è¦

è¯¥ç»¼è¿°ç ”ç©¶è¯•å›¾é€šè¿‡æä¾› prompting æ–¹æ³•çš„æ¦‚è¿°å’Œå½¢å¼åŒ–å®šä¹‰ï¼Œä»¥åŠä½¿ç”¨è¿™äº› prompt çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ¦‚è¿°ï¼Œæ¥æ¢³ç†è¿™ä¸€è¿…é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰çŸ¥è¯†çŠ¶æ€ã€‚ç„¶åè¯¥è®ºæ–‡å¯¹ prompt æ–¹æ³•è¿›è¡Œäº†æ·±å…¥çš„è®¨è®ºï¼ŒåŒ…æ‹¬ **promptå·¥ç¨‹**ã€**answerå·¥ç¨‹**ç­‰åŸºç¡€å’Œ**å¤špromptå­¦ä¹ **æ–¹æ³•ã€**promptç›¸å…³çš„è®­ç»ƒæ–¹æ³•**ç­‰æ›´é«˜çº§çš„æ¦‚å¿µã€‚

ç„¶åï¼Œè¯¥ç ”ç©¶åˆ—å‡ºäº†å·²æœ‰çš„åŸºäº prompt å­¦ä¹ æ–¹æ³•çš„å¤šç§åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†ä¸åŒåº”ç”¨åœºæ™¯ä¸­å¦‚ä½•é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ–¹æ³•ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°è¯•åœ¨ç ”ç©¶ç”Ÿæ€ç³»ç»Ÿä¸­å®šä½ prompt æ–¹æ³•çš„å½“å‰çŠ¶æ€ï¼Œå¹¶ä¸å…¶ä»–ç ”ç©¶é¢†åŸŸå»ºç«‹è”ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºä¸€äº›å¯èƒ½é€‚åˆè¿›ä¸€æ­¥ç ”ç©¶çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå¹¶é’ˆå¯¹å½“å‰ç ”ç©¶è¶‹åŠ¿è¿›è¡Œäº†åˆ†æã€‚

åŸºäº Prompt çš„å­¦ä¹ æ–¹æ³•è¯•å›¾é€šè¿‡**å­¦ä¹ LM**æ¥è§„é¿è¿™ä¸€é—®é¢˜ï¼Œè¯¥ LM å¯¹æ–‡æœ¬ x æœ¬èº«çš„æ¦‚ç‡ P(x;Î¸) è¿›è¡Œå»ºæ¨¡å¹¶ä½¿ç”¨è¯¥æ¦‚ç‡æ¥é¢„æµ‹ yï¼Œä»è€Œå‡å°‘æˆ–æ¶ˆé™¤äº†è®­ç»ƒæ¨¡å‹å¯¹å¤§å‹ç›‘ç£æ•°æ®é›†çš„éœ€æ±‚ã€‚

æœ€åŸºæœ¬çš„ Prompt å½¢å¼çš„æ•°å­¦æè¿°ï¼ŒåŒ…å«è®¸å¤šæœ‰å…³ Prompt çš„å·¥ä½œï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å†…å®¹ã€‚

åŸºç¡€ Prompt åˆ†ä¸‰æ­¥é¢„æµ‹å¾—åˆ†æœ€é«˜çš„ ^yï¼Œå³ï¼š**prompt æ·»åŠ **ã€**answer æœç´¢**å’Œ **answer æ˜ å°„**ã€‚prompting æ–¹æ³•çš„æœ¯è¯­å’Œç¬¦å·ã€‚
- ![img](https://img-blog.csdnimg.cn/img_convert/c441c670a31e4b2669da835842b9f171.png)
ä¸åŒä»»åŠ¡çš„è¾“å…¥ã€æ¨¡æ¿å’Œ answer ç¤ºä¾‹ï¼š
- ![img](https://img-blog.csdnimg.cn/img_convert/90e84a4ad6cf465e3e306fc376581bcf.png)

### Promptè®¾è®¡æ€è·¯

Prompting è®¾è®¡è€ƒè™‘ï¼š
- é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ï¼šæœ‰è®¸å¤šé¢„è®­ç»ƒ LM å¯ä»¥ç”¨æ¥è®¡ç®— P(x; Î¸)ã€‚åœ¨ç¬¬ 3 ç« ä¸­ï¼Œç ”ç©¶è€…å¯¹é¢„è®­ç»ƒ LM è¿›è¡Œäº†åˆæ­¥çš„ä»‹ç»ï¼›
- **Prompt å·¥ç¨‹**ï¼šå¦‚æœ prompt æŒ‡å®šäº†ä»»åŠ¡ï¼Œé‚£ä¹ˆé€‰æ‹©æ­£ç¡®çš„ prompt ä¸ä»…å¯¹å‡†ç¡®ç‡å½±å“å¾ˆå¤§ï¼Œè€Œä¸”å¯¹æ¨¡å‹é¦–å…ˆæ‰§è¡Œçš„ä»»åŠ¡ä¹Ÿæœ‰å¾ˆå¤§å½±å“ã€‚åœ¨ç¬¬ 4 ç« ä¸­ï¼Œç ”ç©¶è€…è®¨è®ºäº†åº”è¯¥é€‰æ‹©å“ªä¸ª prompt ä½œä¸º f_prompt(x) æ–¹æ³•ï¼›
- **Answer å·¥ç¨‹**ï¼šæ ¹æ®ä»»åŠ¡çš„ä¸åŒï¼Œä¼šæœ‰ä¸åŒçš„æ–¹å¼è®¾è®¡ Z (Answer)ï¼Œå¯èƒ½ä¼šå’Œæ˜ å°„å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚åœ¨ç¬¬ 5 ç« ä¸­ï¼Œè¯¦ç»†ä»‹ç»äº†ä¸åŒçš„è®¾è®¡æ–¹å¼ï¼›
- **æ‰©å±•èŒƒå¼**ï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œ ä¸Šé¢çš„å…¬å¼ä»…ä»…ä»£è¡¨äº†å„ç§åº•å±‚æ¡†æ¶ä¸­æœ€ç®€å•çš„ä¸€ç§ï¼Œè¿™äº›æ¡†æ¶å·²ç»è¢«æè®®ç”¨äºæ‰§è¡Œå„ç§ promptingã€‚åœ¨ ç¬¬ 6 ç« ä¸­ï¼Œç ”ç©¶è€…è®¨è®ºäº†æ‰©å±•è¿™ç§åŸºæœ¬èŒƒå¼ä»¥è¿›ä¸€æ­¥æé«˜ç»“æœæˆ–é€‚ç”¨æ€§çš„æ–¹æ³•ï¼›
- åŸºäº Prompt çš„**è®­ç»ƒç­–ç•¥**ï¼šåœ¨ç¬¬ 7 ç« ä¸­ï¼Œç ”ç©¶è€…æ€»ç»“äº†ä¸åŒçš„è®­ç»ƒç­–ç•¥å¹¶è¯¦ç»†è¯´æ˜å®ƒä»¬çš„ç›¸å¯¹ä¼˜åŠ¿
- ![img](https://img-blog.csdnimg.cn/img_convert/eae05d97522535d2a976353daae592c2.png)

## Prompt å·¥ä½œæµ

æ¸…åï¼š[OpenPrompt](https://github.com/thunlp/OpenPrompt)
- ![prompt](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Prompt çš„å·¥ä½œæµåŒ…å«ä»¥ä¸‹4éƒ¨åˆ†ï¼š
1.  Prompt **æ¨¡ç‰ˆ**ï¼ˆTemplateï¼‰æ„é€ *   
2.  Prompt **ç­”æ¡ˆç©ºé—´æ˜ å°„**ï¼ˆVerbalizerï¼‰çš„æ„é€     
3.  æ–‡æœ¬ä»£å…¥templateï¼Œå¹¶ä¸”ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œ**é¢„æµ‹**    
4.  å°†é¢„æµ‹ç»“æœ**æ˜ å°„å›**labelã€‚
 
å…·ä½“çš„æ­¥éª¤å¦‚ä¸‹å›¾ï¼Œæ‹†è§£åˆ†æã€‚
-  ![workflow](https://pic4.zhimg.com/80/v2-65b25d4895d4d7b81d747282cdb4c7f3_1440w.webp)
 
### Step 1: prompt constructionã€Templateã€‘

é¦–å…ˆæ„å»ºä¸€ä¸ªæ¨¡ç‰ˆTemplateï¼Œä½œç”¨æ˜¯å°†è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œ**é‡æ–°æ„é€ **ï¼Œå˜æˆä¸€ä¸ªæ–°çš„å¸¦æœ‰mask slotsçš„æ–‡æœ¬ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
*   å®šä¹‰ä¸€ä¸ªæ¨¡ç‰ˆï¼ŒåŒ…å«äº†2å¤„ä»£å¡«å…¥çš„slotsï¼š\[x\] å’Œ \[z\]
*   å°†\[x\] ç”¨è¾“å…¥æ–‡æœ¬ä»£å…¥

ä¾‹å¦‚ï¼š
*   è¾“å…¥ï¼š<span style='color:green'>x = æˆ‘å–œæ¬¢è¿™ä¸ªç”µå½±</span>ã€‚
*   æ¨¡ç‰ˆï¼š<span style='color:blue'>\[x\]æ€»è€Œè¨€ä¹‹ï¼Œå®ƒæ˜¯ä¸€ä¸ª\[z\]ç”µå½±</span>ã€‚
*   ä»£å…¥ï¼ˆpromptingï¼‰ï¼š<span style='color:green'>æˆ‘å–œæ¬¢è¿™ä¸ªç”µå½±ã€‚æ€»è€Œè¨€ä¹‹ï¼Œå®ƒæ˜¯ä¸€ä¸ª\[z\]ç”µå½±</span>ã€‚
- ![](https://pic2.zhimg.com/80/v2-e6c4edefdb2498229dfa37f9fc883f15_1440w.webp)
 
### Step 2: answer constructionã€Verbalizerã€‘
 
å¯¹äºæ„é€ çš„promptï¼Œè¦çŸ¥é“é¢„æµ‹è¯å’Œlabel ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”ä¸å¯èƒ½è¿è¡Œz(ä»»æ„è¯)ï¼Œå°±éœ€è¦ä¸€ä¸ª`æ˜ å°„å‡½æ•°`ï¼ˆmapping functionï¼‰å°†è¾“å‡ºçš„è¯ä¸labelè¿›è¡Œæ˜ å°„ã€‚
- è¾“å‡ºçš„label æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯ ğŸ˜„ï¼Œä¸€ä¸ªæ˜¯ ğŸ˜­ï¼Œå¯ä»¥é™å®šï¼Œå¦‚æœé¢„æµ‹è¯æ˜¯`fantastic` åˆ™å¯¹åº” ğŸ˜„ï¼Œå¦‚æœæ˜¯ `boring` åˆ™å¯¹åº” ğŸ˜­.
- ![](https://pic3.zhimg.com/80/v2-6c3ab4435a08d559c69d2b46b18a5d1e_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-4708199326266b548a6b3e0361b1bb47_1440w.webp)
 
### Step 3: answer predictionã€Predictionã€‘
 
åªéœ€è¦é€‰æ‹©[åˆé€‚çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹](https://huggingface.co/docs/transformers/model_summary)ï¼Œç„¶åè¿›è¡Œ mask slots \[z\] çš„é¢„æµ‹ã€‚ä¾‹å¦‚ä¸‹å›¾ï¼Œå¾—åˆ°äº†ç»“æœ `fantastic`, å°†å…¶ä»£å…¥\[z\] ä¸­ã€‚
- ![](https://pic2.zhimg.com/80/v2-c12486224648f205c3c8199101a06b75_1440w.webp)

### Step 4: answer-label mappingã€Mappingã€‘
 
ç¬¬å››æ­¥éª¤ï¼Œå¯¹äºå¾—åˆ°çš„ `answer`ï¼Œç”¨ `Verbalizer` å°†å…¶æ˜ å°„å›åŸæœ¬çš„labelã€‚
- ä¾‹å¦‚ï¼šfantastic æ˜ å°„å› labelï¼š
- ![](https://pic2.zhimg.com/80/v2-b9a60cb83f1c6772490053801d13885d_1440w.webp)
 
### æ€»ç»“

- ![](https://pic1.zhimg.com/80/v2-45666504e1714ef274be6ed35a86d388_1440w.webp)
 
## Prompt-based æ–¹æ³•çš„å·¥ç¨‹é€‰æ‹©é—®é¢˜

åœ¨çŸ¥ä¹ä¸­æœ‰ä¸ªæé—®ï¼š
> ç°ä»£çš„deep learning å°±æ˜¯ä¸ºäº†è§„é¿ feature engineeringï¼Œå¯æ˜¯prompt è¿™è¾¹é€‰æ‹©äº† template å’Œ answer ä¸è¿˜æ˜¯ feature engineeringå—ï¼Ÿ

ç¡®å®, å¦‚æœä½¿ç”¨ BERT çš„ fine-tuning èŒƒå¼ï¼ˆä¸‹å›¾å·¦ï¼‰ï¼Œä¸éœ€è¦ä½¿ç”¨ä»»ä½•çš„äººå·¥ç‰¹å¾æ„é€ ï¼Œè€Œä½¿ç”¨ prompt-based çš„æ–¹æ³•çš„è¯ï¼Œéœ€è¦äººå·¥å‚ä¸çš„éƒ¨åˆ†åŒ…å«äº†ä»¥ä¸‹éƒ¨åˆ†ï¼š
*   template æ„é€ 
*   answer æ„é€ 
*   é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©
*   prompt çš„ç»„åˆé—®é¢˜é€‰æ‹©
*   ä»¥åŠè®­ç»ƒç­–ç•¥çš„é€‰æ‹©ç­‰
- ![](https://pic3.zhimg.com/80/v2-011fccce5f1d7367c243def5d3da4e32_1440w.webp)
 
ä¼šå…ˆè¿›è¡Œæ¯ä¸ªéœ€è¦äººå·¥engineering çš„éƒ¨åˆ†è¿›è¡Œè¯¦ç»†è®²è§£ï¼Œç„¶åå†åˆ†æä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦prompt è¿™ç§èŒƒå¼ã€‚
 
### Prompt Template Engineeringï¼ˆPromptæ¨¡ç‰ˆå·¥ç¨‹ï¼‰
 
å¦‚ä½•æ„é€ åˆé€‚çš„Prompt æ¨¡ç‰ˆï¼Ÿå¯¹äºåŒä¸€ä¸ªä»»åŠ¡ï¼Œä¸åŒçš„äººå¯èƒ½æ„é€ ä¸åŒçš„Templateã€‚
- ![](https://pic3.zhimg.com/80/v2-6b1eef9478acd01687934bcaa07a3d0a_1440w.webp)

ä¸”æ¯ä¸ªæ¨¡ç‰ˆéƒ½å…·æœ‰åˆç†æ€§ã€‚Tempalteçš„é€‰æ‹©ï¼Œå¯¹äºPromptä»»åŠ¡èµ·åˆ°äº†å¾ˆé‡å¤§çš„ä½œç”¨ï¼Œå°±ç®—ä¸€ä¸ªwordçš„åŒºåˆ«ï¼Œä¹Ÿå‘å¯¼è‡´10å‡ ä¸ªç‚¹çš„æ•ˆæœå·®åˆ«ï¼Œè®ºæ–‡[GPT Understands, Too](https://arxiv.org/abs/2103.10385) ç»™å‡ºäº†å¦‚ä¸‹çš„ç»“æœï¼š
- ![](https://pic1.zhimg.com/80/v2-b98e6f18abfad252f96b04a549cc9898_1440w.webp)
 
å¯¹äºä¸åŒçš„templateï¼Œå¯ä»¥ä»ä»¥ä¸‹ä¸¤ç§è§’åº¦è¿›è¡ŒåŒºåˆ†ï¼š
1.  æ ¹æ®**slot çš„å½¢çŠ¶/ä½ç½®**åŒºåˆ†
  *   1.1 `å®Œå½¢å¡«ç©º`ï¼ˆClozeï¼‰çš„æ¨¡å¼ï¼Œå³æœªçŸ¥çš„slotåœ¨templateçš„ä¸­é—´ç­‰ä¸å®šçš„ä½ç½®
  *   1.2 `å‰ç¼€æ¨¡å¼`ï¼ˆPrefixï¼‰ï¼ŒæœªçŸ¥çš„slotåœ¨templateçš„å¼€å¤´
1.  æ ¹æ®æ˜¯å¦**ç”±äººæŒ‡å®š**çš„æ¥åŒºåˆ†
  *   2.1 `äººå·¥æŒ‡å®š` template
  *   2.2 `è‡ªåŠ¨æœç´¢` template
  *   2.3 `Discrete` ç¦»æ•£Templateï¼Œå³æœç´¢çš„ç©ºé—´æ˜¯ç¦»æ•£çš„ï¼Œä¸ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å­—å…¸é‡Œçš„å­—ç¬¦ã€‚
  *   2.4 `Continuous` è¿ç»­Templateï¼Œå³æœç´¢çš„ç©ºé—´æ˜¯è¿ç»­çš„ï¼Œå› ä¸ºæ‰€æœ‰æ–°å¢çš„è¿™äº›promptçš„å‚æ•°ä¸»è¦æ˜¯ä¸ºäº†è®©æœºå™¨æ›´å¥½åœ°æœåŠ¡äºä»»åŠ¡ï¼Œæ‰€ä»¥å…¶å‚æ•°çš„å–å€¼ç©ºé—´ä¸éœ€è¦é™å®šåœ¨ç‰¹å®šçš„å–å€¼èŒƒå›´å†…ï¼Œå¯ä»¥æ˜¯è¿ç»­çš„ç©ºé—´ã€‚

å…·ä½“çš„æ€ç»´å¯¼å›¾å¦‚ä¸‹ï¼š
- ![](https://pic3.zhimg.com/80/v2-c96d89a06ca2ec58e31a7cd2b4f30e7e_1440w.webp)

#### P-Tuning è‡ªåŠ¨ç”Ÿæˆæ¨¡æ¿

[è¯¦è§£å½“å‰å¤§ç«çš„æç¤ºå­¦ä¹ prompt learning](https://zhuanlan.zhihu.com/p/594767132)

è‡ªåŠ¨å­¦ä¹ æ¨¡æ¿å¯ä»¥åˆ†ä¸º`ç¦»æ•£`ï¼ˆDiscrete Promptsï¼‰å’Œ`è¿ç»­`ï¼ˆContinuous Promptsï¼‰ä¸¤å¤§ç±»ã€‚
- `ç¦»æ•£`ä¸»è¦åŒ…æ‹¬ Prompt Mining, Prompt Paraphrasing, Gradient-based Search, Prompt Generation å’Œ Prompt Scoringï¼›
- `è¿ç»­`åˆ™ä¸»è¦åŒ…æ‹¬Prefix Tuning, Tuning Initialized with Discrete Prompts å’Œ Hard-Soft Prompt Hybrid Tuning

ä½œè€…ï¼š[ä½•æ](https://www.zhihu.com/question/509079916/answer/2757278140)

äººå·¥æ„å»ºæ¨¡æ¿å¯¹äººç±»æ¥è®²æ˜¯åˆç†çš„ï¼Œä½†æ˜¯åœ¨æœºå™¨çœ¼ä¸­ï¼Œprompt pattern é•¿æˆä»€ä¹ˆæ ·çœŸçš„å…³é”®å—ï¼Ÿ
- æœºå™¨å¯¹è‡ªç„¶è¯­è¨€çš„ç†è§£å’Œäººç±»å¯¹ç†è§£å¤§ä¸ç›¸åŒï¼Œæ›¾ç»æœ‰åšä¸€ä¸ª model attention å’Œäººç±»å¯¹è¯­è¨€é‡è¦æ€§çš„ç†è§£çš„[å¯¹æ¯”å®éªŒ](https://www.zhihu.com/zvideo/1569770114018127872)ï¼Œå‘ç°æœºå™¨å¯¹è¯­è¨€çš„ç†è§£å’Œäººç±»æ˜¯å­˜åœ¨ä¸€å®šçš„åå·®çš„ã€‚

é‚£æ˜¯ä¸æ˜¯ä¸ç”¨ç‰¹æ„ä¸ºæ¨¡å‹å»è®¾å®šä¸€å †æˆ‘ä»¬è§‰å¾—ã€Œåˆç†ã€çš„ prompt patternï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªå·±å»æ‰¾ã€Œåˆç†ã€çš„prompt pattern å‘¢ï¼Ÿ

p-tuning
- è®ºæ–‡ï¼š[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
- ä»£ç ï¼š[p-tuning](https://github.com/HarderThenHarder/transformers_tasks/tree/main/prompt_tasks/p-tuning)

P-Tuning çš„è®­ç»ƒä¸€å…±åˆ†ä¸ºï¼šprompt token(s) ç”Ÿæˆã€mask label ç”Ÿæˆã€mlm loss è®¡ç®— ä¸‰ä¸ªæ­¥éª¤ã€‚
- ï¼ˆ1ï¼‰prompt token(s) ç”Ÿæˆ
- ï¼ˆ2ï¼‰mask label ç”Ÿæˆ
- ï¼ˆ3ï¼‰mlm loss è®¡ç®—

ï¼ˆ1ï¼‰prompt token(s) ç”Ÿæˆ

éšä¾¿ç”Ÿæˆä¸€ä¸ªæ¨¡æ¿æ‰”ç»™æ¨¡å‹
- é€‰ç”¨ä¸­æ–‡ BERT ä½œä¸º backbone æ¨¡å‹ï¼Œé€‰ç”¨ vocab.txt ä¸­çš„ \[unused\] token ä½œä¸ºæ„æˆ prompt æ¨¡æ¿çš„**å…ƒç´ **ã€‚
- \[unused\] æ˜¯ BERT è¯è¡¨é‡Œé¢„ç•™å‡ºæ¥çš„æœªä½¿ç”¨çš„ tokenï¼Œæœ¬èº«æ²¡æœ‰ä»€ä¹ˆå«ä¹‰ï¼Œéšæ„ç»„åˆä¹Ÿä¸ä¼šäº§ç”Ÿå¾ˆå¤§çš„è¯­ä¹‰å½±å“ï¼Œè¿™ä¹Ÿæ˜¯ä½¿ç”¨å®ƒæ¥æ„å»º prompt æ¨¡æ¿çš„åŸå› ã€‚

é‚£ä¹ˆï¼Œæ„å»ºå‡ºæ¥çš„ prompt pattern å°±é•¿è¿™æ ·ï¼š

```sh
[unused1][unused2][unused3][unused4][unused5][unused6]
```

ï¼ˆ2ï¼‰mask label ç”Ÿæˆ

å®Œæˆ prompt æ¨¡æ¿æ„å»ºåï¼Œè¿˜éœ€è¦æŠŠ mask label ç»™åŠ åˆ°å¥å­ä¸­ï¼Œå¥½è®©æ¨¡å‹å®Œæˆæ ‡ç­¾é¢„æµ‹ä»»åŠ¡ã€‚

è®¾å®š label çš„é•¿åº¦ä¸º2ï¼ˆ'æ°´æœ'ã€'ç”µè„‘'ï¼Œéƒ½æ˜¯ 2 ä¸ªå­—çš„é•¿åº¦ï¼‰ï¼Œå¹¶å°† label å¡åˆ°å¥å­çš„å¼€å¤´ä½ç½®ï¼š

```sh
[CLS][MASK][MASK]è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚[SEP]
```

å…¶ä¸­ \[MASK\] token å°±æ˜¯éœ€è¦æ¨¡å‹é¢„æµ‹çš„æ ‡ç­¾ tokenï¼Œç°åœ¨æŠŠä¸¤ä¸ªéƒ¨åˆ†æ‹¼èµ·æ¥ï¼š

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚[SEP]
```

è¿™å°±æ˜¯æœ€ç»ˆè¾“å…¥ç»™æ¨¡å‹çš„æ ·æœ¬ã€‚


ï¼ˆ3ï¼‰mlm loss è®¡ç®—

å¼€å§‹è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå–‚ç»™æ¨¡å‹è¿™æ ·çš„æ•°æ®ï¼š

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚[SEP]
```

å¹¶è·å¾—æ¨¡å‹é¢„æµ‹ \[MASK\] token çš„é¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®—å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„ CrossEntropy Lossã€‚

P-Tuning ä¸­æ ‡ç­¾æ•°æ®é•¿è¿™æ ·ï¼š

```
æ°´æœ    ä»€ä¹ˆè‹¹æœå•Šï¼Œéƒ½æ²¡æœ‰è‹¹æœå‘³ï¼Œæ€ªæ€ªçš„å‘³é“ï¼Œè€Œä¸”ä¸€ç‚¹éƒ½ä¸ç”œï¼Œè¶…çº§éš¾åƒï¼
ç”µè„‘    è¿™ç ´ç¬”è®°æœ¬é€Ÿåº¦å¤ªæ…¢äº†ï¼Œå¡çš„ä¸è¦ä¸è¦çš„ã€‚
...
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œéœ€è¦è®¡ç®—çš„æ˜¯æ¨¡å‹å¯¹ \[MASK\] token çš„è¾“å‡ºä¸ã€Œç”µè„‘ã€è¿™ä¸¤ä¸ªæ ‡ç­¾ token ä¹‹é—´çš„ CrossEntropy Lossï¼Œä»¥æ•™ä¼šæ¨¡å‹åœ¨è¿™æ ·çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¢« \[MASK\] ä½çš„æ ‡ç­¾åº”è¯¥è¢«è¿˜åŸæˆã€Œç‰©å“ç±»åˆ«ã€ã€‚

å®éªŒ

é€‰ç”¨ 63 æ¡è¯„è®ºï¼ˆ8 ä¸ªç±»åˆ«ï¼‰çš„è¯„è®ºä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œåœ¨ 417 æ¡è¯„è®ºä¸Šä½œåˆ†ç±»æµ‹è¯•ï¼Œæ¨¡å‹ F1 èƒ½æ”¶æ•›åœ¨ 76%ã€‚
- é€šè¿‡å®éªŒç»“æœæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸºäº prompt çš„æ–¹å¼å³ä½¿åœ¨è®­ç»ƒæ ·æœ¬æ•°è¾ƒå°çš„æƒ…å†µä¸‹æ¨¡å‹ä¹Ÿèƒ½å–å¾—è¾ƒä¸ºä¸é”™çš„æ•ˆæœã€‚
- ç›¸æ¯”äºä¼ ç»Ÿçš„åˆ†ç±»æ–¹å¼ï¼ŒP-Tuning èƒ½å¤Ÿæ›´å¥½çš„ç¼“è§£æ¨¡å‹åœ¨å°æ ·æœ¬æ•°æ®ä¸‹çš„è¿‡æ‹Ÿåˆï¼Œä»è€Œæ‹¥æœ‰æ›´å¥½çš„é²æ£’æ€§ã€‚


### Answer Engineeringï¼ˆç­”æ¡ˆå·¥ç¨‹ï¼‰
 
åœ¨ç»™å®šä¸€ä¸ªä»»åŠ¡æˆ–è€…Promptï¼Œå¦‚ä½•å¯¹ label ç©ºé—´ å’Œ answer ç©ºé—´è¿›è¡Œæ˜ å°„ï¼Ÿ
- ![](https://pic2.zhimg.com/80/v2-3df9747e3a96385804fce424e5c7b619_1440w.webp)
 
åœ¨ä¸Šå›¾ï¼Œlabel ç©ºé—´ Y æ˜¯: Positive, Negative, ç­”æ¡ˆç©ºé—´ Z å¯ä»¥æ˜¯è¡¨ç¤ºpositiveæˆ–è€…negative çš„è¯ï¼Œä¾‹å¦‚: Interesting/Fantastic/Happy/Boring/1-Star/Badï¼Œå…·ä½“çš„ç­”æ¡ˆç©ºé—´ Z çš„é€‰æ‹©èŒƒå›´å¯ä»¥ç”±æˆ‘ä»¬æŒ‡å®šã€‚å¯ä»¥æŒ‡å®šä¸€ä¸ª y å¯¹åº”1-Nä¸ªå­—ç¬¦/è¯ã€‚
- ![](https://pic4.zhimg.com/80/v2-acf52745920aae345e6c5ac9ee20e92b_1440w.webp)
 
å…·ä½“çš„ç­”æ¡ˆç©ºé—´çš„é€‰æ‹©å¯ä»¥æœ‰ä»¥ä¸‹ä¸‰ä¸ªåˆ†ç±»æ ‡æ³¨ï¼š
1.  æ ¹æ®**å½¢çŠ¶**
  *   1.1 Token ç±»å‹
  *   1.2 Span ç±»å‹
  *   1.3 Sentence ç±»å‹
1.  æ˜¯å¦**æœ‰ç•Œ**
  *   2.1 æœ‰ç•Œ
  *   2.2 æ— ç•Œ
1.  æ˜¯å¦**äººå·¥é€‰æ‹©**
  *   3.1 äººå·¥é€‰æ‹©
  *   3.2 è‡ªåŠ¨æœç´ 
  *   3.2.1 ç¦»æ•£ç©ºé—´
  *   3.2.2 è¿ç»­ç©ºé—´

å…·ä½“çš„æ€ç»´å¯¼å›¾å¦‚ä¸‹ï¼š
- ![](https://pic4.zhimg.com/80/v2-9e58677317b0576537d58fa669c6fc5b_1440w.webp)
 
### Pre-trained Model Choiceï¼ˆé¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ï¼‰

åœ¨å®šä¹‰å®Œæ¨¡ç‰ˆä»¥åŠç­”æ¡ˆç©ºé—´åï¼Œé€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹ prompt è¿›è¡Œé¢„æµ‹ï¼Œå¦‚ä½•é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éœ€è¦äººå·¥ç»éªŒåˆ¤åˆ«çš„ã€‚

å…·ä½“çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥åˆ†ä¸ºå¦‚ä¸‹5ç±»ï¼Œå…·ä½“å‚è€ƒï¼š[Huggingface Summary of the models](https://huggingface.co/docs/transformers/model_summary)
*   autoregressive-models: `è‡ªå›å½’æ¨¡å‹`ï¼Œä¸»è¦ä»£è¡¨æœ‰ `GPT`ï¼Œä¸»è¦ç”¨äºç”Ÿæˆä»»åŠ¡
*   autoencoding-models: `è‡ªç¼–ç æ¨¡å‹`ï¼Œä¸»è¦ä»£è¡¨æœ‰ `BERT`ï¼Œä¸»è¦ç”¨äºNLUä»»åŠ¡
*   seq-to-seq-modelsï¼š`åºåˆ—åˆ°åºåˆ—ä»»åŠ¡`ï¼ŒåŒ…å«äº†an encoder å’Œ a decoderï¼Œä¸»è¦ä»£è¡¨æœ‰ `BART`ï¼Œä¸»è¦ç”¨äºåŸºäºæ¡ä»¶çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘ï¼Œsummaryç­‰
*   multimodal-modelsï¼š`å¤šæ¨¡æ€æ¨¡å‹`
*   retrieval-based-modelsï¼š`åŸºäºå¬å›çš„æ¨¡å‹`ï¼Œä¸»è¦ç”¨äºå¼€æ”¾åŸŸé—®ç­”

åŸºäºæ­¤ï¼Œä¾‹å¦‚ä¸‹å›¾æƒ³è¦åšsummary ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ›´åˆé€‚çš„ BART æ¨¡å‹ã€‚
- ![](https://pic3.zhimg.com/80/v2-f7cf71aaa125547394fc53faadc0e7e6_1440w.webp)
 
å…¶ä»–åˆ†ç±»æ ‡å‡†ä¹Ÿå¯å‚è€ƒï¼š
- ![](https://pic3.zhimg.com/80/v2-654bc36f0f684b17b97f1966fe5904aa_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-f8e5c095bf30cd97176098cd725974e1_1440w.webp)
 
### Expanding the Paradigmï¼ˆèŒƒå¼æ‹“å±•ï¼‰

å¦‚ä½•å¯¹å·²æœ‰çš„ Prompt è¿›è¡Œä»»åŠ¡å¢å¼ºä»¥åŠæ‹“å±•ï¼Œå…·ä½“å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ¢è®¨ï¼š
*   Prompt Ensembleï¼šPrompt é›†æˆï¼Œé‡‡ç”¨å¤šç§æ–¹å¼è¯¢é—®åŒä¸€ä¸ªé—®é¢˜
  - ![](https://pic2.zhimg.com/80/v2-0143f03d078e4c78231b987dd041752d_1440w.webp)
*   Prompt Augmentationï¼šPrompt å¢å¼ºï¼Œé‡‡ç”¨ç±»ä¼¼çš„ prompt æç¤ºè¿›è¡Œå¢å¼º
  - ![](https://pic1.zhimg.com/80/v2-04b7abb8bac6d642808f95c3eaeef088_1440w.webp)
*   Prompt Compositionï¼šPrompt ç»„åˆï¼Œä¾‹å¦‚å°†ä¸€ä¸ªä»»åŠ¡ï¼Œæ‹†æˆå¤šä¸ªä»»åŠ¡çš„ç»„åˆï¼Œæ¯”å¦‚åˆ¤åˆ«ä¸¤ä¸ªå®ä½“ä¹‹é—´æ˜¯å¦æ˜¯çˆ¶å­å…³ç³»ï¼Œé¦–å…ˆå¯¹äºæ¯ä¸ªå®ä½“ï¼Œå…ˆç”¨Prompt åˆ¤åˆ«æ˜¯äººç‰©ï¼Œå†è¿›è¡Œå®ä½“å…³ç³»çš„é¢„æµ‹ã€‚
  - ![](https://pic3.zhimg.com/80/v2-f3fd7d887bcec01d18e410efe74c36aa_1440w.webp)
*   Prompt Decompositionï¼š  
  - å°†ä¸€ä¸ªprompt æ‹†åˆ†æˆå¤šä¸ªprompt
  - ![](https://pic1.zhimg.com/80/v2-a13127a5cd6927bcbff1f278b6c32c28_1440w.webp)
 
å…·ä½“çš„æ€ç»´å¯¼å›¾å¦‚ä¸‹ï¼š
- ![](https://pic1.zhimg.com/80/v2-62bb4b0d499ab4ba4eb57fc27304ec00_1440w.webp)
 
## Prompt-based Training Strategiesï¼ˆè®­ç»ƒç­–ç•¥é€‰æ‹©ï¼‰

Prompt-based æ¨¡å‹åœ¨è®­ç»ƒä¸­ï¼Œæœ‰å¤šç§è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥é€‰æ‹©å“ªäº›æ¨¡å‹éƒ¨åˆ†è®­ç»ƒï¼Œå“ªäº›ä¸è®­ç»ƒã€‚
 
å¯ä»¥æ ¹æ®è®­ç»ƒæ•°æ®çš„å¤šå°‘åˆ†ä¸ºï¼š
*   `Zero-shot`: å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæ²¡æœ‰ä»»ä½•è®­ç»ƒæ•°æ®
*   `Few-shot`: å¯¹äºä¸‹æ¸¸ä»»åŠ¡åªæœ‰å¾ˆå°‘çš„è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚100æ¡
*   `Full-data`: æœ‰å¾ˆå¤šçš„è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚1ä¸‡å¤šæ¡æ•°æ®
 
ä¹Ÿå¯ä»¥æ ¹æ®ä¸åŒçš„å‚æ•°æ›´æ–°çš„éƒ¨åˆ†ï¼Œå¯¹äºprompt-based çš„æ¨¡å‹ï¼Œä¸»è¦åˆ†ä¸ºä¸¤å¤§å—  
- ä¸€ä¸ªæ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸€ä¸ªæ˜¯ Prompts å‚æ•°ã€‚  

è¿™ä¸¤ä¸ªéƒ¨åˆ†ï¼Œéƒ½å¯ä»¥ç‹¬ç«‹é€‰æ‹©å‚æ•°è®­ç»ƒé€‰æ‹©ã€‚  

å¯¹äº
*   é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥é€‰æ‹©ç²¾è°ƒï¼Œæˆ–è€…ä¸è®­ç»ƒ
*   å¯¹äºpromptsï¼š
*   å¯ä»¥æ˜¯æ²¡æœ‰prompts
*   å›ºå®šçš„ç¦»æ•£å­—ç¬¦ promptsã€‚ï¼ˆæ— å‚æ•°ï¼‰
*   ä½¿ç”¨è®­ç»ƒå¥½çš„ promptså‚æ•°ï¼Œä¸å†è®­ç»ƒã€‚
*   ç»§ç»­è®­ç»ƒ promptså‚æ•°
- ![](https://pic3.zhimg.com/80/v2-edcf6508177774d990a83a9867bc96de_1440w.webp)
 
è¿™äº›è®­ç»ƒç­–ç•¥å‡å¯ä»¥ä¸¤ä¸¤ç»„åˆï¼Œä¸‹é¢ä¸¾ä¾‹è¯´æ˜ï¼š
 
### ç­–ç•¥åˆ†ç±»

*   **Promptless** Fine-tuning
  - å¦‚æœåªæœ‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ²¡æœ‰promptsï¼Œç„¶åfine-tuningï¼Œå³æ˜¯bert çš„å¸¸è§„ä½¿ç”¨ã€‚
  - ![](https://pic3.zhimg.com/80/v2-eb301f6533d25e15a4b980581d4c736e_1440w.webp)
*   **Fixed-Prompt** Tuning  
  - å¦‚æœä½¿ç”¨ç²¾è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹+ç¦»æ•£çš„å›ºå®špromptsï¼Œå°±æ˜¯ BERT + Discrete Prompt for text classification
  - ![](https://pic1.zhimg.com/80/v2-f9deac3450d2f4af12cb96fc42784fc8_1440w.webp)
  - å¦‚æœä½¿ç”¨ç²¾è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹+è¿ç»­è®­ç»ƒå¥½çš„å›ºå®špromptsï¼Œå°±æ˜¯ BERT + Transferred Continuous Prompt for text classification
-  ![](https://pic1.zhimg.com/80/v2-ae562fd4a0b05f4a3d87247f64b68e88_1440w.webp)
*   **Prompt+LM** Fine-tuning
  - å¦‚æœä½¿ç”¨ç²¾è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹+å¯è®­ç»ƒçš„promptsï¼Œå°±æ˜¯ BERT + Continuous Prompt for text classification
  - ![](https://pic3.zhimg.com/80/v2-0e47867fb32772bbe94974f4b2a37ff6_1440w.webp)
*   **Adapter** Tuning
  - å¦‚æœä½¿ç”¨å›ºå®šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ— promptï¼Œåªæ˜¯æ’å…¥task-specificæ¨¡å—åˆ°é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œå°±æ˜¯BERT + Adapter for text classification
  - ![](https://pic2.zhimg.com/80/v2-bf82c965a4d2abf020356ff70401ee85_1440w.webp)
*   **Tuning-free** Prompting
  - å¦‚æœä½¿ç”¨å›ºå®šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œç¦»æ•£å›ºå®šçš„promptï¼Œå°±æ˜¯GPT3 + Discrete Prompts for Machine Translation
  - ![](https://pic4.zhimg.com/80/v2-c72a6a803262e3ed3e4ec442a4382ebf_1440w.webp)
  - å¦‚æœä½¿ç”¨å›ºå®šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œè¿ç»­å›ºå®šçš„promptï¼Œå°±æ˜¯ GPT3 + Continuous Prompts for Machine Translation
  - ![](https://pic1.zhimg.com/80/v2-5bfc03ab8699075d28f0f64ca60142cc_1440w.webp)
*   **Fixed-LM** Prompt Tuning
  - å¦‚æœä½¿ç”¨å›ºå®šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¯è®­ç»ƒçš„promptï¼Œå°±æ˜¯ BART + Continuous Prompts for Machine Translation
  - ![](https://pic2.zhimg.com/80/v2-d19c5a6e03987aea08bfd7896fc24c51_1440w.webp)
 
### ç­–ç•¥é€‰æ‹©
 
å¯¹äºä¸åŒçš„ç­–ç•¥ï¼Œéœ€è¦è¿›è¡Œä¸åŒçš„é€‰æ‹©ï¼Œæˆ‘ä»¬å¾€å¾€éœ€è¦è€ƒè™‘ä»¥ä¸‹ä¸¤ç‚¹ï¼š
*   æˆ‘ä»¬çš„æ•°æ®é‡çº§æ˜¯å¤šå°‘
*   æˆ‘ä»¬çš„æ˜¯å¦æœ‰ä¸ªè¶…å¤§çš„ Left-to-right çš„è¯­è¨€æ¨¡å‹  

é€šå¸¸å¦‚æœåªæœ‰å¾ˆå°‘çš„æ•°æ®çš„æ—¶å€™ï¼Œå¸Œæœ›ä¸è¦å» fine-tune é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨LMçš„è¶…å¼ºèƒ½åŠ›ï¼Œåªæ˜¯å»è°ƒprompt å‚æ•°ã€‚è€Œè®©æ•°æ®é‡è¶³å¤Ÿå¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ç²¾è°ƒè¯­è¨€æ¨¡å‹ã€‚

è€Œåªæœ‰åƒGPT-3 è¿™ç§è¶…å¤§çš„è¯­è¨€æ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æ‰èƒ½ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ä»»ä½•çš„fine-tuning.
- ![](https://pic4.zhimg.com/80/v2-6a96f5cf3c0524bce4f9328746c6bff7_1440w.webp)
 
## Prompt çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ
 
Prompt Learning çš„ä¼˜åŠ¿æœ‰å“ªäº›å‘¢ï¼Ÿä»å››ä¸ªè§’åº¦è¿›è¡Œåˆ†æã€‚
*   Level 1. Prompt Learning è§’åº¦
*   Level 2. Prompt Learning å’Œ Fine-tuning çš„åŒºåˆ«
*   Level 3. ç°ä»£ NLP å†å²
*   Level 4. è¶…è¶ŠNLP
- ![](https://pic2.zhimg.com/80/v2-b9363e52298acdc1997182b72b16bfe9_1440w.webp)
 
### Level 1. Prompt Learning ä½¿å¾—æ‰€æœ‰çš„NLPä»»åŠ¡æˆä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„é—®é¢˜

*   Prompt Learning å¯ä»¥å°†æ‰€æœ‰çš„ä»»åŠ¡å½’ä¸€åŒ–é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä»»åŠ¡
*   é¿å…äº†é¢„è®­ç»ƒå’Œfine-tuning ä¹‹é—´çš„gapï¼Œå‡ ä¹æ‰€æœ‰ NLP ä»»åŠ¡éƒ½å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦è®­ç»ƒæ•°æ®ã€‚
*   åœ¨å°‘æ ·æœ¬çš„æ•°æ®é›†ä¸Šï¼Œèƒ½å–å¾—è¶…è¿‡fine-tuningçš„æ•ˆæœã€‚
*   ä½¿å¾—æ‰€æœ‰çš„ä»»åŠ¡åœ¨æ–¹æ³•ä¸Šå˜å¾—ä¸€è‡´
- ![](https://pic2.zhimg.com/80/v2-e344a7b5f4364cb2ab6aa3e38681ebbd_1440w.webp)
 
### Level 2. Prompt Learning å’Œ Fine-tuning çš„èŒƒå¼åŒºåˆ«
 
*   Fine-tuning æ˜¯ä½¿å¾—é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é€‚é…ä¸‹æ¸¸ä»»åŠ¡
*   Prompting æ˜¯å°†ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œä»»åŠ¡é‡å®šä¹‰ï¼Œä½¿å¾—å…¶åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå³é€‚é…è¯­è¨€æ¨¡å‹
- ![](https://pic3.zhimg.com/80/v2-e89d427849c1270e7caffc4990d5d5e6_1440w.webp)
 
### Level 3. ç°ä»£ NLP ç¬¬å››èŒƒå¼
 
Prompting æ–¹æ³•æ˜¯ç°åœ¨NLPçš„ç¬¬å››èŒƒå¼ã€‚å…¶ä¸­ç°åœ¨NLPçš„å‘å±•å²åŒ…å«
1.  Feature Engineeringï¼šå³ä½¿ç”¨æ–‡æœ¬ç‰¹å¾ï¼Œä¾‹å¦‚è¯æ€§ï¼Œé•¿åº¦ç­‰ï¼Œåœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ï¼ˆæ— é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰
2.  Architecture Engineeringï¼šåœ¨W2VåŸºç¡€ä¸Šï¼Œåˆ©ç”¨æ·±åº¦æ¨¡å‹ï¼ŒåŠ ä¸Šå›ºå®šçš„embeddingã€‚ï¼ˆæœ‰å›ºå®šé¢„è®­ç»ƒembeddingï¼Œä½†ä¸ä¸‹æ¸¸ä»»åŠ¡æ— ç›´æ¥å…³ç³»ï¼‰
3.  Objective Engineeringï¼šåœ¨bert çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨åŠ¨æ€çš„embeddingï¼Œåœ¨åŠ ä¸Šfine-tuningã€‚ï¼ˆæœ‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä½†ä¸ä¸‹æ¸¸ä»»åŠ¡æœ‰gapï¼‰
4.  Prompt Engineeringï¼šç›´æ¥åˆ©ç”¨ä¸è®­ç»ƒè¯­è¨€æ¨¡å‹è¾…ä»¥ç‰¹å®šçš„promptã€‚ï¼ˆæœ‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä½†ä¸ä¸‹æ¸¸ä»»åŠ¡æ— gapï¼‰

åœ¨å››ä¸ªèŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„è·ç¦»ï¼Œå˜å¾—è¶Šæ¥è¶Šè¿‘ï¼Œç›´åˆ°æœ€åPrompt Learningæ˜¯ç›´æ¥å®Œå…¨åˆ©ç”¨LMçš„èƒ½åŠ›ã€‚
- ![](https://pic4.zhimg.com/80/v2-041acfa2e38bb9c616e9ba1835f17b1f_1440w.webp)
 
### Level 4. è¶…è¶ŠNLPçš„è§’åº¦
 
Prompt å¯ä»¥ä½œä¸ºè¿æ¥å¤šæ¨¡æ€çš„ä¸€ä¸ªå¥‘æœºï¼Œä¾‹å¦‚ CLIP æ¨¡å‹ï¼Œè¿æ¥äº†æ–‡æœ¬å’Œå›¾ç‰‡ã€‚ç›¸ä¿¡åœ¨æœªæ¥ï¼Œå¯ä»¥è¿æ¥å£°éŸ³å’Œè§†é¢‘ï¼Œè¿™æ˜¯ä¸€ä¸ªå¹¿å¤§çš„å¾…æ¢ç´¢çš„é¢†åŸŸã€‚
- ![](https://pic3.zhimg.com/80/v2-9fa54eb5b1b4b5ebc88843a6647c6c42_1440w.webp)

## æå‡æ•ˆæœ

æå‡ prompting æ•ˆæœçš„æ–¹æ³•
- prompt ensembling
  - æŠŠå¤šä¸ªprompté€šè¿‡æŸç§åŠ æƒæ–¹æ³•ç»„åˆåˆ°ä¸€èµ·
- prompt augmentation
  - å¯å‘å¼å­¦ä¹ 
- prompt composition
  - å°†å¤åˆçš„promptå¥å­ï¼Œæ‹†è§£æˆå¤šä¸ªå°æ®µpromptï¼Œæœ€åå†ç»„åˆåœ¨ä¸€èµ·è®­ç»ƒ
- prompt decomposition
  - ç”±äºä¸€äº›ä»»åŠ¡çš„maskå·¥ä½œä½¿ç”¨å¥å­æ•°é‡æœ‰é™ï¼ˆæ¯”å¦‚è¯æ€§æ ‡æ³¨ä»»åŠ¡ï¼‰ï¼Œäºæ˜¯å°±åªèƒ½é€šè¿‡decompositionå°†ä¸€ä¸ªå¥å­æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†åï¼Œå†å¯¹æ¯ä¸ªéƒ¨åˆ†åšpromptå•ç‹¬è®­ç»ƒ

## prompt åº”ç”¨

promptçš„åº”ç”¨
- çŸ¥è¯†æ¢ç´¢ï¼ˆäº‹å®æ¢ç´¢å’Œè¯­è¨€å­¦æ¢ç´¢ï¼‰
- åˆ†ç±»ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼‰
- ä¿¡æ¯æå–ï¼ˆå…³ç³»æå–ã€è¯­ä¹‰åˆ†æå’Œå‘½åå®ä½“è¯†åˆ«ï¼‰
- NLP ä¸­çš„æ¨ç†ï¼ˆå¸¸è¯†æ¨ç†å’Œæ•°å­¦æ¨ç†ï¼‰
- é—®ç­”
- æ–‡æœ¬ç”Ÿæˆ
- æ–‡æœ¬ç”Ÿæˆçš„è‡ªåŠ¨è¯„ä¼°
- å¤šæ¨¡æ€å­¦ä¹ 
- å…ƒåº”ç”¨ï¼ˆåŸŸè‡ªé€‚åº”ã€é™¤åå’Œæ•°æ®é›†åˆ›å»ºï¼‰

![img](https://pic3.zhimg.com/80/v2-934ee6d5289bcf81b6a32f3e85c88712_1440w.webp)


## prompt å®è·µ

### æ¸…å OpenPrompt

- ã€2023-2-16ã€‘æ¸…åå¼€æºçš„ [OpenPrompt](https://github.com/thunlp/OpenPrompt)ï¼šAn Open-Source Framework for Prompt-learning. åŒ…å«promptå„ä¸ªç®—æ³•ã€æ¨¡å—çš„ä»£ç åŠç¤ºä¾‹
- ![](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Define a task

```py
from openprompt.data_utils import InputExample
classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive
    "negative",
    "positive"
]
dataset = [ # For simplicity, there's only two examples
    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.
    InputExample(
        guid = 0,
        text_a = "Albert Einstein was one of the greatest intellects of his time.",
    ),
    InputExample(
        guid = 1,
        text_a = "The film was badly made.",
    ),
]
```

Define a Pre-trained Language Models (PLMs) as backbone.

```py
from openprompt.plms import load_plm
plm, tokenizer, model_config, WrapperClass = load_plm("bert", "bert-base-cased")
```

å®šä¹‰æ¨¡æ¿

```py
from openprompt.prompts import ManualTemplate
promptTemplate = ManualTemplate(
    text = '{"placeholder":"text_a"} It was {"mask"}',
    tokenizer = tokenizer,
)
```

Define a Verbalizer

```py
from openprompt.prompts import ManualVerbalizer
promptVerbalizer = ManualVerbalizer(
    classes = classes,
    label_words = {
        "negative": ["bad"],
        "positive": ["good", "wonderful", "great"],
    },
    tokenizer = tokenizer,
)
```

Combine them into a PromptModel

```py
from openprompt import PromptForClassification
promptModel = PromptForClassification(
    template = promptTemplate,
    plm = plm,
    verbalizer = promptVerbalizer,
)
```

Define a DataLoader

```py
from openprompt import PromptDataLoader
data_loader = PromptDataLoader(
    dataset = dataset,
    tokenizer = tokenizer,
    template = promptTemplate,
    tokenizer_wrapper_class=WrapperClass,
)
```

Train and inference

```py
import torch

# making zero-shot inference using pretrained MLM with prompt
promptModel.eval()
with torch.no_grad():
    for batch in data_loader:
        logits = promptModel(batch)
        preds = torch.argmax(logits, dim = -1)
        print(classes[preds])
# predictions would be 1, 0 for classes 'positive', 'negative'
```


ç­‰ç­‰ç»„ä»¶


# æ€ç»´é“¾ï¼ˆCoTï¼šChain-of-thoughtsï¼‰


ã€2023-2-19ã€‘[æ€ç»´é“¾ï¼ˆChain-of-thoughtsï¼‰ä½œä¸ºæç¤º](https://zhuanlan.zhihu.com/p/493533589)
- â€œè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒã€‚â€
- Let's think step by step


## èµ„æ–™

ã€2023-2-27ã€‘[Learn Prompting](https://learnprompting.org/)

- ã€2023-3-6ã€‘CoTä¸€ä½œ Jason Weiçš„ppt [New abilities in big language models](https://docs.google.com/presentation/d/1JyvLrfvLOTfGBWrNl7Gk6Mqn6LIgM2NTeRM2d6oyBow/edit#slide=id.g110339e1e35_0_0)ï¼Œtwo new abilities of scale å¤§æ¨¡å‹çš„ä¸¤é¡¹æ–°å¢èƒ½åŠ›
- â‘  Language models follow instructions. **éµä»æŒ‡ä»¤**
  - Finetuned language models are zero-shot learners (ICLR 2022). {J. Wei, M. Bosma, V. Zhao, K. Guu}, A. Yu, B. Lester, N. Du, A. Dai, & Q. Le. 
- â‘¡ Language models do chain of thought reasoning. **æ€ç»´é“¾**
  - Chain of thought prompting elicits reasoning in large language models 
- Emergence and reasoning in large language models - Jason Wei (Google)ï¼Œ[ppt](https://drive.google.com/file/d/1j_CM1fwl_EKB63VlreNUnrKMQsbZHagg/view), [youtube](https://www.youtube.com/watch?v=0Z1ZwY2K2-M)

**Chain-of-thought** prompting elicits reasoning in large language models (Wei et al., 2022).
- â—‹ Self-consistency improves chain-of-thought reasoning in language models (Wang et al., 2022).
- â—‹ Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022).
- â—‹ Language models are multilingual chain-of-thought reasoners (Shi et al., 2022).
- â—‹ Challenging BIG-Bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022).

### Demoä½“éªŒ

[Dyno](https://trydyno.com/)ï¼šPrompt Engineering IDE
- [trydyno](https://embed.trydyno.com/), promptå·¥å…·åŒ…ï¼Œæ”¯æŒè‡ªå®šä¹‰gpt-3æ¨¡å‹ã€tokené•¿åº¦ã€æ¸©åº¦ã€top pç­‰å‚æ•°è°ƒèŠ‚

<script defer="defer" src="https://embed.trydyno.com/embedder.js"></script>
<link href="https://embed.trydyno.com/embedder.css" rel="stylesheet" />

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="ä½ å¤šå¤§" initial-response="" max-tokens="256" box-rows="3" model-temp="0.7" top-p="1">
    <noscript>Failed to load Dyno Embed: JavaScript must be enabled</noscript>
</div>


## CoT è¯ç”Ÿä¹‹åˆ

2021å¹´ï¼Œ`æç¤ºå­¦ä¹ `ï¼ˆprompt learningï¼‰æµªæ½®å…´èµ·ï¼Œä»¥`ç¦»æ•£å¼æç¤ºå­¦ä¹ `ï¼ˆæç¤ºè¯çš„ç»„åˆï¼‰ä¸º**èµ·ç‚¹**ï¼Œ`è¿ç»­åŒ–æç¤ºå­¦ä¹ `ï¼ˆå†»ç»“å¤§æ¨¡å‹æƒé‡+å¾®è°ƒè¾ƒå°å‚æ•°è¾¾åˆ°ç­‰ä»·æ€§èƒ½ï¼‰ä¸º**å¤å…´**ï¼Œå‡ ä¹æ˜¯åœ¨å¹´æœ«è¾¾åˆ°äº†ç ”ç©¶çš„ä¸€ä¸ªå·…å³°ã€‚

2022å¹´å¼€å§‹ï¼Œé€æ¸æœ‰å¾ˆå¤šäººæ„è¯†åˆ°: `è¿ç»­åŒ–æç¤ºå­¦ä¹ `å…¶ä¸­çš„ä¸€äº›å¥½å¤„ä¼´éšçš„ä¸€äº›**å±€é™æ€§**ï¼Œæ¯”å¦‚**ä¼ªèµ„æºèŠ‚çº¦**ï¼Œ**ä¸ç¨³å®š**ç­‰ç­‰ã€‚å¾ˆå¤šç ”ç©¶è€…æ‹’ç»é™ªç©ï¼Œè™½è®¤åŒæç¤ºå­¦ä¹ å°†ä¼šå¸¦æ¥ä¸‹ä¸€ä»£NLPç•Œçš„é©å‘½ï¼Œä½†æ˜¯è®¤ä¸ºæ‹’ç»åšä»–äººå¤§æ¨¡å‹çš„é™„åº¸ï¼Œå¼€å§‹æ¢ç´¢å¤§æ¨¡å‹çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶ä¸”è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹ï¼›è€Œæ‰‹å¤´æš‚æ—¶æ²¡æœ‰æŒæ¡èµ„æºçš„ç ”ç©¶è€…å¼€å§‹å†æ¬¡å°†ç ”ç©¶é‡å¿ƒä»`è¿ç»­åŒ–å­¦ä¹ `è½¬ç§»åˆ°`ç¦»æ•£å¼æç¤ºå­¦ä¹ `ä¸Šå»ï¼Œå°†ç ”ç©¶èšç„¦äºç‰¹å®šçš„`å¤§æ¨¡å‹GPT3`ä¸Šã€‚

æ­¤æ—¶ï¼Œè·ç¦»175Bçš„GPT3æ¨¡å‹è¢«å‘å¸ƒå’Œä¸Šä¸‹æ–‡å­¦ä¹ è¢«å‘ç°è¿‡å»äº†ä¸åˆ°2å¹´ï¼Œçƒ­åº¦ç»å†äº†é«˜æ½®ä¸ä½è°·ï¼Œç»å†äº†æ·±åº¦å­¦ä¹ æµæ´¾å…³äº`è¿æ¥å­¦æ´¾`å’Œ`ç¬¦å·å­¦æ´¾`çš„è¾©è®ºå’Œæ˜¯å¦å…·æœ‰**æ„è¯†**å’Œ**æ¨ç†èƒ½åŠ›**çš„è®¨è®ºï¼Œä¸€äº›åŸºç¡€çš„ç©æ³•åœ¨è¢«å¼€å‘ä¹‹åå°±è¢«æç½®äº†ä¸€æ®µæ—¶é—´ç›´åˆ°æç¤ºå­¦ä¹ çš„å…´èµ·ã€‚

2022å¹´1æœˆï¼ŒOpenAIé€šè¿‡`å¼ºåŒ–å­¦ä¹ `è°ƒè¯•æ¨¡å‹ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è°ƒè¯•æ›´æ–°äº†ä»–ä»¬çš„æ¨¡å‹åˆ°äº†ç¬¬äºŒä»£ï¼ŒLLMè‚‰çœ¼å¯è§åœ°å˜å¾—æ›´å¥½æç¤ºï¼Œå¾ˆå¤šä»»åŠ¡çš„æ€§èƒ½ä¹Ÿæ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯ä¸€äº›ä¹‹å‰æ²¡æœ‰åŠæ³•å¾ˆå¥½è¿›è¡Œçš„ä»»åŠ¡è¢«æ˜¾è‘—åœ°æé«˜äº†èµ·æ¥ã€‚

æ€ç»´é“¾ç³»åˆ—å·¥ä½œå°±æ˜¯åœ¨è¿™æ ·ä¸€ä¸ªå¤§ç¯å¢ƒä¸‹äº§ç”Ÿçš„ã€‚

## CoT ä»‹ç»

2022å¹´5æœˆï¼Œè°·æ­Œå¹´åº¦å¼€å‘è€…å¤§ä¼šï¼ˆGoogle I/Oï¼‰ä¸Šå¯¹CoTæŠ€æœ¯è¿›è¡Œäº†å®£ä¼ ï¼Œè¿˜æœ‰è°·æ­Œçš„540Bç³»æ•°å¤§æ¨¡å‹PaLMå’ŒPixelç³»åˆ—æ‰‹æœºæ‰‹è¡¨ç­‰ç­‰ã€‚
- ![](https://pic2.zhimg.com/80/v2-50bd83f418275a6db5afeb32c1be86a9_1440w.webp)

æ€ç»´é“¾çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å‘å¤§è¯­è¨€æ¨¡å‹å±•ç¤ºä¸€äº›å°‘é‡çš„ exemplarsï¼Œåœ¨æ ·ä¾‹ä¸­è§£é‡Šæ¨ç†è¿‡ç¨‹ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨å›ç­”æç¤ºæ—¶ä¹Ÿä¼šæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ¨ç†çš„è§£é‡Šå¾€å¾€ä¼šå¼•å¯¼å‡ºæ›´å‡†ç¡®çš„ç»“æœã€‚

[æ€ç»´é“¾æç¤ºè¿‡ç¨‹](https://learnprompting.org/zh-Hans/docs/intermediate/chain_of_thought) Weiç­‰äºº
> - â€œæ€ç»´é“¾ä»…åœ¨ä½¿ç”¨**âˆ¼100B**å‚æ•°çš„æ¨¡å‹æ—¶æ‰ä¼šäº§ç”Ÿæ€§èƒ½æå‡â€ã€‚

è¾ƒå°çš„æ¨¡å‹ç¼–å†™äº†ä¸åˆé€»è¾‘çš„æ€ç»´é“¾ä¼šå¯¼è‡´ç²¾åº¦æ¯”æ ‡å‡†æç¤ºæ›´å·®ã€‚

`æ€ç»´é“¾`(CoT)æç¤ºè¿‡ç¨‹ æ˜¯ä¸€ç§æœ€è¿‘å¼€å‘çš„æç¤ºæ–¹æ³•ï¼Œå®ƒé¼“åŠ±å¤§è¯­è¨€æ¨¡å‹è§£é‡Šå…¶æ¨ç†è¿‡ç¨‹
- ![](https://learnprompting.org/zh-Hans/assets/images/chain_of_thought_example-37c925a2720c9c4bb4c823d237bc72c8.png)
- å¸¸è§„æç¤ºè¿‡ç¨‹ vs æ€ç»´é“¾æç¤ºè¿‡ç¨‹

æ€ç»´é“¾å·²è¢«è¯æ˜å¯¹äºç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ç­‰ä»»åŠ¡çš„ç»“æœæœ‰æ‰€æ”¹è¿›ã€‚
- ç‰¹åˆ«æ˜¯ï¼Œåœ¨GSM8K2åŸºå‡†æµ‹è¯•ä¸Šï¼ŒPaLM 540B3çš„æç¤ºè¾¾åˆ°äº†**57%**çš„è§£å†³ç‡å‡†ç¡®æ€§ã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/prompted_palm-20fba06418ed8569b51f0dd376c03b41.png)

### CoT æå‡º

22å¹´1æœˆ, è°·æ­Œå¤§è„‘ç ”ç©¶å‘˜[Jason Wei](https://twitter.com/_jasonwei)æ”¾åˆ°arxivä¸Šé¢çš„æ–‡ç« ï¼Œæå‡ºäº†`æ€ç»´é“¾`è¿™ä¸ªæ¦‚å¿µã€‚
- è®ºæ–‡ï¼š[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- 2023å¹´2æœˆ15æ—¥ï¼Œä¸€ä½œ[Jason Wei](https://www.jasonwei.net/),[twitter](https://twitter.com/_jasonwei),[github](https://github.com/jasonwei20), [personal page](https://jasonwei20.github.io/personal/), ç¦»å¼€Google Brainï¼ŒåŠ å…¥OpenAI
- 2020å¹´ï¼Œè¾¾ç‰¹èŒ…æ–¯æœ¬ç§‘æ¯•ä¸šçš„ABCï¼ŒI graduated with an AB from Dartmouth College in 2020.

<img src="https://images.squarespace-cdn.com/content/v1/633a6c0dd119b45af0d898c2/1226b6e8-1232-4d1f-aeb2-7c4a61657610/taverna.jpeg?format=500w" width=300 heigth="100%">

> We explore how generating a **chain of thought** (`CoT`) -- a series of intermediate reasoning steps -- significantly improves the ability of **large language models** (LLM) to perform complex reasoning (å¤æ‚æ¨ç†). 
>- In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called **chain of thought prompting**, where a few chain of thought demonstrations are provided as exemplars in prompting. 
>- Experiments on three large language models show that **chain of thought prompting** improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.

`æ€ç»´é“¾`æ˜¯ä¸€ç§`ç¦»æ•£å¼æç¤ºå­¦ä¹ `
- å¤§æ¨¡å‹ä¸‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆå³ä¸è¿›è¡Œè®­ç»ƒï¼Œå°†ä¾‹å­æ·»åŠ åˆ°å½“å‰æ ·æœ¬è¾“å…¥çš„å‰é¢ï¼Œè®©æ¨¡å‹ä¸€æ¬¡è¾“å…¥è¿™äº›æ–‡æœ¬è¿›è¡Œè¾“å‡ºå®Œæˆä»»åŠ¡ï¼‰ï¼Œç›¸æ¯”äºä¹‹å‰ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå³é€šè¿‡ x1,y1,x2,y2,....x_test ä½œä¸ºè¾“å…¥æ¥è®©å¤§æ¨¡å‹è¡¥å…¨è¾“å‡º y_testï¼Œæ€ç»´é“¾å¤šäº†ä¸­é—´çš„ä¸€äº›é—²è¨€ç¢è¯­çµ®çµ®å¨å¨ï¼Œä»¥ä¸‹é¢è¿™å¼ å›¾ä¸ºä¾‹å­
- ![CoT](https://pic3.zhimg.com/80/v2-e1b5adff46170f633e8ed635e7a57646_1440w.webp)

ä¾‹å­é€‰æ‹©è‡ªä¸€ä¸ªæ•°æ®é›†å«`GSM8K`ï¼Œæ¯ä¸€ä¸ªæ ·ä¾‹å¤§æ¦‚å°±æ˜¯ä¸€ä¸ªå°å­¦ä¸€äºŒå¹´çº§çš„çœ‹å‡ å¥è¯ï¼ˆåŸºæœ¬éƒ½æ˜¯ä¸‰å¥ï¼‰å†™ç®—å¼ç„¶åç®—ç­”æ¡ˆçš„éš¾åº¦ï¼Œä½†æ˜¯`GPT-3`é€šè¿‡åˆšåˆšè¯´çš„æœ€ç®€å•çš„æç¤ºæ–¹æ³•æ›¾ç»åªèƒ½åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šåšåˆ°6%å·¦å³çš„å‡†ç¡®åº¦ã€‚
- ç”±æ­¤å¯è§ï¼Œç›´æ¥é¢„æµ‹yä¸å¤ªå¯è¡Œã€‚

æ€ç»´é“¾çš„çµ®çµ®å¨å¨ï¼Œå³ä¸ç›´æ¥é¢„æµ‹yï¼Œè€Œæ˜¯å°†yçš„â€œæ€ç»´è¿‡ç¨‹â€rï¼ˆå­¦æœ¯ä¸Šç»Ÿç§°ä¸ºrelationaleï¼‰ä¹Ÿè¦é¢„æµ‹å‡ºæ¥ã€‚å½“ç„¶æœ€åä¸éœ€è¦è¿™äº›â€œæ€ç»´è¿‡ç¨‹â€ï¼Œè¿™äº›åªæ˜¯ç”¨æ¥**æç¤º**ï¼Œè·å¾—æ›´å¥½çš„ç­”æ¡ˆï¼Œåªé€‰æ‹©æœ€åçš„ç­”æ¡ˆå³å¯ã€‚ä½œè€…å¯¹ä¸åŒçš„æ•°æ®é›†çš„åŸæœ¬ç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ çš„æç¤ºæ ‡æ³¨äº†è¿™äº›æ€ç»´é“¾ç„¶åè·‘äº†å®éªŒï¼Œå‘ç°è¿™ä¹ˆåšèƒ½å¤Ÿ**æ˜¾è‘—**æå‡æ€§èƒ½ï¼ˆå·¦å›¾ï¼‰ï¼Œä¸”è¿™ç§æ€§èƒ½çš„æå‡æ˜¯å…·æœ‰ç±»ä¼¼äº**äº•å–·æ€§è´¨**ï¼ˆå³å›¾ï¼‰çš„ï¼ˆåæ¥ç§°è¿™ç§æ€§è´¨å«`æ¶Œç°æ€§`ï¼‰ã€‚
- ![](https://pic1.zhimg.com/80/v2-a5e20815867c458a74b5e30a13f3b53c_1440w.webp)

## CoT åŸç†


### å¦‚ä½•ç”ŸæˆCoTæç¤ºï¼Ÿ

å¾…å®š

## CoT æ”¹è¿›

- (1) å¤šæ•°æŠ•ç¥¨æ˜¾è‘—æé«˜CoTæ€§èƒ½
  - 2023å¹´ï¼Œ[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)ï¼Œ[Jason Wei](https://www.jasonwei.net/) äºŒä½œ
- (2) æå‡ºäº†ä¸€ç§boostæ–¹æ³•ï¼Œè®©**ä¸­å°æ¨¡å‹**ä¹Ÿå¯ä»¥é€šè¿‡è®­ç»ƒå…·æœ‰æ€ç»´é“¾èƒ½åŠ›
  - STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning
- (3) Zero-Shot æ¨ç†
  - Large Language Models are Zero-Shot Reasoners
  - â€œLet's do it step by stepâ€œ
- (4) å…¶å®ƒæ–‡ç« 
  - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models
  - On the Advance of Making Language Models Better Reasoners
  - Rationale-Augmented Ensembles in Language Models

### å¤šæ•°æŠ•ç¥¨æ˜¾è‘—æé«˜CoTæ€§èƒ½

2022å¹´3æœˆåœ¨arxivä¸Šæ”¾å‡ºæ¥
- è®ºæ–‡ï¼š[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
- æ”¹è¿›æ€ç»´é“¾è§£ç æ•ˆç‡ï¼šæ–°çš„è§£ç ç­–ç•¥ï¼ˆè‡ªä¸€è‡´æ€§ï¼‰æ›¿æ¢åŸç”Ÿçš„è´ªå¿ƒè§£ç 

>**Chain-of-thought prompting** combined with pre-trained large language models has achieved encouraging results on **complex reasoning** tasks. In this paper, we propose a new **decoding strategy**, self-consistency, to replace the **naive greedy decoding** used in chain-of-thought prompting. 
>- It first samples a diverse set of **reasoning paths** instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. 
>- Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.
>- Our extensive empirical evaluation shows that **self-consistency** boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).

æ€ç»´é“¾åˆä»£æ–‡ç« å¾ˆå¿«çš„ä¸€ä¸ªè·Ÿè¿›å·¥ä½œï¼Œæ˜¯æ€ç»´é“¾ç³»åˆ—æ–‡ç« ç‰ˆå›¾çš„é‡è¦ä¸€æ­¥ã€‚
- è¿™ç¯‡æ–‡ç« å‡ ä¹ç”¨çš„å’Œåˆä»£æ€ç»´é“¾æ–‡ç« å®Œå…¨ä¸€æ ·çš„æ•°æ®é›†å’Œè®¾ç½®ï¼Œä¸»è¦æ”¹è¿›æ˜¯ä½¿ç”¨äº†å¯¹ç­”æ¡ˆè¿›è¡Œäº†**å¤šæ•°æŠ•ç¥¨**ï¼ˆmajority voteï¼‰ï¼Œå¹¶ä¸”å‘ç°å…¶å¯ä»¥æ˜¾è‘—åœ°æé«˜æ€ç»´é“¾æ–¹æ³•çš„æ€§èƒ½ã€‚
- å°†greedy searchå˜æˆäº†sample+vote
- ![](https://pic1.zhimg.com/80/v2-82f1234d53f8c0623d783eccdff272e4_1440w.webp)
- ä¸€å¼€å§‹ä¸å¦‚cotï¼ˆå› ä¸ºtemperatureï¼‰ï¼Œåé¢å¤§å¹…åº¦è¶…è¿‡
- ![](https://pic3.zhimg.com/80/v2-bc59df588bc49b11c82a46dab77379de_1440w.webp)
- å°†`è´ªå©ªæœç´¢`ï¼ˆgreedy searchï¼‰ï¼Œå³å°†GPTæ¨¡å‹çš„ temperature ä»0è®¾ç½®ä¸ºæŸä¸ªæ•°å€¼ï¼Œæ¯”å¦‚è¯´0.4ï¼Œç„¶åsampleå¤šä¸ªæŒ‰ç…§yè¿›è¡ŒæŠ•ç¥¨ï¼Œä¼šæ˜¾è‘—åœ°æå‡æ€§èƒ½

### è‡ªæ´½æ€§

`è‡ªæ´½æ€§`ï¼ˆSelf-consistencyï¼‰æ˜¯å¯¹ CoT çš„è¡¥å……ï¼Œå®ƒç”Ÿæˆ**å¤šä¸ª**æ€è·¯é“¾ï¼Œç„¶åå–**å¤šæ•°ç­”æ¡ˆ**ä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚

åœ¨ä¸‹é¢çš„å›¾ä¸­ï¼Œå·¦ä¾§çš„æç¤ºæ˜¯ä½¿ç”¨å°‘æ ·æœ¬æ€è·¯é“¾èŒƒä¾‹ç¼–å†™çš„ã€‚ä½¿ç”¨è¿™ä¸ªæç¤ºï¼Œç‹¬ç«‹ç”Ÿæˆå¤šä¸ªæ€è·¯é“¾ï¼Œä»æ¯ä¸ªæ€è·¯é“¾ä¸­æå–ç­”æ¡ˆï¼Œé€šè¿‡â€œè¾¹ç¼˜åŒ–æ¨ç†è·¯å¾„â€æ¥è®¡ç®—æœ€ç»ˆç­”æ¡ˆã€‚å®é™…ä¸Šï¼Œè¿™æ„å‘³ç€å–å¤šæ•°ç­”æ¡ˆã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/self_consistency-3db2492237f38cf1567b25e0c902e3f5.png)

ç»“è®º
- è‡ªæ´½æ€§å¯ä»¥æé«˜ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡çš„ç»“æœã€‚
- å³ä½¿æ™®é€šçš„æ€è·¯é“¾æç¤ºè¢«å‘ç°æ— æ•ˆ2ï¼Œè‡ªæ´½æ€§ä»ç„¶èƒ½å¤Ÿæ”¹å–„ç»“æœã€‚

### çŸ¥è¯†ç”Ÿæˆ

ç”Ÿæˆçš„çŸ¥è¯†æ–¹æ³•ï¼ˆGenerated Knowledge Approachï¼‰è¦æ±‚ LLM åœ¨ç”Ÿæˆå“åº”ä¹‹å‰ç”Ÿæˆä¸é—®é¢˜ç›¸å…³çš„å¯èƒ½æœ‰ç”¨çš„ä¿¡æ¯ã€‚

è¯¥æ–¹æ³•ç”±ä¸¤ä¸ªä¸­é—´æ­¥éª¤ç»„æˆï¼šçŸ¥è¯†ç”Ÿæˆ å’Œ çŸ¥è¯†é›†æˆ
- çŸ¥è¯†ç”Ÿæˆ: è¦æ±‚ LLM ç”Ÿæˆæœ‰å…³é—®é¢˜çš„ä¸€ç»„äº‹å®ã€‚å¤§è¯­è¨€æ¨¡å‹å°†ä»¥ few-shot æ–¹å¼è¿›è¡Œæç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ä½¿ç”¨ç›¸åŒæç¤ºç”Ÿæˆ M ä¸ªä¸åŒçš„å®Œæˆã€‚
  - ![](https://learnprompting.org/zh-Hans/assets/images/gen_k_p1-13cb7c5d4bcd11edceda40a150b4f2cd.png)
- çŸ¥è¯†é›†æˆ:

é—®é¢˜: â€œå¤§å¤šæ•°è¢‹é¼ æœ‰ \<mask\> è‚¢ä½“â€ã€‚å‡è®¾åœ¨çŸ¥è¯†ç”Ÿæˆæ­¥éª¤ä¸­ï¼Œç”Ÿæˆäº† 2 ä¸ªçŸ¥è¯†ï¼ˆM=2ï¼‰ï¼š
>- çŸ¥è¯†1ï¼šâ€œè¢‹é¼ æ˜¯ç”Ÿæ´»åœ¨æ¾³å¤§åˆ©äºšçš„æœ‰è¢‹åŠ¨ç‰©ã€‚â€
>- çŸ¥è¯†2ï¼šâ€œè¢‹é¼ æ˜¯æœ‰ 5 æ¡è‚¢ä½“çš„æœ‰è¢‹åŠ¨ç‰©ã€‚â€

å°†æ¯ä¸ªçŸ¥è¯†ä¸é—®é¢˜è¿æ¥èµ·æ¥ï¼Œç”ŸæˆçŸ¥è¯†å¢å¼ºçš„é—®é¢˜ï¼š
>- çŸ¥è¯†å¢å¼ºé—®é¢˜1ï¼šâ€œå¤§å¤šæ•°è¢‹é¼ æœ‰ \<mask\> è‚¢ä½“ã€‚è¢‹é¼ æ˜¯ç”Ÿæ´»åœ¨æ¾³å¤§åˆ©äºšçš„æœ‰è¢‹åŠ¨ç‰©ã€‚â€
>- çŸ¥è¯†å¢å¼ºé—®é¢˜2ï¼šâ€œå¤§å¤šæ•°è¢‹é¼ æœ‰ \<mask\> è‚¢ä½“ã€‚è¢‹é¼ æ˜¯æœ‰ 5 æ¡è‚¢ä½“çš„æœ‰è¢‹åŠ¨ç‰©ã€‚â€

ç„¶åï¼Œç”¨è¿™äº›çŸ¥è¯†å¢å¼ºçš„é—®é¢˜æç¤º LLMï¼Œå¹¶è·å¾—æœ€ç»ˆç­”æ¡ˆçš„ææ¡ˆï¼š
>- ç­”æ¡ˆ1ï¼šâ€œ4â€
>- ç­”æ¡ˆ2ï¼šâ€œ5â€

é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç­”æ¡ˆä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚æœ€é«˜æ¦‚ç‡å¯èƒ½æ˜¯ç­”æ¡ˆä»¤ç‰Œçš„ softmax æ¦‚ç‡ï¼Œæˆ–ç­”æ¡ˆä»¤ç‰Œçš„å¯¹æ•°æ¦‚ç‡ã€‚

### æœ€å°‘åˆ°æœ€å¤šæç¤ºè¿‡ç¨‹ Least to Most prompting

`æœ€å°‘åˆ°æœ€å¤šæç¤ºè¿‡ç¨‹` (Least to Most prompting, LtM) å°† `æ€ç»´é“¾æç¤ºè¿‡ç¨‹` (CoT prompting) è¿›ä¸€æ­¥å‘å±•
- é¦–å…ˆå°†é—®é¢˜åˆ†è§£ä¸º**å­é—®é¢˜**ï¼Œç„¶åé€ä¸ªè§£å†³ã€‚

å—é’ˆå¯¹å„¿ç«¥çš„ç°å®æ•™è‚²ç­–ç•¥çš„å¯å‘, å‘å±•å‡ºçš„ä¸€ç§æŠ€æœ¯ã€‚

ä¸`æ€ç»´é“¾æç¤ºè¿‡ç¨‹`ç±»ä¼¼ï¼Œéœ€è¦è§£å†³çš„é—®é¢˜è¢«åˆ†è§£æˆä¸€ç»„å»ºç«‹åœ¨å½¼æ­¤ä¹‹ä¸Šçš„å­é—®é¢˜ã€‚
- åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œè¿™äº›å­é—®é¢˜è¢«é€ä¸ªè§£å†³ã€‚
- ä¸æ€ç»´é“¾ä¸åŒçš„æ˜¯ï¼Œå…ˆå‰å­é—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¢«è¾“å…¥åˆ°æç¤ºä¸­ï¼Œä»¥å°è¯•è§£å†³ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚
- ![LtM](https://learnprompting.org/zh-Hans/assets/images/least_to_most_formal-23db97bcd5ecbabe7d5db9b0d0645741.png)

[ç»“è®º](https://learnprompting.org/zh-Hans/docs/intermediate/least_to_most): LtM å¸¦æ¥äº†å¤šé¡¹æå‡ï¼š
- ç›¸å¯¹äºæ€ç»´é“¾æé«˜äº†å‡†ç¡®æ€§
- åœ¨éš¾åº¦é«˜äºæç¤ºçš„é—®é¢˜ä¸Šæå‡äº†æ³›åŒ–èƒ½åŠ›
- åœ¨ç»„åˆæ³›åŒ–æ–¹é¢çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨SCANåŸºå‡†æµ‹è¯•3ä¸­
- ä½¿ç”¨ text-davinci-002ï¼ˆè®ºæ–‡æ¨¡å‹ï¼‰çš„æ ‡å‡†æç¤ºè§£å†³äº† 6% çš„ SCAN é—®é¢˜ï¼Œè€Œ LtM æç¤ºåˆ™å–å¾—äº†æƒŠäººçš„ 76% çš„æˆåŠŸç‡ã€‚åœ¨ code-davinci-002 ä¸­ï¼Œç»“æœæ›´ä¸ºæ˜¾è‘—ï¼ŒLtM è¾¾åˆ°äº† 99.7% çš„æˆåŠŸç‡ã€‚

### Zero-Shot æ€ç»´é“¾

`é›¶æ ·æœ¬æ€ç»´é“¾`ï¼ˆZero Shot Chain of Thoughtï¼ŒZero-shot-CoTï¼‰æç¤ºè¿‡ç¨‹ æ˜¯å¯¹ CoT prompting çš„åç»­ç ”ç©¶ï¼Œå®ƒå¼•å…¥ä¸€ç§éå¸¸ç®€å•çš„é›¶æ ·æœ¬æç¤ºã€‚
- åœ¨é—®é¢˜ç»“å°¾é™„åŠ â€œ<font style='color:blue'>è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒ</span>ã€‚â€ è¿™å‡ ä¸ªè¯ï¼Œå¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªå›ç­”é—®é¢˜çš„æ€ç»´é“¾ã€‚
- ä»è¿™ä¸ªæ€ç»´é“¾ä¸­ï¼Œä»–ä»¬èƒ½å¤Ÿæå–æ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/zero_shot-1af9e1cb88412f9fdefa3b07b67c4193.png)

ä»æŠ€æœ¯ä¸Šè®²ï¼Œå®Œæ•´çš„é›¶æ ·æœ¬æ€ç»´é“¾è¿‡ç¨‹æ¶‰åŠä¸¤ä¸ªå•ç‹¬çš„æç¤º/è¡¥å…¨ç»“æœã€‚ä¸‹å›¾ä¸­ï¼Œå·¦ä¾§é¡¶éƒ¨æ°”æ³¡ç”Ÿæˆä¸€ä¸ªæ€ç»´é“¾ï¼Œè€Œå³ä¾§é¡¶éƒ¨æ°”æ³¡æ¥æ”¶æ¥è‡ªç¬¬ä¸€ä¸ªæç¤ºï¼ˆåŒ…æ‹¬ç¬¬ä¸€ä¸ªæç¤ºæœ¬èº«ï¼‰çš„è¾“å‡ºï¼Œå¹¶ä»æ€ç»´é“¾ä¸­æå–ç­”æ¡ˆã€‚è¿™ä¸ªç¬¬äºŒä¸ªæç¤ºæ˜¯ä¸€ä¸ª è‡ªæˆ‘å¢å¼º çš„æç¤ºã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/zero_shot_example-89065990663d4ef044011844ff77f9af.png)

ç»“è®º
- é›¶æ ·æœ¬æ€ç»´é“¾ä¹Ÿæœ‰æ•ˆåœ°æ”¹å–„äº†ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡çš„ç»“æœã€‚ç„¶è€Œï¼Œé€šå¸¸ä¸å¦‚æ€ç»´é“¾æç¤ºè¿‡ç¨‹æœ‰æ•ˆã€‚åœ¨è·å–æ€ç»´é“¾æç¤ºçš„å°‘é‡ç¤ºä¾‹æœ‰å›°éš¾çš„æ—¶å€™ï¼Œé›¶æ ·æœ¬æ€ç»´é“¾å¯ä»¥æ´¾ä¸Šç”¨åœºã€‚

CoT ä½¿ç”¨ few shot è®© llm è¯´å‡ºæ¥ CoT ï¼Œç„¶åå¾—åˆ°ç­”æ¡ˆï¼Œåˆ†æˆä¸¤éƒ¨åˆ†ï¼š
- é¦–å…ˆ, åŠ ä¸€ä¸ªâ€œ<font style='color:blue'>Lets think step by step</span>â€ï¼Œå¾—åˆ°ä¸€ä¸ªCoTçš„ murmur
- ç„¶å, åé¢æ¥ä¸Šä¸€å¥â€œso the anwser isâ€ï¼Œç„¶åå¾—åˆ°ç­”æ¡ˆã€‚

æ€§èƒ½è¿œè¶…åŸæ¥çš„zero shoté€¼è¿‘few shotå’ŒCoTçš„few shotæ€§èƒ½ã€‚

## CoT åº”ç”¨

åº”ç”¨
- [å¤šé¡¹é€‰æ‹©é¢˜](https://learnprompting.org/zh-Hans/docs/applied_prompting/mc_tutorial)
- [è§£ç­”è®¨è®ºæ€§é—®é¢˜](https://learnprompting.org/zh-Hans/docs/applied_prompting/short_response): é€šè¿‡æ­£ç¡®çš„æç¤ºï¼ŒGPT-3éå¸¸æ“…é•¿å†™ä½œçŸ­æ ¼å¼å›ç­”ã€‚
- [GPT-3æ„å»ºChatGPT](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatgpt)

### å¤šé¡¹é€‰æ‹©

[å¤šé¡¹é€‰æ‹©é¢˜](https://learnprompting.org/zh-Hans/docs/applied_prompting/mc_tutorial)
- LSATï¼ˆLaw School Admission Testï¼‰æ˜¯ç¾å›½æ³•å­¦é™¢ç”¨äºè¯„ä¼°æ½œåœ¨å­¦ç”Ÿçš„æ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ†ææ¨ç†èƒ½åŠ›çš„æ ‡å‡†åŒ–è€ƒè¯•
- å¼•å…¥ CoT æŠ€æœ¯ï¼Œå¯ä»¥è®© GPT-3 å®Œæˆ LSAT çš„å¤šé¡¹é€‰æ‹©é¢˜
- GPTä¸è®¡ç®—å™¨ç­‰å¤–éƒ¨å·¥å…· [MRKL](https://learnprompting.org/docs/advanced_applications/mrkl)
- [èŠå¤©æœºå™¨äºº+çŸ¥è¯†åº“](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatbot_from_kb)

### è®¨è®ºæ€§é—®é¢˜

[è§£ç­”è®¨è®ºæ€§é—®é¢˜](https://learnprompting.org/zh-Hans/docs/applied_prompting/short_response): é€šè¿‡æ­£ç¡®çš„æç¤ºï¼ŒGPT-3éå¸¸æ“…é•¿å†™ä½œçŸ­æ ¼å¼å›ç­”ã€‚
- è®¨è®ºæ€§é—®é¢˜çš„å›ç­”é€šå¸¸çº¦ä¸º100åˆ°700å­—ã€‚æ›´é•¿çš„å†…å®¹å¯èƒ½ä¼šæœ‰äº›æ£˜æ‰‹ï¼Œå› ä¸ºè¯­è¨€æ¨¡å‹çš„è®°å¿†æœ‰é™ï¼Œå¹¶ä¸”éš¾ä»¥ç†è§£ä»–ä»¬æ‰€å†™çš„å†…å®¹å…¨å±€ã€‚
- ä¸€ä¸ªå¥½çš„æç¤ºåº”è¯¥ç»™å‡º**å…·ä½“æ ¼å¼**å’Œ**å†…å®¹æŒ‡ä»¤**ã€‚
- è®¸å¤šè®¨è®ºæ€§é—®é¢˜å¹¶ä¸é€‚åˆæç¤º
  - Bad: å†…æˆ˜æ˜¯ä¸€åœºå…³äºæ‰©å¼ çš„å†²çªå—ï¼ŸåŒæ„è¿˜æ˜¯ä¸åŒæ„ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
  - Good(æ˜ç¡®æŒ‡ä»¤): è§£é‡Šå†…æˆ˜çš„åŸå› ä»¥åŠæ‰©å¼ æ˜¯å¦åœ¨å†²çªä¸­èµ·äº†ä½œç”¨ã€‚é™„ä¸Šæ”¯æŒæ‚¨è®ºç‚¹çš„è¯æ®
  - Better(å…·ä½“æ ¼å¼): å†™ä¸€ç¯‡é«˜åº¦è¯¦ç»†çš„è®¨è®ºå›ç­”ï¼ŒæŒ‰ç…§è®ºæ–‡ç»“æ„å›ç­”ä»¥ä¸‹æç¤º: è§£é‡Šå†…æˆ˜çš„åŸå› ä»¥åŠæ‰©å¼ æ˜¯å¦åœ¨å†²çªä¸­èµ·äº†ä½œç”¨ã€‚é™„ä¸Šæ”¯æŒæ‚¨è®ºç‚¹çš„è¯æ®ã€‚

### GPT-3æ„å»ºChatGPT

- [GPT-3æ„å»ºChatGPT](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatgpt)
  - ä¸GPT-3.5ç³»åˆ—æ¨¡å‹ä¸€æ ·ï¼Œ ChatGPT æ˜¯ä½¿ç”¨RLHFè®­ç»ƒï¼Œä½†å¤§éƒ¨åˆ†æ•ˆæœæ¥è‡ªäºä½¿ç”¨äº†**å¥½çš„æç¤º**ã€‚
  - ![](https://learnprompting.org/zh-Hans/assets/images/chatgpt_ui_diagram-87b55966a74fe72526d9e2c4b86c6650.png)
  - å¦‚ä½•è®°å¿†ï¼Ÿå°†ä¸Šæ–‡é™„åŠ åˆ°ä¸‹ä¸€ä¸ªæç¤ºä¸­ï¼ŒåŒ…æ‹¬ChatGPTåœ¨å†…GPT-3æ¨¡å‹ï¼Œç»„åˆæç¤ºå’Œç”Ÿæˆå“åº”çš„æ ‡è®°é™åˆ¶ä¸º4097ä¸ªï¼ˆçº¦3000ä¸ªå•è¯ï¼‰ã€‚

### èŠå¤©æœºå™¨äºº+çŸ¥è¯†åº“

[èŠå¤©æœºå™¨äºº+çŸ¥è¯†åº“](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatbot_from_kb)
- ä¼ ç»ŸèŠå¤©æœºå™¨äººåŸºäºæ„å›¾è¯†åˆ«ï¼Œä»¥ **< ä¸€ç»„é—®é¢˜, å›å¤ >** å½¢å¼å­˜åœ¨ï¼Œ é—®é¢˜ï¼šåŸŸå¤–æ„å›¾æ— æ³•å›ç­”
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_intents-26defe08be3840898a001f744580bebd.png)
- çŸ¥è¯†åº“ Knowledge Base æ˜¯å­˜å‚¨ä¸ºç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®çš„ä¿¡æ¯ï¼Œå¯ç”¨äºåˆ†ææˆ–æ¨æ–­ã€‚
- æ¯ä¸ªæ„å›¾ä¸æ–‡æ¡£ç›¸å…³è”ï¼Œè€Œä¸æ˜¯ä¸€ç»„é—®é¢˜å’Œç‰¹å®šç­”æ¡ˆ, è¿™æ ·æ„å›¾å¯ä»¥æ›´åŠ å¹¿æ³›
- å½“ç”¨æˆ·è¯¢é—®æœ‰å…³ç™»å½•çš„é—®é¢˜æ—¶ï¼Œå°†â€œç™»å½•é—®é¢˜â€**æ–‡æ¡£**ä¼ é€’ç»™ GPT-3 ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆç‰¹å®šçš„å“åº”ã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_gpt3-b1b71b5a76270f0ce2e14716f7546d33.png)
- è¿™ç§æ–¹æ³•å‡å°‘äº†éœ€è¦å¤„ç†çš„æ„å›¾æ•°é‡ï¼Œå¹¶å…è®¸æ›´å¥½åœ°é€‚åº”æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œå¦‚æœä¸æ„å›¾å…³è”çš„æ–‡æ¡£æè¿°äº†ä¸åŒçš„æµç¨‹ï¼ˆä¾‹å¦‚â€œåœ¨ç½‘ç«™ä¸Šç™»å½•â€çš„æµç¨‹å’Œâ€œåœ¨ç§»åŠ¨åº”ç”¨ç¨‹åºä¸Šç™»å½•â€çš„æµç¨‹ï¼‰ï¼ŒGPT-3å¯ä»¥åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰è‡ªåŠ¨è¯¢é—®ç”¨æˆ·ä»¥è·å¾—æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- GPT-3è¿™æ ·çš„LLMæ¨¡å‹, æœ€å¤§æç¤ºçš„é•¿åº¦çº¦ä¸º**4kä»¤ç‰Œ**ï¼ˆtext-davinci-003æ¨¡å‹ï¼‰ï¼Œä¸è¶³ä»¥å°†æ•´ä¸ªçŸ¥è¯†åº“é¦ˆå…¥å•ä¸ªæç¤ºä¸­ã€‚ LLMç”±äºè®¡ç®—åŸå› å…·æœ‰æœ€å¤§æç¤ºçš„é™åˆ¶ï¼Œå› ä¸ºç”Ÿæˆæ–‡æœ¬æ¶‰åŠå¤šä¸ªè®¡ç®—ï¼Œéšç€æç¤ºå¤§å°çš„å¢åŠ ï¼Œè®¡ç®—é‡ä¹Ÿä¼šè¿…é€Ÿå¢åŠ ã€‚æœªæ¥çš„LLMå¯èƒ½ä¸ä¼šæœ‰è¿™ç§é™åˆ¶ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚
- ç”¨GPT-3æ„å»ºä¸€ä¸ªèŠå¤©æœºå™¨äºº, ä¸¤ä¸ªæ­¥éª¤
  - ä¸ºç”¨æˆ·é—®é¢˜é€‰æ‹©é€‚å½“æ„å›¾ï¼Œå³ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢æ­£ç¡®çš„æ–‡æ¡£ã€‚--- è¯­ä¹‰æœç´¢ semantic search, å¦‚ sentence-transformers åº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚åˆ†æ•°æœ€é«˜çš„æ–‡æ¡£å°†ç”¨äºç”ŸæˆèŠå¤©æœºå™¨äººç­”æ¡ˆã€‚
  - æœ‰äº†æ­£ç¡®çš„æ–‡æ¡£ï¼Œå°±å¯ä»¥ç”¨ GPT-3 ç”Ÿæˆé€‚å½“ç­”æ¡ˆï¼›--- ç”¨temperatureä¸º0.7çš„text-davinci-003æ¨¡å‹ã€‚
- æ³¨æ„
  - åªè¦æœ‰æ­£ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒGPT-3å°±å¯ä»¥è¿›è¡Œæ¶ˆæ­§ä¹‰ã€‚
  - å½“ç”¨æˆ·é—®é¢˜å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆæ—¶ï¼ŒGPT-3å¾ˆå°‘ç”Ÿæˆè™šå‡ä¿¡æ¯ã€‚ç”±äºç”¨æˆ·é—®é¢˜é€šå¸¸æ˜¯çŸ­å°æ¨¡ç³Šçš„æ–‡æœ¬ï¼Œä¸èƒ½æ€»æ˜¯ä¾èµ–è¯­ä¹‰æœç´¢æ­¥éª¤æ¥æ£€ç´¢æ­£ç¡®çš„æ–‡æ¡£ï¼Œå› æ­¤å®¹æ˜“å—åˆ°**è™šå‡ä¿¡æ¯ç”Ÿæˆ**çš„å½±å“ã€‚
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_gpt3_organized-7a4e195a1ae2dfc507fb58a6a987f2b6.png)

æç¤ºåˆ¶ä½œ
- è§’è‰²æç¤º: ä¸€ç§å¯å‘å¼æŠ€æœ¯ï¼Œä¸ºAIåˆ†é…ç‰¹å®šçš„è§’è‰²ã€‚
- ç›¸å…³çš„çŸ¥è¯†åº“ä¿¡æ¯, å³åœ¨è¯­ä¹‰æœç´¢æ­¥éª¤ä¸­æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚
- ç”¨æˆ·å’ŒèŠå¤©æœºå™¨äººä¹‹é—´æœ€åä¸€æ¬¡äº¤æ¢çš„æ¶ˆæ¯. è¿™å¯¹äºç”¨æˆ·å‘é€çš„æœªæŒ‡å®šæ•´ä¸ªä¸Šä¸‹æ–‡çš„æ¶ˆæ¯éå¸¸æœ‰ç”¨ã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„ä¾‹å­ä¸­çœ‹åˆ°å®ƒã€‚è¯·æŸ¥çœ‹æ­¤ç¤ºä¾‹ äº†è§£å¦‚ä½•ä½¿ç”¨GPT-3ç®¡ç†å¯¹è¯ã€‚
- æœ€å, ç”¨æˆ·çš„é—®é¢˜.
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_prompt-8f462a976480059dc1dc16128205efb5.png)


ChatGPT çš„ Prompt æ„é€ 
- `system`: ç³»ç»Ÿè®¾ç½®ï¼Œé»˜è®¤æ˜¯é«˜çº§èŠå¤©æœºå™¨äºº
- `conversation`: ä¼šè¯å†å²
- `user`: ç”¨æˆ·è¾“å…¥

```py
chatbot_prompt = """
    ä½œä¸ºä¸€ä¸ªé«˜çº§èŠå¤©æœºå™¨äººï¼Œä½ çš„ä¸»è¦ç›®æ ‡æ˜¯å°½å¯èƒ½åœ°ååŠ©ç”¨æˆ·ã€‚è¿™å¯èƒ½æ¶‰åŠå›ç­”é—®é¢˜ã€æä¾›æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæˆ–æ ¹æ®ç”¨æˆ·è¾“å…¥å®Œæˆä»»åŠ¡ã€‚ä¸ºäº†æœ‰æ•ˆåœ°ååŠ©ç”¨æˆ·ï¼Œé‡è¦çš„æ˜¯åœ¨ä½ çš„å›ç­”ä¸­è¯¦ç»†å’Œå…¨é¢ã€‚ä½¿ç”¨ä¾‹å­å’Œè¯æ®æ”¯æŒä½ çš„è§‚ç‚¹ï¼Œå¹¶ä¸ºä½ çš„å»ºè®®æˆ–è§£å†³æ–¹æ¡ˆæä¾›ç†ç”±ã€‚

    <conversation history>

    User: <user input>
    Chatbot:"""
```

# ICL

ICLï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰

ã€2023-2-20ã€‘
- æ–¯å¦ç¦åŸæ–‡ï¼š[Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious](https://ai.stanford.edu/blog/in-context-learning/)
- [ä¸€æ–‡ç†è§£â€œä¸Šä¸‹æ–‡å­¦ä¹ â€----å¤§è¯­è¨€æ¨¡å‹çªç°èƒ½åŠ›](https://zhuanlan.zhihu.com/p/606788655)

GPT-3 æ¨¡å‹å±•ç°äº†ä¸€äº›å¤§æ¨¡å‹æ‰å…·å¤‡çš„**çªç°èƒ½åŠ›**ï¼ˆæ¨¡å‹è§„æ¨¡å¿…é¡»å¾—å¢å¤§åˆ°ä¸€å®šç¨‹åº¦æ‰ä¼šæ˜¾ç°çš„èƒ½åŠ›ï¼Œæ¯”å¦‚è‡³å°‘ç™¾äº¿çº§ï¼‰ï¼Œå…¶ä¸­ä¸€é¡¹èƒ½åŠ›å°±æ˜¯`ä¸Šä¸‹æ–‡å­¦ä¹ `ï¼ˆIn-Context Learningï¼‰ï¼ˆæˆ–ï¼š`æƒ…å¢ƒå­¦ä¹ `ï¼‰
- `çªç°èƒ½åŠ›`ï¼šé¢„è®­ç»ƒå¥½çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œè¿ç§»åˆ°æ–°ä»»åŠ¡ä¸Šçš„æ—¶å€™ï¼Œåªéœ€è¦ç»™æ¨¡å‹è¾“å…¥å‡ ä¸ªç¤ºä¾‹ï¼ˆç¤ºä¾‹è¾“å…¥å’Œç¤ºä¾‹è¾“å‡ºå¯¹ï¼‰ï¼Œæ¨¡å‹å°±èƒ½ä¸ºæ–°è¾“å…¥ç”Ÿæˆæ­£ç¡®è¾“å‡ºè€Œä¸éœ€è¦å¯¹æ¨¡å‹åš fine-tuningã€‚

## ä»€ä¹ˆæ˜¯ICLï¼ˆIn-Context Learningï¼‰ï¼Ÿ

`æƒ…å¢ƒå­¦ä¹ `ï¼ˆIn-context Learningï¼‰æä¾›å‡ ä¸ªä»»åŠ¡ç¤ºä¾‹/è¯´æ˜ï¼Œè¦æ±‚é¢„è®­ç»ƒæ¨¡å‹è¦å¯¹ä»»åŠ¡æœ¬èº«è¿›è¡Œç†è§£ã€‚

GPT-n ç³»åˆ—æ¨¡å‹éƒ½å±äº**è‡ªå›å½’**ç±»çš„è¯­è¨€æ¨¡å‹
- `è‡ªå›å½’æ¨¡å‹`: æ ¹æ®å½“å‰è¾“å…¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œç„¶åå°†é¢„æµ‹ç»“æœå’Œè¾“å…¥æ‹¼æ¥å†å½“åšæ¨¡å‹çš„è¾“å…¥å†é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¿™æ ·å¾ªç¯å¾€å¤ã€‚

è‡ªå›å½’æ¨¡å‹çš„è®­ç»ƒç›®æ ‡: ä»è¶…å¤§è§„æ¨¡è¯­æ–™åº“ä¸­é‡‡æ ·è®­ç»ƒæ ·æœ¬ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥è¾“å‡ºä¸€ä¸ªæ¦‚ç‡å‘é‡ï¼ˆæ¦‚ç‡å‘é‡åŒ…å«æ‰€æœ‰è¯çš„é¢„æµ‹æ¦‚ç‡ï¼Œå¯¹äºGPT-3 æ¨¡å‹æ¥è¯´ï¼Œç»´åº¦çº¦1åƒå¤šä¸‡ï¼‰ï¼Œè€Œå› ä¸ºæ–‡æœ¬æ•°æ®è‡ªå¸¦æ ‡æ³¨æ‰€ä»¥æˆ‘ä»¬æ˜¯çŸ¥é“çœŸå®çš„ä¸‹ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥æŸå¤±å‡½æ•°å°±é‡‡ç”¨å¾—`äº¤å‰ç†µ`ã€‚

é¢„è®­ç»ƒå¥½çš„ GPT-3 æ¨¡å‹æ‹¥æœ‰ä¸€é¡¹ç¥å¥‡çš„èƒ½åŠ›ï¼Œåæ¥è¢«ç§°ä¸ºï¼š`ä¸Šä¸‹æ–‡å­¦ä¹ `ï¼ˆIn-Context Learningï¼‰ã€‚
- é¢„è®­ç»ƒå¥½çš„ GPT-3 æ¨¡å‹åœ¨è¿ç§»åˆ°æ–°ä»»åŠ¡æ—¶å¹¶ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œè€Œåªéœ€è¦æä¾›**ä»»åŠ¡æè¿°**ï¼ˆå¯é€‰é¡¹ï¼‰æ¥ç€æä¾›å‡ ä¸ª**ç¤ºä¾‹**ï¼ˆä»»åŠ¡æŸ¥è¯¢å’Œå¯¹åº”ç­”æ¡ˆï¼Œä»¥ä¸€å¯¹å¯¹çš„å½¢å¼ç»„ç»‡ï¼‰ï¼Œæœ€ååŠ ä¸Šè¦æ¨¡å‹å›ç­”çš„æŸ¥è¯¢ã€‚å°†ä»¥ä¸Šå†…å®¹æ‰“åŒ…ä¸€èµ·ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œåˆ™æ¨¡å‹å°±èƒ½æ­£ç¡®è¾“å‡ºæœ€åä¸€ä¸ªæŸ¥è¯¢å¯¹åº”çš„ç­”æ¡ˆã€‚

ç”¨ GPT-3 æ¥åšä¸ªç¿»è¯‘ä»»åŠ¡ï¼Œç¿»è¯‘è‹±æ–‡ä¸ºæ³•æ–‡ã€‚è¾“å…¥çš„æ ¼å¼å¦‚ä¸‹:
- ![icl](https://pic3.zhimg.com/80/v2-2c3994fe518c7f154b7541a91fb49a7e_1440w.webp)
- ç¬¬ä¸€è¡Œæ˜¯å¯¹ä»»åŠ¡æè¿°ï¼Œå‘Šè¯‰æ¨¡å‹è¦åšç¿»è¯‘
- æ¥ä¸‹æ¥ä¸‰è¡Œå°±æ˜¯ç¤ºä¾‹ï¼Œè‹±æ–‡å•è¯å’Œå¯¹åº”çš„æ³•æ–‡å•è¯å¯¹
- æœ€åä¸€è¡Œå°±æ˜¯å¾…ç¿»è¯‘çš„è‹±æ–‡å•è¯ã€‚

å°†ä»¥ä¸Šå†…å®¹æ•´ä½“ä½œä¸º GPT-3 çš„è¾“å…¥ï¼Œè®©æ¨¡å‹å»è¡¥å…¨è¾“å‡ºå°±èƒ½å¾—åˆ° cheese å¯¹åº”çš„æ³•æ–‡å•è¯ã€‚

ä¸Šä¸‹æ–‡å­¦ä¹ éå¸¸çµæ´»ï¼Œé™¤äº†ä¸Šé¢å±•ç¤ºçš„ç¿»è¯‘ä»»åŠ¡ï¼Œè¿˜å¯ä»¥åšè¯­æ³•ä¿®é¥°ç”šè‡³å†™ä»£ç ã€‚
- è€Œç¥å¥‡çš„åœ°æ–¹å°±åœ¨äºï¼ŒGPT-3 è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¹¶æ²¡æœ‰æ˜¾å¼æä¾›ï¼Œç±»ä¼¼æµ‹è¯•é˜¶æ®µä»»åŠ¡æè¿°åŠ ç¤ºä¾‹è¿™æ ·çš„è®­ç»ƒæ•°æ®ã€‚

å½“ç„¶ GPT-3 çš„è®­ç»ƒæ•°æ®é‡éå¸¸å·¨å¤§ï¼ˆåŒ…å«äº† wiki, ä¹¦æœ¬æœŸåˆŠï¼Œreddit ä¸Šçš„è®¨è®ºç­‰ç­‰ï¼‰ï¼Œæˆ–è®¸é‡Œé¢å°±å·²ç»å°±åŒ…å«äº†å„ç§ä»»åŠ¡ç±»ä¼¼ç»“æ„çš„æ•°æ®ï¼ŒGPT-3 æ¨¡å‹å®¹é‡è¶³å¤Ÿå¤§èƒ½å¤Ÿå°†æ‰€æœ‰è®­ç»ƒæ•°æ®éƒ½è®°äº†ä¸‹æ¥ã€‚

å¯¹äºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„æˆå› ï¼Œç›®å‰è¿˜æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚ä¸ºä»€ä¹ˆåªæœ‰å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹æ‰ä¼šå…·å¤‡è¯¥èƒ½åŠ›ï¼Ÿæˆ–è®¸åªæœ‰æ¨¡å‹å‚æ•°é‡å¤§è¿˜ä¸å¤Ÿï¼Œè¿˜å¿…é¡»è¦è®­ç»ƒæ•°æ®é‡ä¹Ÿè¶³å¤Ÿå¤§ï¼Œæ¨¡å‹æ‰èƒ½æ˜¾ç°å‡ºè¯¥èƒ½åŠ›ï¼Ÿ


### ICL ä»‹ç»

In Context Learningï¼ˆICLï¼‰çš„æ ¸å¿ƒæ€æƒ³ï¼š<span style='color:blue'>ä»ç±»æ¯”ä¸­å­¦ä¹ </span>ã€‚

ä¸€ä¸ªæè¿°è¯­è¨€æ¨¡å‹å¦‚ä½•ä½¿ç”¨ICLè¿›è¡Œå†³ç­–çš„ä¾‹å­ã€‚
- ![img](https://pic2.zhimg.com/80/v2-16eadb0b8f3e1c03bd5a642b83a61279_1440w.webp)
- é¦–å…ˆï¼ŒICLéœ€è¦ä¸€äº›ç¤ºä¾‹, å½¢æˆä¸€ä¸ª`æ¼”ç¤ºä¸Šä¸‹æ–‡`ã€‚è¿™äº›ç¤ºä¾‹é€šå¸¸æ˜¯ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ç¼–å†™çš„ã€‚
- ç„¶åï¼ŒICLå°†æŸ¥è¯¢çš„`é—®é¢˜`ï¼ˆå³inputï¼‰å’Œä¸€ä¸ª`ä¸Šä¸‹æ–‡æ¼”ç¤º`ï¼ˆä¸€äº›ç›¸å…³çš„casesï¼‰è¿æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆå¸¦æœ‰æç¤ºçš„è¾“å…¥ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œé¢„æµ‹ã€‚

æ³¨æ„ï¼š
- ä¸è®­ç»ƒé˜¶æ®µç”¨åå‘æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„ç›‘ç£å­¦ä¹ ä¸åŒï¼Œ`ICL`ä¸éœ€è¦å‚æ•°æ›´æ–°ï¼Œå¹¶ç›´æ¥å¯¹é¢„å…ˆè®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹
  - è¿™æ˜¯ICLä¸promptï¼Œä¼ ç»Ÿdemonstration learningä¸åŒçš„åœ°æ–¹ï¼ŒICLä¸éœ€è¦åœ¨ä¸‹æ¸¸ P-tuning æˆ– Fine-tuningã€‚
- å¸Œæœ›è¯¥æ¨¡å‹å­¦ä¹ éšè—åœ¨æ¼”ç¤ºä¸­çš„æ¨¡å¼ï¼Œå¹¶æ®æ­¤åšå‡ºæ­£ç¡®çš„é¢„æµ‹ã€‚

ç»“è®º1ï¼š
- ICLä¸­ Ground Truth ä¿¡æ¯æ— å…³ç´§è¦
- ICLçš„æ€§èƒ½æ”¶ç›Šä¸»è¦æ¥è‡ªç‹¬ç«‹è§„èŒƒçš„ `è¾“å…¥ç©ºé—´` å’Œ `æ ‡ç­¾ç©ºé—´` ï¼Œä»¥åŠæ­£ç¡®ä¸€è‡´çš„æ¼”ç¤ºæ ¼å¼

æ¨¡å‹æ˜¯å¦åœ¨Testé˜¶æ®µå­¦ä¹ åˆ°äº†çŸ¥è¯†ï¼Ÿ
- å¦‚æœå¯¹å­¦ä¹ è¿›è¡Œä¸¥æ ¼çš„å®šä¹‰ï¼Œå³å­¦ä¹ åœ¨è®­ç»ƒæ•°æ®ä¸­ç»™å‡ºçš„è¾“å…¥**æ ‡ç­¾å¯¹**ï¼Œé‚£ä¹ˆï¼Œlmåœ¨æµ‹è¯•æ—¶ä¸å­¦ä¹ æ–°çš„ä»»åŠ¡ã€‚
- ç„¶è€Œï¼Œå­¦ä¹ ä¸€é¡¹æ–°ä»»åŠ¡æ›´å¹¿æ³›åœ°è§£é‡Šï¼šå¯èƒ½åŒ…æ‹¬é€‚åº”ç‰¹å®šçš„è¾“å…¥å’Œæ ‡ç­¾åˆ†å¸ƒä»¥åŠæ¼”ç¤ºçš„æ ¼å¼ï¼Œå¹¶æœ€ç»ˆæ›´å‡†ç¡®åœ°åšå‡ºé¢„æµ‹ã€‚
- æœ‰äº†å®šä¹‰ï¼Œè¯¥æ¨¡å‹ç¡®å®å¯ä»¥ä»æ¼”ç¤ºä¸­å­¦ä¹ ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç¡®å®åˆ©ç”¨äº†æ¼”ç¤ºçš„å„ä¸ªæ–¹é¢ï¼Œå¹¶å®ç°äº†æ€§èƒ½çš„æé«˜ã€‚

æ¼”ç¤ºæ˜¯å¦‚ä½•è®© In-context learning åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­äº§ç”Ÿæ€§èƒ½å¢ç›Šçš„ï¼Œè€Œä¸”éšç€ fine-tune é˜¶æ®µçš„**é»‘ç›’åŒ–**ï¼Œå¾ˆå¤šæ–‡ç« ä¹Ÿæå‡º fine-tune é˜¶æ®µå¯èƒ½è®©æ¨¡å‹ä¸§å¤±äº†æ³›åŒ–æ€§ï¼Œé‚£ä¹ˆICLè¿™ç§ä¸fine tuneçš„æ–¹æ³•æ—¢èŠ‚çœæ—¶é—´ä¸èµ„æºå¼€é”€ï¼Œä¸”èƒ½æå‡æ•ˆæœï¼Œåº”è¯¥ä¼šåœ¨å¤§æ¨¡å‹æ—ç«‹çš„æ—¶ä»£è¢«äººå…³æ³¨ï¼Œå¹¶è¿…é€Ÿç«èµ·æ¥ã€‚

### ICL å‘å±•å†å²

æ¼”å˜å†ç¨‹
- 2021å¹´åˆï¼ŒPrompt learning æç¤ºå­¦ä¹ 
- 2021å¹´åº•ï¼ŒDemonstration learning æ¼”ç¤ºå­¦ä¹ 
- 2022å¹´åˆï¼ŒIn-cotnext learning æƒ…å¢ƒå­¦ä¹ 

<div class="mermaid">
    flowchart TD
    %% èŠ‚ç‚¹é¢œè‰²
    classDef red fill:#f02;
    classDef green fill:#5CF77B;
    classDef blue fill:#6BE0F7;
    classDef orange fill:#F7CF6B;
    classDef grass fill:#C8D64B;
    %%èŠ‚ç‚¹å…³ç³»å®šä¹‰
    O(ä¸¤é˜¶æ®µèŒƒå¼):::orange-->|2021å¹´åˆ,æç¤ºæ¨¡æ¿|A(æç¤ºå­¦ä¹ ):::blue
    A-->|2021å¹´åº•,æ¼”ç¤ºæ¡ˆä¾‹|B(æ¼”ç¤ºå­¦ä¹ ):::grass
    B-->|2022å¹´åˆ,ä¸Šä¸‹æ–‡ä¿¡æ¯|C(æƒ…å¢ƒå­¦ä¹ ):::grass
</div>

## ä¸Šä¸‹æ–‡å­¦ä¹ åŸç†

### ICL åŸç†

[How does in-context learning work?](http://ai.stanford.edu/blog/understanding-incontext/)
- [In-context learningç»¼è¿°](https://arxiv.org/pdf/2301.00234.pdf)
- [github](https://github.com/dtsip/in-context-learning)
- [In-Context Paperlist](https://github.com/dongguanting/In-Context-Learning_PaperList)

å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å…¶ä¸­ä¸€ä¸ªé‡è¦ç‰¹ç‚¹ï¼š`ä¸Šä¸‹æ–‡å­¦ä¹ `ï¼ˆIn-Context Learningï¼Œ`ICL`ï¼‰èƒ½åŠ›ï¼Œå³é€šè¿‡ä¸€äº›ç¤ºèŒƒæ€§çš„ **\<è¾“å…¥-æ ‡ç­¾>** å¯¹ï¼Œå°±å¯ä»¥åœ¨ä¸æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹å¯¹æ–°è¾“å…¥çš„æ ‡ç­¾è¿›è¡Œé¢„æµ‹ã€‚

æ€§èƒ½è™½ç„¶ä¸Šå»äº†ï¼Œä½†å¤§æ¨¡å‹çš„`ICL`èƒ½åŠ›åˆ°åº•ä»ä½•è€Œæ¥ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ICLçš„å·¥ä½œåŸç†ï¼Œæ¸…åå¤§å­¦ã€åŒ—äº¬å¤§å­¦å’Œå¾®è½¯çš„ç ”ç©¶äººå‘˜å…±åŒå‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œå°†è¯­è¨€æ¨¡å‹è§£é‡Šä¸º`å…ƒä¼˜åŒ–å™¨`ï¼ˆmeta-optimizerï¼‰ï¼Œå¹¶å°†ICLç†è§£ä¸ºä¸€ç§éšæ€§çš„ï¼ˆimplicitï¼‰å¾®è°ƒã€‚

æƒ…å¢ƒå­¦ä¹ ä¸‰ç§åˆ†ç±»çš„å®šä¹‰å’Œç¤ºä¾‹å¦‚ä¸‹ï¼š
- `few-shot learning` **å¤šä¸ªç¤ºä¾‹**
  - è¾“å…¥ï¼šâ€œè¿™ä¸ªä»»åŠ¡è¦æ±‚å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚ä½ å¥½->helloï¼Œå†è§->goodbyeï¼Œè´­ä¹°->purchaseï¼Œé”€å”®->â€
  - è¦æ±‚æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆï¼Œæ­£ç¡®ç­”æ¡ˆåº”ä¸ºâ€œsellâ€ã€‚
- `one-shot learning` **ä¸€ä¸ªç¤ºä¾‹**
  - è¾“å…¥ï¼šâ€œè¿™ä¸ªä»»åŠ¡è¦æ±‚å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚ä½ å¥½->helloï¼Œé”€å”®->â€
  - è¦æ±‚æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆï¼Œæ­£ç¡®ç­”æ¡ˆåº”ä¸ºâ€œsellâ€ã€‚
- `zero-shot learning`
  - è¾“å…¥ï¼šâ€œè¿™ä¸ªä»»åŠ¡è¦æ±‚å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚é”€å”®->â€
  - è¦æ±‚æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆï¼Œæ­£ç¡®ç­”æ¡ˆåº”ä¸ºâ€œsellâ€ã€‚

### ICL ä¸ GPT

GPT-3 ä¸­Few-shot Learningæ²¡æœ‰ fine-tuneï¼Œç›´æ¥å½“åš GPT model çš„è¾“å…¥ï¼Œæ²¡æœ‰è°ƒæ•´æ¨¡å‹
- [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- GPTé¦–å…ˆæ ¹æ®ç¤ºèŒƒå®ä¾‹äº§ç”Ÿå…ƒæ¢¯åº¦ï¼Œç„¶åå°†è¿™äº›å…ƒæ¢¯åº¦åº”ç”¨äºåŸå§‹çš„GPTï¼Œå»ºç«‹ICLæ¨¡å‹ã€‚
- ICLåœ¨é¢„æµ‹å±‚é¢ã€è¡¨å¾å±‚é¢å’Œæ³¨æ„è¡Œä¸ºå±‚é¢çš„è¡¨ç°ä¸æ˜¾å¼å¾®è°ƒç±»ä¼¼ã€‚
æ­¤å¤–ï¼Œå—åˆ°å…ƒä¼˜åŒ–ç†è§£çš„å¯å‘ï¼Œé€šè¿‡ä¸åŸºäºåŠ¨é‡çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ç±»æ¯”ï¼Œæ–‡ä¸­è¿˜è®¾è®¡äº†ä¸€ä¸ªåŸºäºåŠ¨é‡çš„æ³¨æ„åŠ›ï¼Œæ¯”æ™®é€šçš„æ³¨æ„åŠ›æœ‰æ›´å¥½çš„è¡¨ç°ï¼Œä»å¦ä¸€ä¸ªæ–¹é¢å†æ¬¡æ”¯æŒäº†è¯¥ç†è§£çš„æ­£ç¡®æ€§ï¼Œä¹Ÿå±•ç°äº†åˆ©ç”¨è¯¥ç†è§£å¯¹æ¨¡å‹åšè¿›ä¸€æ­¥è®¾è®¡çš„æ½œåŠ›ã€‚

ã€2023-2-8ã€‘[In-Context Learningï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰ç›¸å…³åˆ†äº«](https://zhuanlan.zhihu.com/p/603650082)

## ç¤ºä¾‹è®²è§£

### æ‹·è´è¾“å‡º

ä¸€ä¸ªå¾ˆç®€å•çš„ä»»åŠ¡ï¼Œè®©æ¨¡å‹ç›´æ¥å¤åˆ¶è¾“å…¥çš„å†…å®¹ã€‚

é¦–å…ˆç¤ºä¾‹ä¸ªæ•°è®¾ç½®ä¸º 5ä¸ªï¼Œæ¯ä¸ªç¤ºä¾‹è¾“å…¥åŒ…å« 5 ä¸ªä¸åŒçš„å°å†™å•è¯ï¼ˆä»å­—æ¯è¡¨å‰ 8 ä¸ªå°å†™å­—æ¯ä¸­éšæœºé€‰5ä¸ªå¾—åˆ°ï¼‰ï¼Œè¿™äº›å•è¯ç”¨é€—å·åˆ†éš”ï¼Œè¾“å‡ºç›´æ¥æ‹·è´çš„è¾“å…¥ï¼Œæ¯”å¦‚ï¼š

```s
Input: g, c, b, h, d
Output: g, c, b, h, d
Input: b, g, d, h, a
Output: b, g, d, h, a
Input: f, c, d, e, h
Output: f, c, d, e, h
Input: c, f, g, h, d
Output: c, f, g, h, d
Input: e, f, b, g, d
Output: e, f, b, g, d
Input: a, b, c, d, e
Output:
```

æœŸå¾…æ¨¡å‹çš„è¾“å‡ºæ˜¯ï¼š
- a, b, c, d, e

æ¥ç€å¯¹äº5ä¸ªå­—æ¯é¡ºåºçš„**æ‰€æœ‰**å¯èƒ½æƒ…å†µ (8!/3!=6720ï¼Œä»8ä¸ªæ ·æœ¬ä¸­é€‰5ä¸ªæ€»çš„ç»„åˆæ•°)ä¹Ÿå°±æ˜¯æœ€å input çš„ä½ç½®å°† 6720 ä¸ªæƒ…å†µéƒ½æµ‹è¯•äº†
- GPT-3 æ¨¡å‹çš„å‡†ç¡®ç‡æ˜¯ 100%ã€‚
- GPT-3 ç³»åˆ—æœ€å°çš„æ¨¡å‹ text-ada-001 æ¥åšè¿™ä¸ªä»»åŠ¡ï¼Œè·å¾—äº† 6705/6720 = 99.78% çš„å‡†ç¡®ç‡

ä¸€å®šç¨‹åº¦ä¸Šè¯æ˜äº†<span style='color:red'>æ¨¡å‹è§„æ¨¡çš„é‡è¦æ€§</span>ã€‚

### æ ¼å¼åŒ–æ—¥æœŸ

å¤æ‚ä¸€äº›çš„ä»»åŠ¡ã€‚
- å¯¹æ—¥æœŸåšæ ¼å¼åŒ–ï¼Œå°† **å¹´-æœˆ-æ—¥** æ ¼å¼çš„è¾“å…¥æ ¼å¼è½¬åŒ–æˆ **!æœˆ!æ—¥!å¹´!**ï¼Œå…¶ä¸­å¹´ä»½å››ä½æ•°ï¼Œæœˆä»½å’Œæ—¥å­æ˜¯ä¸¤ä½æ•°ï¼Œæ¯”å¦‚ï¼š
- ![](https://pic4.zhimg.com/80/v2-ba83f805fcd0ea4c5b98c768f9e1e807_1440w.webp)
- æºè‡ªï¼šæ–¯å¦ç¦[in-context-learning](https://ai.stanford.edu/blog/in-context-learning/)

ç¤ºä¾‹ä¸ªæ•°æ˜¯3ï¼Œæœ€åæ˜¯å¾…æµ‹è¯•çš„æ—¥æœŸï¼š2005-07-23ã€‚

ä¸ºä»€ä¹ˆé€‰æ‹©æ—¥æœŸæ ¼å¼åŒ–è¿™ä¸ªä»»åŠ¡å‘¢ï¼Ÿ
- è¶³å¤Ÿç®€å•ï¼Œæ—¥æœŸåŒ…å«ä¸‰ä¸ªéšæœºå˜é‡ï¼ˆå¹´æœˆæ—¥ï¼‰ï¼Œé•¿åº¦å›ºå®šï¼Œè€Œä¸”è®¾å®šçš„è¾“å‡ºæ ¼å¼ä¹Ÿä¸æ˜¯æ­£å¸¸æ ¼å¼ï¼Œæ‰€ä»¥è®­ç»ƒæ•°æ®ä¸­ä¸å¤ªå¯èƒ½åŒ…å«ç±»ä¼¼çš„æ ·æœ¬ï¼Œä¹Ÿæ’é™¤äº†æ¨¡å‹å¯èƒ½åªæ˜¯å°†è®­ç»ƒæ•°æ®éƒ½**è®°å¿†**äº†ä¸‹æ¥ã€‚

æ¥ä¸‹æ¥çœ‹çœ‹æµ‹è¯•ç»“æœ
- GPT-3 å…¨ç³»åˆ—çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ text-ada-001,text-babbage-001,text-curie-001 å’Œ text-davinci-003ï¼Œæ¨¡å‹å‚æ•°é‡ä¾æ¬¡ä»å°åˆ°å¤§æ’åˆ—ã€‚å¹¶é€šè¿‡è®¾ç½®ä¸åŒçš„**ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸ªæ•°**ï¼ˆå¯¹äºæ¯ä¸ªç¤ºä¾‹ä¸ªæ•°çš„è®¾ç½®ï¼Œéƒ½æœ‰2000ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼Œè®°å½•å„ä¸ªæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡ï¼Œæµ‹è¯•ç»“æœå¦‚ä¸‹ï¼š
- ![](https://pic2.zhimg.com/80/v2-fe5d963c6fba070092190a8e9b8ee2b9_1440w.webp)

åˆ†æï¼š
- å›ºå®šæ¨ªåæ ‡ç¤ºä¾‹ä¸ªæ•°ï¼Œåˆ™æ¨¡å‹è¶Šå¤§å‡†ç¡®ç‡ä¹Ÿè¶Šé«˜ï¼Œ**æ¨¡å‹è¶Šå¤§**å‡†ç¡®ç‡æ›²çº¿ä¹Ÿå°±æ›´åŠ çš„é™¡å³­ã€‚
- è€Œå¯¹äºæ¯ä¸ªæ¨¡å‹æ¥è¯´ï¼Œ**å¢åŠ ç¤ºä¾‹ä¸ªæ•°**ä¹Ÿèƒ½æœ‰æ•ˆæå‡å‡†ç¡®ç‡ã€‚
- ä¸è¿‡ï¼Œå³ä½¿å¢å¤§ç¤ºä¾‹ä¸ªæ•°å’Œæ¨¡å‹ï¼Œæ¨¡å‹çš„ç²¾ç¡®åº¦ä¹Ÿåªæ˜¯æ— é™æ¥è¿‘ 100% ä½†è¿˜æ˜¯è¾¾ä¸åˆ°ã€‚

GPT-3 é¢„æµ‹é”™è¯¯çš„æ ·æœ¬éƒ½åŒ…å«å“ªäº›ç±»å‹ã€‚
- éšç€ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸ªæ•°çš„å¢åŠ ï¼Œé¢„æµ‹é”™è¯¯çš„æ ·æœ¬ä¸ªæ•°ä¹Ÿåœ¨ä¸‹é™ã€‚
- 2019 å¹´ä»½çš„è¾“å…¥ï¼Œæ¨¡å‹æ˜¯æœ€å®¹æ˜“é¢„æµ‹é”™è¯¯çš„ï¼Œè¿™ä¹Ÿèƒ½ç†è§£å› ä¸ºè®­ç»ƒæ•°æ®ä¸­ 2019 å¹´ä»½çš„æ•°æ®ä¸å¤š

### æ ‡ç­¾é‡æ˜ å°„

ä»»åŠ¡æè¿°
- å°†å®ä½“åšä¸€ä¸ªä¸æ­£å¸¸çš„é‡æ–°åˆ†ç±»

```s
volleyball: animal
onions: sport
broccoli: sport
hockey: animal
kale: sport
beet: sport
golf: animal
horse: plant/vegetable
corn: sport
...
```

è¾“å…¥ç¤ºä¾‹ä¸­åŒ…å«äº† \[animalï¼ˆåŠ¨ç‰©ï¼‰, plant/vegetableï¼ˆæ¤ç‰©/è”¬èœï¼‰, sportï¼ˆè¿åŠ¨ï¼‰\] ä¸‰ç§ç±»å‹æ ‡ç­¾ã€‚

ç°åœ¨å°†åŸæ¥çš„æ ‡ç­¾æ˜ å°„æ‰“ä¹±ï¼Œå°†**åŠ¨ç‰©**æ˜ å°„ä¸º**æ¤ç‰©**ï¼ˆduck: plant/vegetableï¼‰ï¼Œå°†**è¿åŠ¨**æ˜ å°„ä¸º**åŠ¨ç‰©**ï¼ˆgolf: animalï¼‰ï¼Œå°†**æ¤ç‰©**æ˜ å°„ä¸º**è¿åŠ¨**ï¼ˆbeans: sportï¼‰ã€‚

æµ‹è¯• GPT-3 èƒ½å¦æ ¹æ®ä»…æœ‰çš„ç¤ºä¾‹å­¦ä¼šé¢„æµ‹æ–°çš„æ˜ å°„ï¼šGPT-3 èƒ½æ­£ç¡®è¾“å‡ºæ˜ å°„å…³ç³»ã€‚

## ICLèƒ½åŠ›æˆå› 

ä¸ºä»€ä¹ˆ LLM èƒ½å¤Ÿå…·å¤‡è¯¥èƒ½åŠ›ï¼Ÿä¸Šä¸‹æ–‡å­¦ä¹ çš„åŸç†ç©¶ç«Ÿæ˜¯æ€æ ·çš„å‘¢ï¼Ÿ

å¾®è½¯ç ”ç©¶é™¢å‘å¸ƒçš„æ–‡ç« ï¼Œå¯¹äºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¥æºçš„æ¢ç©¶ã€‚
- è®ºæ–‡ï¼š[Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we figure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit finetuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit finetuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more importantly, it shows the potential to utilize our understanding for future model designing.
- å…³é”®åœ¨äº LLM ä¸­çš„`æ³¨æ„åŠ›å±‚`ï¼ˆattention layersï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹å®ç°äº†ä¸€ä¸ªéšå¼å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™å’Œ fine-tuning çš„æ—¶å€™é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•æ˜¾å¼ä¼˜åŒ–å‚æ•°çš„è¿‡ç¨‹æ˜¯ç±»ä¼¼çš„ã€‚

åŸºäºæ¢¯åº¦ä¸‹é™æ³•çš„ä¼˜åŒ–è¿‡ç¨‹å’Œæ³¨æ„åŠ›å±‚çš„è”ç³»
- ä¸€ä¸ªçº¿æ€§æ³¨æ„åŠ›å±‚å…¶å®å’ŒåŸºäºæ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–çš„å…¨è¿æ¥å±‚æ˜¯äº’ä¸º**å¯¹å¶**çš„å½¢å¼

ä¸Šä¸‹æ–‡å­¦ä¹ æ€ä¹ˆå®ç°éšå¼ finetuning
- ç•¥ï¼Œè¯¦è§ï¼š[ä¸€æ–‡ç†è§£â€œä¸Šä¸‹æ–‡å­¦ä¹ â€----å¤§è¯­è¨€æ¨¡å‹çªç°èƒ½åŠ›](https://zhuanlan.zhihu.com/p/606788655)

# ç»“æŸ
