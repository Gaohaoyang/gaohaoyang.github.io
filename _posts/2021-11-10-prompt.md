---
layout: post
title:  提示学习 Prompt learning
date:   2021-11-10 19:58:00
categories: 深度学习 自然语言处理
tags: 自监督 prompt 思维链
excerpt: NLP新范式：Prompt（提示学习）
mathjax: true
permalink: /prompt
---

* content
{:toc}

# NLP新范式：Prompt


## 最新进展

### 进展

【2023-3-22】[一文带你概览Prompt工作新进展](https://zhuanlan.zhihu.com/p/446660117)

Prompt Learning（`提示学习`）已经成为现在NLP研究的**第四范式**，现在已经成为NLP领域一大热点。
- `刘鹏飞`博士在7月的综述更是将Prompt系统性地进行了总结，让更多人所熟知。
- 自8月以来，粗略统计，约为**109篇**关于Prompt的论文发表在EMNLP 2021、arXiv或投稿ICLR 2022、ARR（ACL 2022所采用的每月滚动评审机制）。笔者从中挑选了13行解读。

根据论文的创新点分成 `Prompt Engineering`，`Answer Engineering`和`Multi-Prompt Learning`来进行整理讨论，每个类别里根据论文出现的时间进行排序。

#### Prompt Engineering

(1) PPT: Pre-trained Prompt Tuning for Few-shot Learning – 9.9 ARR Oct.

超大模型的预训练的Soft Prompt，在NLU任务中进行了小样本实验。首先作者通过先导实验得出了4个结论：
- ① Verbalizer的选择对结果有很大影响；
- ② 将Soft Prompt初始化为具体单词的Embedding并不能提升性能；
- ③ 使用Hard和Soft的混合Prompt有帮助；
- ④ Google提出的Prompt Tuning并不能在小样本场景下取得和微调一样的效果。因此引出了Soft Prompt预训练的想法。

Prompt框架是，基于Hard和Soft的混合完形填空式Prompt，人工设计的Answer，以T5-XXL（英文）为框架，只微调Soft Prompt。论文提出将NLU任务划分为三种：**单句分类**（情感分类）、**句子对分类**（NLI）、**多选分类**（阅读理解），同时这三种任务也可以统一为多选分类任务。然后，在大规模无标注语料上设计针对三种任务的自监督任务，依次达到预训练Soft Prompt的目的。

(2) P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks – 10.14 arXiv
- 聚焦于通用NLU任务的提示方法。Prompt架构类似于Prefix-Tuning，在Transformer的每一层前加上连续型Prompt，利用\[CLS\]进行预测（没有Verbalizer），模型尝试了BERT-Large，RoBERTa-Large等，仅微调Prompt参数。笔者认为论文的想法和写作一般，但是进行了大量实验，有一些结论值得借鉴。例如进行了Prompt长度、层数、重参数方法、多任务、Verbalizer探究，并在一些序列标注任务上进行了实验。

(3) Towards Unified Prompt Tuning for Few-shot Learning (UPT) – 11.15 ARR Nov.
- 聚焦小样本的NLU任务。想法有点类似于PPT，通过额外的自监督任务来使得预训练模型适应Prompt。具体地
- 论文设计了统一的Prompt-Answer-Verbalizer框架，即`[INPUT] Is it [x1] or [x2]? It is [MASK]`；
- 然后设计了自监督任务，在无标注\[INPUT\]上mask形容词（与大部分NLU任务一致）
- 然后将其作为\[x1],再挑选意思相反的形容词作为\[x2]，最后让模型去预测结果。

模型骨架是RoBERTa-Large，微调模型参数。

#### Answer Engineering

(1) Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification (KPT) – 8.4 ARR Sept.
- 聚集于文本（主题、情感）分类。Prompt是人工设计的完形填空式；论文的重点是在Answer的映射段，首先使用KB查询Label的相关词作为候选集，然后提出了3种方法对候选集进行去噪（即删去一些候选词），最后采用平均或者加权平均候选集的方法得到标签；最后模型使用RoBERTa-Large，微调所有参数，在零样本和小样本场景有了一定提升。

(2) Prompt-Learning for Fine-Grained Entity Typing (PLET) - 8.24 arXiv
- 聚焦于细粒度实体分类问题。在有监督场景下，该论文方案比较常规，Prompt是完形填空式，尝试了人工提示、连续型提示两种方法；在Answer映射时与该Label相关的词都加入候选集；论文使用了BERT-Base作为骨架，训练时微调所有参数，该方案在小样本场景相比传统微调有大幅提升。论文有意思的地方是在零样本场景，认为不同句子的同一个实体在候选集上的预测分布应该越相似越好（虽然笔者不是非常认同这个观点），因此采用了对比学习的办法基于少量的标注数据和大量的无标注数据进行自监督学习。

(3) Prototypical Verbalizer for Prompt-based Few-shot Tuning – 11.15 ARR Nov.
- 聚焦于小样本场景下的文本分类任务。与WARP的Soft Verbalizer思想有点类似，本文使用对比学习更加显示地来学习每个Soft Label。具体地，Prompt是人工设计的完形填空式，在训练时，可以得到每个\[MASK]的表示，然后我们希望同一个Label下的\[MASK]表示尽可能接近，并由此学习一个Label的Soft Prompt；模型基于RoBERTa-Large，微调所有参数。但是本论文的结果并不能比过精心设计的Verbalizer，只是在搜索式、连续式中有提升。

#### Multi-Prompt Learning

(1) Finetuned Language Models Are Zero-Shot Learners (FLAN) – 9.3 ICLR 2022
- 提出了基于超大模型的零样本学习方法，该论文正接受ICLR 2022评审，获得4个8分的高分。本文针对62个数据集，每个精心设计了10组人工Prompt和Answer（文中所提的Instruction），然后利用一个预训练的137B的Decoder-only的模型（不是GPT-3，是作者自己利用无标注数据预训练的）结合精心设计的Instruction在60多组数据上进行全参数微调，最后零样本迁移到其他的任务中去。结果表明在大部分数据集上FLAN优于GPT-3，甚至优于GPT-3的小样本学习（专指Demonstration Learning），特别地，FLAN在容易表示成Instruction的任务（NLI，QA）中非常有效，在补全句子（语言建模）这类任务（常识推理，共指消解）中并不是很有效。

(2) SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer – 10.15 arXiv
- 本文聚集于小模型的Soft Prompt学习（可以看做一种预训练）。Prompt Tuning[2]证明了在模型足够大的时候，仅微调Prompt可以媲美微调模型，但是在小模型上还不足以。因此本文提出了Soft Prompt的迁移学习，先在源任务集上学习自己的Prompt，然后将其作为目标任务的Prompt初始化。本文基于Prompt Tuning的Prompt架构，仅在输入前拼上Soft Prompt，然后基于各种尺度的T5，训练微调时仅学习Soft Prompt。论文尝试了几组源任务，发现以GLUE作为源任务，迁移到GLUE和SuperGLUE上效果最佳。但是，笔者发现不同的源任务选择对结果影响较大，有的源任务甚至会低于Prompt Tuning，作者并未提出较优的选择源任务的方法，主要是启发式的人工尝试。

(3) Multitask Prompted Training Enables Zero-Shot Task Generalization (T0) – 10.15 ICLR 2022
- 本文和FLAN思想相似，该论文正接受ICLR 2022评审，获得了8863的得分。本文与FLAN的区别是，基于11B的T5+LM-XXL（Prompt Tuning论文中使用）在171个数据集上使用了近2000个精心设计的Prompt和Answer进行多任务学习。该论文还开发了Prompt模版协作平台，Prompt更加贴合任务特点。最终结果超过GPT-3，与FLAN可比，并且Prompt鲁棒性更好。

(4) Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning (IPT) – 10.15 ARR Nov.
- 本文是一篇分析性工作，主要利用Prompt Tuning聚焦小样本场景下预训练模型中低维度内在任务子空间。IPT基于的Prompt架构，在输入前加上Soft Prompt，以BART作为骨架。论文的核心内容是，先在多任务场景下训练一个Prompt的Auto-encoder，即通过一个Encoder将Soft Prompt编码成任务特定的低维子空间，再通过一个Decoder将其恢复成Soft Prompt；之后在迁移到新任务时，我们固定Decoder，只需要训练一个低维的任务特征向量。作者实验发现，在多任务学习阶段，将Prompt到低维空间再恢复相比Prompt Tuning有所提升，说明了低维的任务子空间的确存在；同时只需要学习一个5~100维的任务特征向量，对于一个新的数据集可以达到Prompt Tuning效果的80%，对于新的任务可以达到60%。但是，该发现还值得进一步研究，使重构子空间可以获得更好的表现和泛化性。

(5) On Transferability of Prompt Tuning for Natural Language Understanding (TPT) – 11.12 arXiv
- 本文聚焦于跨任务、跨模型的Soft Prompt学习。论文的Prompt框架、想法与SPoT一致，模型骨架是RoBERTa-Base。在跨任务迁移中，论文发现相似任务的Soft Prompt可以进行零样本学习，并在全量数据下提升效果，并且加快收敛速度（缓解Prompt Tuning收敛慢的问题）；同时，论文发现Soft Prompt对Transformer的FFN中的激活神经元的重合度，相比于Soft Prompt本身的矩阵相似度，可以更好地度量两个任务的相似度，以此更好地选择源任务Prompt。但在跨模型迁移中，用一个模型的Soft Prompt映射到另一个模型的方法，并不能取得提升。

(6) TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification – 11.6 EMNLP 2021
- 本文聚集于小样本NLU任务。该论文的Prompt框架基于P-Tuning，在输入两端加上连续型Prompt，并用BiLSTM进行编码，最后进行完形填空式预测，模型用RoBERTa-large作为骨架，微调所有参数。论文的核心想法是每一个任务使用一个Prompt，外加一个Universal的Prompt学习通用知识（考虑任务间迁移性），然后进行多任务训练。同时还提出了针对小样本场景提出了Prototype-based和Entropy-based两种去偏方法。最终模型在小样本和全量数据场景下都超越了基线模型。

(7) Contrastive Demonstration Tuning for Pre-trained Language Models (Demo-Tuning) – 11.15 ARR Nov.
- 本文聚集于NLU的Demonstration Learning（GPT-3中的In-context Learning）。作者借鉴了Soft Prompt的思想，提出了Soft Demonstration（笔者命名）想法，缓解了Hard Demonstration采样偏差大、长度受限的问题。论文使用对比学习，训练时交替保留正负例的Hard Demonstration，学习另一个的Soft Demonstration。模型骨架是RoBERTa-Large，微调所有的参数。


### 资料

- 【2023-2-26】[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide), Motivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest **papers**, learning **guides**, **lectures**, references, and **tools** related to prompt engineering.
- 【2023-2-16】清华开源的 [OpenPrompt](https://github.com/thunlp/OpenPrompt)：An Open-Source Framework for Prompt-learning. 包含prompt各个算法、模块的代码及示例
  - [PromptPapers](https://github.com/thunlp/PromptPapers)
  - CMU的Neubig老师的[Advanced NLP课程](http://www.phontron.com/class/anlp2021/schedule/prompting.html)
- 【2021-8-3】[Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://blog.csdn.net/xixiaoyaoww/article/details/119363189)，CMU 博士后研究员刘鹏飞：近代自然语言处理技术发展的第四范式可能是预训练语言模型加持下的 Prompt Learning。
- 从 BERT 开始，**预训练+finetune** 已经成为了整个领域的常规范式。但是从 GPT-3 开始，一种新范式开始引起大家的关注并越来越流行：**prompting**。
- [论文地址](https://arxiv.org/pdf/2107.13586.pdf)，更多研究见：清华大学开源的论文列表 [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers)
- ![img](https://img-blog.csdnimg.cn/img_convert/e538ffef7d05deaf84a5a66225c7f4bc.png)

prompt讲解
- [CMU](https://blender.cs.illinois.edu/course/fall22/lecture9.pdf)

<object type="application/pdf" data="https://blender.cs.illinois.edu/course/fall22/lecture9.pdf"
           id="review" style="width:100%;  height:800px; margin-top:0px;  margin-left:0px" >
</object>

## NLP 范式

全监督学习在 NLP 领域也非常重要。但是全监督的数据集对于学习高质量的模型来说是不充足的，<span style='color:red'>早期的 NLP 模型严重依赖特征工程</span>。
- 随着用于 NLP 任务的神经网络出现，使得特征学习与模型训练相结合，研究者将研究重点转向了**架构工程**，即通过设计一个网络架构能够学习数据特征。

各种模式的对比如下：

|模式paradigm|工程重心engineering|示例|任务关系task relation|
|---|---|---|---|
|① 全监督（非神经网络）|特征|如单词，词性，句子长度等|分类、序列标注、语言模型（无监督）、生成|
|② 全监督（神经网络）|结构|如卷积、循环、自注意力|同上|
|③ pre-train与fine-tune|目标|掩码语言模型、NSP下一句预测|以语言模型为中心，含无监督训练|
|④ pre-train、prompt与predict|提示|完形填空、前缀|语言模型为中心，含文本提示|

- ![img](https://img-blog.csdnimg.cn/img_convert/e7be4976c76f47cf54a95a7dcd2150b9.png)
- 传统的监督学习（不需要神经网络）
- 神经网络-监督学习：不同NLP任务需要单独训练
- pre-train + fine-tune：目前流行的范式，可以适应不同的场景任务
- pre-train + prompt + predict：模板prompt范式，可以适应不同的场景任务

【2023-2-12】[【NLP】Prompt Learning 超强入门教程](https://zhuanlan.zhihu.com/p/442486331), 刘鹏飞在[北京智源大会](https://event.baai.ac.cn/activities/172)上关于 Prompt 的分享

【2023-2-9】
- [【NLP】Prompt Learning 超强入门教程](https://zhuanlan.zhihu.com/p/442486331)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)

很长一段时间内，NLP任务采用的都是 `Pretrain` + `Fine-tuning`（Model Tuning）的解决方案，需要对于每个任务都重新 fine-tune 一个新的模型，且**不能共用**。
- 但是对于一个预训练的大语言模型来说，这就仿佛对于每个任务都进行了**定制化**，十分不高效。
- 是否存在一种方式，可以将预训练语言模型作为**电源**，不同的任务当作**电器**，仅需要根据不同的电器（任务），选择不同的**插座**，对于模型来说，即插入不同的任务特定的参数，就可以使得模型适配该下游任务。
- `Prompt Learning` 就是这个**适配器**，它能高效得进行预训练语言模型的使用。

这种方式大大地提升了预训练模型的**使用效率**，如下图：
- ![](https://pic2.zhimg.com/80/v2-ffa9e652a07961216d1ed260dfdea95d_1440w.webp)
*   左边是传统的 `Model Tuning` 的范式：对于不同的任务，都需要将整个预训练语言模型进行**精调**，每个任务都有自己的一整套参数。
*   右边是`Prompt Tuning`，对于不同任务，仅需要插入不同的 prompt参数，每个任务都单独训练 Prompt 参数，不训练预训练语言模型，这样子可以大大缩短训练时间，也极大的提升了模型的使用率。


### NLP四大范式

NLP四大范式
- 第一范式：**非神经网络时代的完全监督学习**（`特征工程`）。该阶段需要大量任务相关的训练数据，通过特征工程和算法，比较有代表的算法是朴素贝叶斯Naïve Bayes、支持向量机SVM、逻辑回归LR等；
- 第二范式：**基于神经网络的完全监督学习**（`架构工程`）。该阶段也需要大量任务相关的训练数据，通过深度学习方法，自动获取特征(表示学习)进行端到端分类学习；
- 第三范式：**预训练，精调范式**（`目标工程`）：该阶段是当前使用比较多的预训练+微调范式，通过预训练的方式(比如掩码语言模型Masked Language Model)来学习海量的语言学知识，然后下游使用少量的任务相关的数据对预训练模型进行微调即可完成相关任务；
- 第四范式：**预训练，提示，预测范式**（`Prompt工程`）：当前进入了Prompt Learning提示学习的新范式，使用Few shot或者Zero shot即可完成下游任务。
- ![img](https://pic3.zhimg.com/80/v2-c4227aa430fb3be39172a08274f0c5da_1440w.webp)

预训练+微调和Prompt Learning流程对比
- ![](https://pic1.zhimg.com/80/v2-bc80e4117e9cb014decdde51f5685180_1440w.webp)

### NLP 范式发展历史

| 年份	| NLP模型范式变化 |
|---|---|
| 2017年以前 |	传统机器学习模型、神经网络 |
| 2017-2019	| 预训练 + 微调（pre-train + fine-tune） |
| 2019-至今	| ”预训练，prompt和预测“（pre-train，prompt and predict）范式 |

NLP发展历史上的三种范式（时间顺序）
*   很久以前发展起来的`全监督学习` Fully Supervised Learning
  - Fully Supervised Learning，即仅在目标任务的输入输出样本数据集上训练特定任务模型，长期以来在许多机器学习任务中发挥着核心作用，同样的，全监督学习在 NLP 领域也非常重要。
  - ![基于神经网络的监督学习](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
  - 但是全监督数据集对于学习高质量的模型来说是不充足的，**早期的 NLP 模型严重依赖特征工程**。随着用于 NLP 任务的神经网络出现，使得特征学习与模型训练相结合，研究者将研究重点转向了架构工程，即通过设计一个网络架构能够学习数据特征。
  - ![基于神经网络的监督学习](https://pic4.zhimg.com/80/v2-82767e969a98bc63c647374a049f6f17_1440w.webp)
*   前三年火爆的`预训练+微调`  Pre-train, Fine-tune
  - 然而，从 2017-2019 年开始，NLP 模型发生了翻天覆地的变化，这种`全监督范式`发挥的作用越来越小。具体而言，研究重点开始转向预训练、Fine-tuning范式。一个具有固定架构的模型通过**预训练**作为`语言模型`（LM），用来预测观测到的文本数据的概率。
  - ![预训练+微调](https://pic1.zhimg.com/80/v2-9b02ff7acb3ac010d683c76dd53a4c14_1440w.webp)
*   最新的 `预训练+提示+预测 `  Pre-train, Prompt, Predict  
  - 当前正处于第二次巨变中，「预训练、Fine-tuning」过程被「预训练、prompt 和预测」过程所取代。
  - 在这种范式中，不是通过目标工程使预训练的语言模型（LM）适应下游任务，而是**重新形式化**（Reformulate）下游任务，使其看起来更像是在文本 prompt 的帮助下在原始 LM 训练期间解决的任务。
  - 通过这种方式，选择适当的 prompt，该方法可以操纵模型的行为，以便预训练的 LM 本身可以用于预测所需的输出，有时甚至无需任何额外的特定任务训练。
  - 优点是给定一组合适的 prompt，以**完全无监督**方式训练的单个 LM 就能够用于解决大量任务。
  - 然而该方法也存在一个问题 —— 这种方法引入了 **prompt 挖掘工程**的必要性，即需要找出最合适的 prompt 来让 LM 解决面临的问题

Prompt刚刚出现时，还没有被叫做Prompt，是研究者们为了下游任务设计出来的一种输入**形式**或**模板**，它能够帮助PLM“回忆”起自己在预训练时“学习”到的东西，因此后来慢慢地被叫做Prompt了。
- ![prompt](https://pic1.zhimg.com/80/v2-f152c022e31126bd8520e453899aaadc_1440w.webp)

不同于fine-tuning方法，prompt 范式需要给出一个定义好的模板，这个模板可以是离散的或者是连续的，来提醒模型在预训练的时候学习的知识。这是因为预训练的任务和下游任务往往差别较大，模型可能会存在特定性遗忘。

## prompt 介绍

什么是 `Prompt`, Prompt 就是 **提示**：  
- 有人忘记了某个事情，给予特定提示，他就可以想起来
- **白日依山尽**, 大家自然而然地会想起来下一句诗：**黄河入海流**。
- 搜索引擎，可以根据输入，进行输出提示：
  - ![](https://pic3.zhimg.com/80/v2-81d4c4b774e7910935d97fcde34b3842_1440w.webp)
 
文本情感分类：
>- “I missed the bus today.” 句子后紧跟着给出这样一个prompt：“I felt so _____”
>- “English: I missed the bus today. French: ______”

PLM将自动补充单词sad，或使用法语来进行填空。

### prompt演变过程

【2023-2-16】[一个小白如何学好prompt tuning?](https://www.zhihu.com/question/509079916/answer/2757278140)

任务：
>- 对描述的商品类型进行分类
>- 第一句需要被分类到「水果」类别中；
>- 第二句则需要分类到「电脑」类别中。

标注数据集：

```sh
什么苹果啊，都没有苹果味，怪怪的味道，而且一点都不甜，超级难吃！  1
这破笔记本速度太慢了，卡的不要不要的。    0
```

#### （1）早期分类方法

直觉上分类方式：
- 将该问题建模成一个**传统文本分类**任务，通过人工标注，为每一个类别设置一个id，例如：

```json
{
    '电脑': 0,
    '水果': 1,
    ....
}
```

这种方法可行，但需要「较多的标注数据」才能取得不错的效果。

#### （2）BERT 预训练+分类

由于大多数**预训练模型**（如BRET）在 pretrain 的时候都使用了 \[MASK\] token 做 MLM 任务，而实际下游任务中不会使用到 \[MASK\] 这个 token，这就意味着今天训练下游任务时需要较多的数据集去抹平上下游任务不一致的 gap。

#### （3）Prompt

如果没有足够多的训练数据呢？
- prompt learning 就是为了解决这一问题，它将 \[MASK\] 的 token 引入到了下游任务中，<span style='color:blue'>将下游任务构造成类似MLM 的任务</span>。

上述评论改写为：

```sh
这是一条[MASK][MASK]评论：这破笔记本速度太慢了，卡的不要不要的。
```

然后让模型去预测两个 \[MASK\] token 的真实值是什么，那模型根据**上下文**能推测出被掩码住的词应该为「电脑」。由于下游任务中也使用了和预训练任务中同样的 MLM 任务，这样就可以使用更少的训练数据来进行微调了。

#### （4）p-tuning

构建句子最关键的部分是在于 **prompt生成**，即：

```sh
「这是一条[MASK][MASK]评论：」(prompt) + 这破笔记本速度太慢了，卡的不要不要的。(content)
```

被括号括起来的前缀（prompt）的生成非常重要，<span style='color:red'>不同 prompt 会极大影响模型对 \[MASK\] 预测的正确率</span>。

这个 prompt 怎么生成？
- 当然可以通过**人工去设计**很多不同类型的前缀 prompt，即 `prompt pattern`，例如：

```sh
这是一条[MASK][MASK]评论：
下面是一条描述[MASK][MASK]的评论：
[MASK][MASK]：
...
```

但是人工列 `prompt pattern` 非常麻烦，不同数据集所需要的 prompt pattern 也不同，可复用性很低。能不能通过机器自己去学习 prompt pattern 呢？可以，`P-tuning`。

作者：[何枝](https://www.zhihu.com/question/509079916/answer/2757278140)

### 什么是 prompt？

NLP中 `Prompt` 代表的是什么呢？
*   prompt 就是给 预训练语言模型 的一个**线索**/**提示**，更好的理解 人类的问题。

下图的 BERT/BART/ERNIE 均为预训练语言模型，对于人类提出的问题以及线索，预训练语言模型可以给出正确的答案。
- ![](https://pic4.zhimg.com/80/v2-f09af919f520b65363e38d8340ac4e4f_1440w.webp)
*   根据提示，BERT能回答，JDK 是 _Oracle_ 研发的
*   根据 `TL;DR:` 的提示，BART知道人类想要问的是文章的摘要
*   根据提示，ERNIE 知道人类想要问鸟类的能力--飞行

Prompt 更严谨的定义如下：
> - Prompt is the technique of making better use of the knowledge from the pre-trained model by adding additional texts to the input.  
> - Prompt 是一种为了更好的使用预训练语言模型的知识，采用在输入段添加**额外文本**的技术。
 
*   目的：更好挖掘预训练语言模型的能力
*   手段：在输入端添加文本，即重新定义任务（task reformulation）

Prompting, 又称in-context learning `ICL`，指“`预训练-Prompt`"这一NLP最新的范式，也属于`Parameter Efficient`(参数高效)学习方法的一种，但是把Prompting仅仅理解为Parameter Efficient学习方法, 格局小了
- 之前的范式产生的模型从本质上决定了无法在任务级别拥有**泛化能力**，而Prompting在任务级别具有泛化能力，对之前的NLP范式降维打击。
- 预训练+微调的范式可以大幅超越只使用特定任务数据训练的模型。
- 预训练模型的性能和模型的大小有关, 模型越大效果越好。

问题：
- 传统微调需要更新所有模型参数，由于特定任务的那点数据跟预训练数据相比，少得可怜，从而导致训练效果并不好。
- 此外，微调好的模型用到线上时，每个任务甚至每个领域都需要单独部署一个微调好的模型，很不经济实惠。

研究人员提出了<span style='color:blue'>部分参数微调/冻结部分参数</span>的方法：
- 采用关于模型参数分布的先验知识---模型的某些部分有特定的功能（偏向特定的功能）。
- Transformer模型用在机器翻译时，Encoder部分负责将源语言编码成高维向量，Decoder部分负责生成目标语种中的对应意思的句子。
- 假设之后，在微调模型参数时，利用这些先验知识，只更新和目标任务最相关的那部分。

好处：
- （1）训练更快、更有针对性、效果也往往更好
- （2）部署的时候可以多个任务/领域共用一个背景预训练模型，只需针对每个任务/领域替换被更新的部分参数即可。

在上述”部分参数微调“方法的基础之上，研究人员提出了`Adapter`方法。通过添加一个灵活的即插即用的`可训练适配模块`(Adaper Module)，改变模型某一层（或某几层）的数据分布，从而让模型可以适用于各种各样的任务和领域。
- 典型的做法: 每一任务或者每一领域对应了一个Adapter Module，训练时只更新对应任务Adapter的参数，推理的时候激活对应任务/领域的Adapter，其它任务/领域的Adapter直接忽视。

和”`部分参数微调`“方法相比，`Adapter`方法的优势有
- （1）更轻量级
- （2）同样比例的参数效果更好。

以上方法都是基于”预训练-微调“的范式的方法，无论多么轻量级，每个任务都还是有一套特定的参数（即使大部分参数可以共享）。

大多数人其实并没有意识到，致命缺点正是：<span style='color:red'>任务本身需要人为进行定义</span>。
- 但是现实生活中的知识，并不是完全地按照任务进行分类的，现实中大多数任务会涉及到多个任务的能力: 比如一个中国人阅读了一篇英文的文章，最后用中文写了个总结 --- 这就涉及到 翻译+总结 两个任务。

而Prompting可以做到<span style='color:red'>模糊任务的界限</span>---不需要人为对任务进行划分，相反，任务的描述会作为输入的一部分直接输入预训练模型。

这就是为什么Prompting范式是"<span style='color:red'>通往真正大一统语言模型的关键一步</span>。
- ![](https://pic1.zhimg.com/80/v2-3f478c2d338396177ad9f9067cfb028f_720w.webp?source=1940ef5c)
- 两种prompt范式对比：`预训练-微调`范式，`预训练-Prompting`范式

Prompting方法最初在2020年和GPT-3一同提出来。
- Prompt范式认为预训练模型本身就可以完成很多任务，只需要在输入的时候对模型进行引导(又称：提供context)即可

怎么引导呢？
- 最开始的Prompt版本只需要用自然语言将任务本身进行描述，将任务变为"填空"(针对双向模型BERT)或者"生成"(针对自回归模型GPT)的任务即可。

示例
- **文本情绪分类**任务：任务目的是任意输入一段文本，预测对应的情绪（正面、负面、中性）。
  - 经典的预训练-微调范式：将在预训练模型的基础上增加分类器模块，使用特定任务数据进行微调分类器模块。在遇到输入“今天天气很好”时，模型输出“正面”。
  - 而在Prompt范式下，将如下文字直接输入没有经过微调的模型: "今天天气很好，我的心情是[MASK]的" ==> [MASK]预测值是"开心”，再将“开心”映射为"正面"就很容易了。
- **机器翻译**任务：任务本身的目的是输入一种语言的一段文本，模型生成另一种语言的同义句。
  - 在Prompt范式下，将如下文字输入模型：“翻译成英语：今天天气真好” ==> 模型输出“This is a good day.”

Prompt范式特点：
- 无需特定领域的数据进行训练就可以用于各项NLP任务，并且无需在模型参数上做任何调整。

Prompt的成功证明了<span style='color:red'>在模型和训练数据量大到一定程度时，模型本身就更接近“百科全书”，而Prompt就是将“百科全书”里的知识金矿挖掘出来的各种钥匙</span>。
- ![](https://picx.zhimg.com/80/v2-f3381f7d4af76750326ab5858de7f67d_720w.webp?source=1940ef5c)
- 短短5年从GPT-1到GPT3.5，模型变大了3000倍，未来的GPT-4更是拥有100Trillion参数

Prompting方法分为
- (1) 手工Prompt
  - 对任务进行自然语言描述”的方法。
  - 主要分为“`Prefix Prompt`"和"`Cloze Prompt`，其中"`Prefix Prompt`"一般针对生成式NLP任务(NLG)，而`Cloze Prompt`针对理解式NLP任务(NLU) 
- (2)参数化Prompt: 也叫"自动Prompt"，主要分为`离散Prompt`和`连续Prompt`。
  - “离散”指候选的Prompts依然是自然语言的词；
  - “连续”指Prompts本身不需要是自然语言的序列，而可以是词表中的token的任意组合，甚至可以引入不在原来词表中的token。其中比较出名的就是`Prefix Tuning`：在input层添加一串连续的向量(拥有独立embedding的新token)前缀，同时在每个hidden layer对应添加相同长度的前缀(这一步不引入额外参数，但是略微修改了模型结构)。训练时对每个独立的任务做分别的参数更新。需要注意的是，在下游任务上进行训练时，只有Prefix对应的Embedding参数进行更新。
 
 Prompt Tuning 在Prefix Tuning的基础上进一步做了简化，每个hidden layer不再需要对应添加相同长度的前缀，因此模型结构完全没有改变，只是多了prefix token对应的embedding参数(大概占模型总参数的0.1%)。Prompt Tuning也因此更加灵活。
- ![](https://picx.zhimg.com/80/v2-e820c3598183de8d32dfd21e0bb78cd1_720w.webp?source=1940ef5c)

Prompt范式的效果怎么样？又有哪些重要结论呢？
- 预训练-Prompt能达到和预训练-微调相当的效果，即使可训练参数缩减了1000倍
- 模型的规模是决定性因素：模型越大，prompting的模型效果越好
- 对任务有泛化能力，即few-shot / zero-shot能力强


### Prompt 概要

该综述研究试图通过提供 prompting 方法的概述和形式化定义，以及使用这些 prompt 的预训练语言模型的概述，来梳理这一迅速发展领域的当前知识状态。然后该论文对 prompt 方法进行了深入的讨论，包括 **prompt工程**、**answer工程**等基础和**多prompt学习**方法、**prompt相关的训练方法**等更高级的概念。

然后，该研究列出了已有的基于 prompt 学习方法的多种应用，并探讨了不同应用场景中如何选择合适的训练方法。最后，该研究尝试在研究生态系统中定位 prompt 方法的当前状态，并与其他研究领域建立联系。此外，该研究提出一些可能适合进一步研究的挑战性问题，并针对当前研究趋势进行了分析。

基于 Prompt 的学习方法试图通过**学习LM**来规避这一问题，该 LM 对文本 x 本身的概率 P(x;θ) 进行建模并使用该概率来预测 y，从而减少或消除了训练模型对大型监督数据集的需求。

最基本的 Prompt 形式的数学描述，包含许多有关 Prompt 的工作，并且可以扩展到其他内容。

基础 Prompt 分三步预测得分最高的 ^y，即：**prompt 添加**、**answer 搜索**和 **answer 映射**。prompting 方法的术语和符号。
- ![img](https://img-blog.csdnimg.cn/img_convert/c441c670a31e4b2669da835842b9f171.png)
不同任务的输入、模板和 answer 示例：
- ![img](https://img-blog.csdnimg.cn/img_convert/90e84a4ad6cf465e3e306fc376581bcf.png)

### Prompt设计思路

Prompting 设计考虑：
- 预训练模型选择：有许多预训练 LM 可以用来计算 P(x; θ)。在第 3 章中，研究者对预训练 LM 进行了初步的介绍；
- **Prompt 工程**：如果 prompt 指定了任务，那么选择正确的 prompt 不仅对准确率影响很大，而且对模型首先执行的任务也有很大影响。在第 4 章中，研究者讨论了应该选择哪个 prompt 作为 f_prompt(x) 方法；
- **Answer 工程**：根据任务的不同，会有不同的方式设计 Z (Answer)，可能会和映射函数一起使用。在第 5 章中，详细介绍了不同的设计方式；
- **扩展范式**：如上所述， 上面的公式仅仅代表了各种底层框架中最简单的一种，这些框架已经被提议用于执行各种 prompting。在 第 6 章中，研究者讨论了扩展这种基本范式以进一步提高结果或适用性的方法；
- 基于 Prompt 的**训练策略**：在第 7 章中，研究者总结了不同的训练策略并详细说明它们的相对优势
- ![img](https://img-blog.csdnimg.cn/img_convert/eae05d97522535d2a976353daae592c2.png)

## Prompt 工作流

清华：[OpenPrompt](https://github.com/thunlp/OpenPrompt)
- ![prompt](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Prompt 的工作流包含以下4部分：
1.  Prompt **模版**（Template）构造*   
2.  Prompt **答案空间映射**（Verbalizer）的构造    
3.  文本代入template，并且使用预训练语言模型进行**预测**    
4.  将预测结果**映射回**label。
 
具体的步骤如下图，拆解分析。
-  ![workflow](https://pic4.zhimg.com/80/v2-65b25d4895d4d7b81d747282cdb4c7f3_1440w.webp)
 
### Step 1: prompt construction【Template】

首先构建一个模版Template，作用是将输入和输出进行**重新构造**，变成一个新的带有mask slots的文本，具体如下：
*   定义一个模版，包含了2处代填入的slots：\[x\] 和 \[z\]
*   将\[x\] 用输入文本代入

例如：
*   输入：<span style='color:green'>x = 我喜欢这个电影</span>。
*   模版：<span style='color:blue'>\[x\]总而言之，它是一个\[z\]电影</span>。
*   代入（prompting）：<span style='color:green'>我喜欢这个电影。总而言之，它是一个\[z\]电影</span>。
- ![](https://pic2.zhimg.com/80/v2-e6c4edefdb2498229dfa37f9fc883f15_1440w.webp)
 
### Step 2: answer construction【Verbalizer】
 
对于构造的prompt，要知道预测词和label 之间的关系，并且不可能运行z(任意词)，就需要一个`映射函数`（mapping function）将输出的词与label进行映射。
- 输出的label 有两个，一个是 😄，一个是 😭，可以限定，如果预测词是`fantastic` 则对应 😄，如果是 `boring` 则对应 😭.
- ![](https://pic3.zhimg.com/80/v2-6c3ab4435a08d559c69d2b46b18a5d1e_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-4708199326266b548a6b3e0361b1bb47_1440w.webp)
 
### Step 3: answer prediction【Prediction】
 
只需要选择[合适的预训练语言模型](https://huggingface.co/docs/transformers/model_summary)，然后进行 mask slots \[z\] 的预测。例如下图，得到了结果 `fantastic`, 将其代入\[z\] 中。
- ![](https://pic2.zhimg.com/80/v2-c12486224648f205c3c8199101a06b75_1440w.webp)

### Step 4: answer-label mapping【Mapping】
 
第四步骤，对于得到的 `answer`，用 `Verbalizer` 将其映射回原本的label。
- 例如：fantastic 映射回 label：
- ![](https://pic2.zhimg.com/80/v2-b9a60cb83f1c6772490053801d13885d_1440w.webp)
 
### 总结

- ![](https://pic1.zhimg.com/80/v2-45666504e1714ef274be6ed35a86d388_1440w.webp)
 
## Prompt-based 方法的工程选择问题

在知乎中有个提问：
> 现代的deep learning 就是为了规避 feature engineering，可是prompt 这边选择了 template 和 answer 不还是 feature engineering吗？

确实, 如果使用 BERT 的 fine-tuning 范式（下图左），不需要使用任何的人工特征构造，而使用 prompt-based 的方法的话，需要人工参与的部分包含了以下部分：
*   template 构造
*   answer 构造
*   预训练模型选择
*   prompt 的组合问题选择
*   以及训练策略的选择等
- ![](https://pic3.zhimg.com/80/v2-011fccce5f1d7367c243def5d3da4e32_1440w.webp)
 
会先进行每个需要人工engineering 的部分进行详细讲解，然后再分析为什么我们还需要prompt 这种范式。
 
### Prompt Template Engineering（Prompt模版工程）
 
如何构造合适的Prompt 模版？对于同一个任务，不同的人可能构造不同的Template。
- ![](https://pic3.zhimg.com/80/v2-6b1eef9478acd01687934bcaa07a3d0a_1440w.webp)

且每个模版都具有合理性。Tempalte的选择，对于Prompt任务起到了很重大的作用，就算一个word的区别，也坑导致10几个点的效果差别，论文[GPT Understands, Too](https://arxiv.org/abs/2103.10385) 给出了如下的结果：
- ![](https://pic1.zhimg.com/80/v2-b98e6f18abfad252f96b04a549cc9898_1440w.webp)
 
对于不同的template，可以从以下两种角度进行区分：
1.  根据**slot 的形状/位置**区分
  *   1.1 `完形填空`（Cloze）的模式，即未知的slot在template的中间等不定的位置
  *   1.2 `前缀模式`（Prefix），未知的slot在template的开头
1.  根据是否**由人指定**的来区分
  *   2.1 `人工指定` template
  *   2.2 `自动搜索` template
  *   2.3 `Discrete` 离散Template，即搜索的空间是离散的，为预训练语言模型的字典里的字符。
  *   2.4 `Continuous` 连续Template，即搜索的空间是连续的，因为所有新增的这些prompt的参数主要是为了让机器更好地服务于任务，所以其参数的取值空间不需要限定在特定的取值范围内，可以是连续的空间。

具体的思维导图如下：
- ![](https://pic3.zhimg.com/80/v2-c96d89a06ca2ec58e31a7cd2b4f30e7e_1440w.webp)

#### P-Tuning 自动生成模板

[详解当前大火的提示学习prompt learning](https://zhuanlan.zhihu.com/p/594767132)

自动学习模板可以分为`离散`（Discrete Prompts）和`连续`（Continuous Prompts）两大类。
- `离散`主要包括 Prompt Mining, Prompt Paraphrasing, Gradient-based Search, Prompt Generation 和 Prompt Scoring；
- `连续`则主要包括Prefix Tuning, Tuning Initialized with Discrete Prompts 和 Hard-Soft Prompt Hybrid Tuning

作者：[何枝](https://www.zhihu.com/question/509079916/answer/2757278140)

人工构建模板对人类来讲是合理的，但是在机器眼中，prompt pattern 长成什么样真的关键吗？
- 机器对自然语言的理解和人类对理解大不相同，曾经有做一个 model attention 和人类对语言重要性的理解的[对比实验](https://www.zhihu.com/zvideo/1569770114018127872)，发现机器对语言的理解和人类是存在一定的偏差的。

那是不是不用特意为模型去设定一堆我们觉得「合理」的 prompt pattern，而是让模型自己去找「合理」的prompt pattern 呢？

p-tuning
- 论文：[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
- 代码：[p-tuning](https://github.com/HarderThenHarder/transformers_tasks/tree/main/prompt_tasks/p-tuning)

P-Tuning 的训练一共分为：prompt token(s) 生成、mask label 生成、mlm loss 计算 三个步骤。
- （1）prompt token(s) 生成
- （2）mask label 生成
- （3）mlm loss 计算

（1）prompt token(s) 生成

随便生成一个模板扔给模型
- 选用中文 BERT 作为 backbone 模型，选用 vocab.txt 中的 \[unused\] token 作为构成 prompt 模板的**元素**。
- \[unused\] 是 BERT 词表里预留出来的未使用的 token，本身没有什么含义，随意组合也不会产生很大的语义影响，这也是使用它来构建 prompt 模板的原因。

那么，构建出来的 prompt pattern 就长这样：

```sh
[unused1][unused2][unused3][unused4][unused5][unused6]
```

（2）mask label 生成

完成 prompt 模板构建后，还需要把 mask label 给加到句子中，好让模型完成标签预测任务。

设定 label 的长度为2（'水果'、'电脑'，都是 2 个字的长度），并将 label 塞到句子的开头位置：

```sh
[CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

其中 \[MASK\] token 就是需要模型预测的标签 token，现在把两个部分拼起来：

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

这就是最终输入给模型的样本。


（3）mlm loss 计算

开始进行模型微调，喂给模型这样的数据：

```
[unused1][unused2][unused3][unused4][unused5][unused6][CLS][MASK][MASK]这破笔记本速度太慢了，卡的不要不要的。[SEP]
```

并获得模型预测 \[MASK\] token 的预测结果，并计算和真实标签之间的 CrossEntropy Loss。

P-Tuning 中标签数据长这样：

```
水果    什么苹果啊，都没有苹果味，怪怪的味道，而且一点都不甜，超级难吃！
电脑    这破笔记本速度太慢了，卡的不要不要的。
...
```

也就是说，需要计算的是模型对 \[MASK\] token 的输出与「电脑」这两个标签 token 之间的 CrossEntropy Loss，以教会模型在这样的上下文中，被 \[MASK\] 住的标签应该被还原成「物品类别」。

实验

选用 63 条评论（8 个类别）的评论作为训练数据，在 417 条评论上作分类测试，模型 F1 能收敛在 76%。
- 通过实验结果我们可以看到，基于 prompt 的方式即使在训练样本数较小的情况下模型也能取得较为不错的效果。
- 相比于传统的分类方式，P-Tuning 能够更好的缓解模型在小样本数据下的过拟合，从而拥有更好的鲁棒性。


### Answer Engineering（答案工程）
 
在给定一个任务或者Prompt，如何对 label 空间 和 answer 空间进行映射？
- ![](https://pic2.zhimg.com/80/v2-3df9747e3a96385804fce424e5c7b619_1440w.webp)
 
在上图，label 空间 Y 是: Positive, Negative, 答案空间 Z 可以是表示positive或者negative 的词，例如: Interesting/Fantastic/Happy/Boring/1-Star/Bad，具体的答案空间 Z 的选择范围可以由我们指定。可以指定一个 y 对应1-N个字符/词。
- ![](https://pic4.zhimg.com/80/v2-acf52745920aae345e6c5ac9ee20e92b_1440w.webp)
 
具体的答案空间的选择可以有以下三个分类标注：
1.  根据**形状**
  *   1.1 Token 类型
  *   1.2 Span 类型
  *   1.3 Sentence 类型
1.  是否**有界**
  *   2.1 有界
  *   2.2 无界
1.  是否**人工选择**
  *   3.1 人工选择
  *   3.2 自动搜素
  *   3.2.1 离散空间
  *   3.2.2 连续空间

具体的思维导图如下：
- ![](https://pic4.zhimg.com/80/v2-9e58677317b0576537d58fa669c6fc5b_1440w.webp)
 
### Pre-trained Model Choice（预训练模型选择）

在定义完模版以及答案空间后，选择合适的预训练语言模型对 prompt 进行预测，如何选择一个合适的预训练语言模型需要人工经验判别的。

具体的预训练语言模型可以分为如下5类，具体参考：[Huggingface Summary of the models](https://huggingface.co/docs/transformers/model_summary)
*   autoregressive-models: `自回归模型`，主要代表有 `GPT`，主要用于生成任务
*   autoencoding-models: `自编码模型`，主要代表有 `BERT`，主要用于NLU任务
*   seq-to-seq-models：`序列到序列任务`，包含了an encoder 和 a decoder，主要代表有 `BART`，主要用于基于条件的生成任务，例如翻译，summary等
*   multimodal-models：`多模态模型`
*   retrieval-based-models：`基于召回的模型`，主要用于开放域问答

基于此，例如下图想要做summary 任务，我们可以选择更合适的 BART 模型。
- ![](https://pic3.zhimg.com/80/v2-f7cf71aaa125547394fc53faadc0e7e6_1440w.webp)
 
其他分类标准也可参考：
- ![](https://pic3.zhimg.com/80/v2-654bc36f0f684b17b97f1966fe5904aa_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-f8e5c095bf30cd97176098cd725974e1_1440w.webp)
 
### Expanding the Paradigm（范式拓展）

如何对已有的 Prompt 进行任务增强以及拓展，具体可以从以下几个方面进行探讨：
*   Prompt Ensemble：Prompt 集成，采用多种方式询问同一个问题
  - ![](https://pic2.zhimg.com/80/v2-0143f03d078e4c78231b987dd041752d_1440w.webp)
*   Prompt Augmentation：Prompt 增强，采用类似的 prompt 提示进行增强
  - ![](https://pic1.zhimg.com/80/v2-04b7abb8bac6d642808f95c3eaeef088_1440w.webp)
*   Prompt Composition：Prompt 组合，例如将一个任务，拆成多个任务的组合，比如判别两个实体之间是否是父子关系，首先对于每个实体，先用Prompt 判别是人物，再进行实体关系的预测。
  - ![](https://pic3.zhimg.com/80/v2-f3fd7d887bcec01d18e410efe74c36aa_1440w.webp)
*   Prompt Decomposition：  
  - 将一个prompt 拆分成多个prompt
  - ![](https://pic1.zhimg.com/80/v2-a13127a5cd6927bcbff1f278b6c32c28_1440w.webp)
 
具体的思维导图如下：
- ![](https://pic1.zhimg.com/80/v2-62bb4b0d499ab4ba4eb57fc27304ec00_1440w.webp)
 
## Prompt-based Training Strategies（训练策略选择）

Prompt-based 模型在训练中，有多种训练策略，可以选择哪些模型部分训练，哪些不训练。
 
可以根据训练数据的多少分为：
*   `Zero-shot`: 对于下游任务，没有任何训练数据
*   `Few-shot`: 对于下游任务只有很少的训练数据，例如100条
*   `Full-data`: 有很多的训练数据，例如1万多条数据
 
也可以根据不同的参数更新的部分，对于prompt-based 的模型，主要分为两大块  
- 一个是预训练模型，一个是 Prompts 参数。  

这两个部分，都可以独立选择参数训练选择。  

对于
*   预训练语言模型，可以选择精调，或者不训练
*   对于prompts：
*   可以是没有prompts
*   固定的离散字符 prompts。（无参数）
*   使用训练好的 prompts参数，不再训练。
*   继续训练 prompts参数
- ![](https://pic3.zhimg.com/80/v2-edcf6508177774d990a83a9867bc96de_1440w.webp)
 
这些训练策略均可以两两组合，下面举例说明：
 
### 策略分类

*   **Promptless** Fine-tuning
  - 如果只有预训练语言模型，没有prompts，然后fine-tuning，即是bert 的常规使用。
  - ![](https://pic3.zhimg.com/80/v2-eb301f6533d25e15a4b980581d4c736e_1440w.webp)
*   **Fixed-Prompt** Tuning  
  - 如果使用精调预训练语言模型+离散的固定prompts，就是 BERT + Discrete Prompt for text classification
  - ![](https://pic1.zhimg.com/80/v2-f9deac3450d2f4af12cb96fc42784fc8_1440w.webp)
  - 如果使用精调预训练语言模型+连续训练好的固定prompts，就是 BERT + Transferred Continuous Prompt for text classification
-  ![](https://pic1.zhimg.com/80/v2-ae562fd4a0b05f4a3d87247f64b68e88_1440w.webp)
*   **Prompt+LM** Fine-tuning
  - 如果使用精调预训练语言模型+可训练的prompts，就是 BERT + Continuous Prompt for text classification
  - ![](https://pic3.zhimg.com/80/v2-0e47867fb32772bbe94974f4b2a37ff6_1440w.webp)
*   **Adapter** Tuning
  - 如果使用固定预训练语言模型无prompt，只是插入task-specific模块到预训练语言模型中，就是BERT + Adapter for text classification
  - ![](https://pic2.zhimg.com/80/v2-bf82c965a4d2abf020356ff70401ee85_1440w.webp)
*   **Tuning-free** Prompting
  - 如果使用固定预训练语言模型和离散固定的prompt，就是GPT3 + Discrete Prompts for Machine Translation
  - ![](https://pic4.zhimg.com/80/v2-c72a6a803262e3ed3e4ec442a4382ebf_1440w.webp)
  - 如果使用固定预训练语言模型和连续固定的prompt，就是 GPT3 + Continuous Prompts for Machine Translation
  - ![](https://pic1.zhimg.com/80/v2-5bfc03ab8699075d28f0f64ca60142cc_1440w.webp)
*   **Fixed-LM** Prompt Tuning
  - 如果使用固定预训练语言模型和可训练的prompt，就是 BART + Continuous Prompts for Machine Translation
  - ![](https://pic2.zhimg.com/80/v2-d19c5a6e03987aea08bfd7896fc24c51_1440w.webp)
 
### 策略选择
 
对于不同的策略，需要进行不同的选择，我们往往需要考虑以下两点：
*   我们的数据量级是多少
*   我们的是否有个超大的 Left-to-right 的语言模型  

通常如果只有很少的数据的时候，希望不要去 fine-tune 预训练语言模型，而是使用LM的超强能力，只是去调prompt 参数。而让数据量足够多的时候，我们可以精调语言模型。

而只有像GPT-3 这种超大的语言模型的时候，我们才能直接使用，不需要任何的fine-tuning.
- ![](https://pic4.zhimg.com/80/v2-6a96f5cf3c0524bce4f9328746c6bff7_1440w.webp)
 
## Prompt 的优势是什么
 
Prompt Learning 的优势有哪些呢？从四个角度进行分析。
*   Level 1. Prompt Learning 角度
*   Level 2. Prompt Learning 和 Fine-tuning 的区别
*   Level 3. 现代 NLP 历史
*   Level 4. 超越NLP
- ![](https://pic2.zhimg.com/80/v2-b9363e52298acdc1997182b72b16bfe9_1440w.webp)
 
### Level 1. Prompt Learning 使得所有的NLP任务成为一个语言模型的问题

*   Prompt Learning 可以将所有的任务归一化预训练语言模型的任务
*   避免了预训练和fine-tuning 之间的gap，几乎所有 NLP 任务都可以直接使用，不需要训练数据。
*   在少样本的数据集上，能取得超过fine-tuning的效果。
*   使得所有的任务在方法上变得一致
- ![](https://pic2.zhimg.com/80/v2-e344a7b5f4364cb2ab6aa3e38681ebbd_1440w.webp)
 
### Level 2. Prompt Learning 和 Fine-tuning 的范式区别
 
*   Fine-tuning 是使得预训练语言模型适配下游任务
*   Prompting 是将下游任务进行任务重定义，使得其利用预训练语言模型的能力，即适配语言模型
- ![](https://pic3.zhimg.com/80/v2-e89d427849c1270e7caffc4990d5d5e6_1440w.webp)
 
### Level 3. 现代 NLP 第四范式
 
Prompting 方法是现在NLP的第四范式。其中现在NLP的发展史包含
1.  Feature Engineering：即使用文本特征，例如词性，长度等，在使用机器学习的方法进行模型训练。（无预训练语言模型）
2.  Architecture Engineering：在W2V基础上，利用深度模型，加上固定的embedding。（有固定预训练embedding，但与下游任务无直接关系）
3.  Objective Engineering：在bert 的基础上，使用动态的embedding，在加上fine-tuning。（有预训练语言模型，但与下游任务有gap）
4.  Prompt Engineering：直接利用与训练语言模型辅以特定的prompt。（有预训练语言模型，但与下游任务无gap）

在四个范式中，预训练语言模型，和下游任务之间的距离，变得越来越近，直到最后Prompt Learning是直接完全利用LM的能力。
- ![](https://pic4.zhimg.com/80/v2-041acfa2e38bb9c616e9ba1835f17b1f_1440w.webp)
 
### Level 4. 超越NLP的角度
 
Prompt 可以作为连接多模态的一个契机，例如 CLIP 模型，连接了文本和图片。相信在未来，可以连接声音和视频，这是一个广大的待探索的领域。
- ![](https://pic3.zhimg.com/80/v2-9fa54eb5b1b4b5ebc88843a6647c6c42_1440w.webp)

## 提升效果

提升 prompting 效果的方法
- prompt ensembling
  - 把多个prompt通过某种加权方法组合到一起
- prompt augmentation
  - 启发式学习
- prompt composition
  - 将复合的prompt句子，拆解成多个小段prompt，最后再组合在一起训练
- prompt decomposition
  - 由于一些任务的mask工作使用句子数量有限（比如词性标注任务），于是就只能通过decomposition将一个句子拆分成多个部分后，再对每个部分做prompt单独训练


### 最新论文

【2023-3-22】Context-faithful Prompting for Large Language Models


## prompt 应用

prompt的应用
- 知识探索（事实探索和语言学探索）
- 分类任务（文本分类和自然语言推理）
- 信息提取（关系提取、语义分析和命名实体识别）
- NLP 中的推理（常识推理和数学推理）
- 问答
- 文本生成
- 文本生成的自动评估
- 多模态学习
- 元应用（域自适应、除偏和数据集创建）

![img](https://pic3.zhimg.com/80/v2-934ee6d5289bcf81b6a32f3e85c88712_1440w.webp)


## prompt 实践

### 清华 OpenPrompt

- 【2023-2-16】清华开源的 [OpenPrompt](https://github.com/thunlp/OpenPrompt)：An Open-Source Framework for Prompt-learning. 包含prompt各个算法、模块的代码及示例
- ![](https://camo.githubusercontent.com/d26f498b1bc63fe804fe21df9d1f91fa55673ee3c45242fd3f49439718376db7/68747470733a2f2f7a332e617831782e636f6d2f323032312f31312f30332f4941645433442e706e67)

Define a task

```py
from openprompt.data_utils import InputExample
classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive
    "negative",
    "positive"
]
dataset = [ # For simplicity, there's only two examples
    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.
    InputExample(
        guid = 0,
        text_a = "Albert Einstein was one of the greatest intellects of his time.",
    ),
    InputExample(
        guid = 1,
        text_a = "The film was badly made.",
    ),
]
```

Define a Pre-trained Language Models (PLMs) as backbone.

```py
from openprompt.plms import load_plm
plm, tokenizer, model_config, WrapperClass = load_plm("bert", "bert-base-cased")
```

定义模板

```py
from openprompt.prompts import ManualTemplate
promptTemplate = ManualTemplate(
    text = '{"placeholder":"text_a"} It was {"mask"}',
    tokenizer = tokenizer,
)
```

Define a Verbalizer

```py
from openprompt.prompts import ManualVerbalizer
promptVerbalizer = ManualVerbalizer(
    classes = classes,
    label_words = {
        "negative": ["bad"],
        "positive": ["good", "wonderful", "great"],
    },
    tokenizer = tokenizer,
)
```

Combine them into a PromptModel

```py
from openprompt import PromptForClassification
promptModel = PromptForClassification(
    template = promptTemplate,
    plm = plm,
    verbalizer = promptVerbalizer,
)
```

Define a DataLoader

```py
from openprompt import PromptDataLoader
data_loader = PromptDataLoader(
    dataset = dataset,
    tokenizer = tokenizer,
    template = promptTemplate,
    tokenizer_wrapper_class=WrapperClass,
)
```

Train and inference

```py
import torch

# making zero-shot inference using pretrained MLM with prompt
promptModel.eval()
with torch.no_grad():
    for batch in data_loader:
        logits = promptModel(batch)
        preds = torch.argmax(logits, dim = -1)
        print(classes[preds])
# predictions would be 1, 0 for classes 'positive', 'negative'
```


等等组件

# ICL

ICL（上下文学习）

【2023-2-20】
- 斯坦福原文：[Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious](https://ai.stanford.edu/blog/in-context-learning/)
- [一文理解“上下文学习”----大语言模型突现能力](https://zhuanlan.zhihu.com/p/606788655)

GPT-3 模型展现了一些大模型才具备的**突现能力**（模型规模必须得增大到一定程度才会显现的能力，比如至少百亿级），其中一项能力就是`上下文学习`（In-Context Learning）（或：`情境学习`）
- `突现能力`：预训练好的大语言模型，迁移到新任务上的时候，只需要给模型输入几个示例（示例输入和示例输出对），模型就能为新输入生成正确输出而不需要对模型做 fine-tuning。

## 什么是ICL（In-Context Learning）？

`情境学习`（In-context Learning）提供几个任务示例/说明，要求预训练模型要对任务本身进行理解。

GPT-n 系列模型都属于**自回归**类的语言模型
- `自回归模型`: 根据当前输入预测下一个词，然后将预测结果和输入拼接再当做模型的输入再预测下一个词，这样循环往复。

自回归模型的训练目标: 从超大规模语料库中采样训练样本，模型根据输入输出一个概率向量（概率向量包含所有词的预测概率，对于GPT-3 模型来说，维度约1千多万），而因为文本数据自带标注所以我们是知道真实的下一个词，所以损失函数就采用得`交叉熵`。

预训练好的 GPT-3 模型拥有一项神奇的能力，后来被称为：`上下文学习`（In-Context Learning）。
- 预训练好的 GPT-3 模型在迁移到新任务时并不需要重新训练，而只需要提供**任务描述**（可选项）接着提供几个**示例**（任务查询和对应答案，以一对对的形式组织），最后加上要模型回答的查询。将以上内容打包一起作为模型的输入，则模型就能正确输出最后一个查询对应的答案。

用 GPT-3 来做个翻译任务，翻译英文为法文。输入的格式如下:
- ![icl](https://pic3.zhimg.com/80/v2-2c3994fe518c7f154b7541a91fb49a7e_1440w.webp)
- 第一行是对任务描述，告诉模型要做翻译
- 接下来三行就是示例，英文单词和对应的法文单词对
- 最后一行就是待翻译的英文单词。

将以上内容整体作为 GPT-3 的输入，让模型去补全输出就能得到 cheese 对应的法文单词。

上下文学习非常灵活，除了上面展示的翻译任务，还可以做语法修饰甚至写代码。
- 而神奇的地方就在于，GPT-3 训练过程中是并没有显式提供，类似测试阶段任务描述加示例这样的训练数据。

当然 GPT-3 的训练数据量非常巨大（包含了 wiki, 书本期刊，reddit 上的讨论等等），或许里面就已经就包含了各种任务类似结构的数据，GPT-3 模型容量足够大能够将所有训练数据都记了下来。

对于上下文学习能力的成因，目前还是一个开放性问题。为什么只有大规模的语言模型才会具备该能力？或许只有模型参数量大还不够，还必须要训练数据量也足够大，模型才能显现出该能力？


### ICL 发展历史

演变历程
- 2021年初，Prompt learning `提示学习`
- 2021年底，Demonstration learning `演示学习`
- 2022年初，In-cotnext learning `情境学习`

<div class="mermaid">
    flowchart TD
    %% 节点颜色
    classDef red fill:#f02;
    classDef green fill:#5CF77B;
    classDef blue fill:#6BE0F7;
    classDef orange fill:#F7CF6B;
    classDef grass fill:#C8D64B;
    %%节点关系定义
    O(两阶段范式):::orange-->|2021年初,提示模板|A(提示学习):::blue
    A-->|2021年底,演示案例|B(演示学习):::grass
    B-->|2022年初,上下文信息|C(情境学习):::grass
</div>


### ICL 资料

- 【2023-3-22】princeton university 普林斯顿大学的ppt讲义：[Towards Understanding In-context Learning ](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec07.pdf)
- 【2022-8-1】斯坦福AI Lab文章：[How does in-context learning work? A framework for understanding the differences from traditional supervised learning](https://ai.stanford.edu/blog/understanding-incontext/)
  - `ICL`是LLM的一项神奇的**涌现能力**，无需优化参数，只需提供输入输出示例。斯坦福提供一个贝叶斯推理框架，可以更好的理解ICL，定位预训练数据中学到的隐含概念。prompt提示的所有组成部分（输入/输出/格式化/映射关系）都能为推理隐含概念提供信息
  - This suggests that all components of the prompt (inputs, outputs, formatting, and the input-output mapping) can provide information for inferring the latent concept

Research Goals
- Develop a mathematical framework for understanding how in-context learning emerges during pre-training
  - 2022年论文: [An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080)
- Analyze empirically which aspects of the prompt affect downstream task performance
  - 2022年论文: [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work]()

GPT-3 paper found that the **large scale** leads to a particularly interesting **emergent behavior** 涌现能力  called in-context learning 上下文学习.

An Explanation of In-context Learning as Implicit Bayesian Inference
- ![](https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image13.gif)
- Two examples of in-context learning, where a language model (LM) is given a list of training examples (black) and a test input (green) and asked to make a prediction (orange) by predicting the next tokens/words to fill in the blank.
- 黑色是训练样本，绿色是测试输入，目标是预测空白处的单词（negative/positive，finance/sport...）

Concepts (long-term coherence 长程连贯性)
- A **latent variable** 隐变量 that contains various **document-level** statistics: a distribution of **words**, a **format**, a **relation** between sentences, and other semantic and syntactic **relations** in general.
- For example, a “news topics” 新闻主题 concept 概念 describes a distribution of words 单词 (news and their topics), a format 格式 (the way that news articles are written), a relation between news and topics 新闻和主题关系, and other semantic and syntactic relationships between words 单词之间的语义语法关系. In general, concepts may be a combination of many latent variables 概念是许多隐变量的组合 that specify different aspects of the semantics and syntax of a document, but we simplify here by grouping them all into one concept variable.
- ![](https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image11.gif)
- the LM uses the training examples to internally figure out that the task is either sentiment analysis (left) or topic classification (right) and apply the same mapping to the test input.

Bayesian inference view of in-context learning

In our framework, the document-level latent concept 文档级别隐概念 creates long-term coherence, and modeling this coherence 连贯性 during pretraining requires learning to infer the latent concept:
- `Pretrain`: To predict the next token during pretraining, the LM must infer (“locate”) the latent concept for the document using evidence from the previous sentences.
  - assumption: a document is generated by first sampling a latent concept, and then the document is generated by conditioning on the latent concept.
  - 假设：采样隐含概念后，据此生成文档
- `In-context learning`: If the LM also infers the **prompt concept** (the latent concept shared by examples in the prompt) using in-context examples in the prompt, then in-context learning occurs!
  - In-context learning prompts are lists of `IID` (independent and identically distributed) training examples. Each example in the prompt is drawn as a sequence conditioned on the same prompt concept, which describes the task to be learned.
  - ICL 示例 是独立同分布的训练样本，附带一个测试样本，共同组成prompt concept提示概念，描述要学习的任务
  - The process of “locating” learned capabilities can be viewed as Bayesian inference of a prompt concept that every example in the prompt shares. If the model can infer the prompt concept, then it can be used to make the correct prediction on the test example. Mathematically, the prompt provides evidence for the model (p) to sharpen the posterior distribution over concepts, $p(concept\∣prompt)$. If $p(concept\∣prompt)$ is concentrated on the prompt concept, the model has effectively “learned” the concept from the prompt.
  - ![](https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image8.png)
  - `concept` --> `prompt` --> `output`

Prompts provide noisy evidence for Bayesian inference
- LM will infer the prompt concept from in-context examples, even though prompts are sampled from the prompt distribution, which can be very different from the pretraining distribution. Interestingly, the LM can still do **Bayesian inference** despite the mismatch between the pretraining and prompt distributions, as seen empirically in GPT-3. In-context learning via Bayesian inference can emerge from latent concept structure in the pretraining data in a simplified theoretical setting and use this to generate a synthetic dataset where in-context learning emerges for both Transformers and LSTMs.
- 语言模型从上文示例从推断提示概念，即使提示样本不同于训练数据（包含噪声）。紧固那预训练分布和提示分布不同，语言模型（如GPT-3）依然可以执行贝叶斯推断
- ![](https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image6.gif)
- 绿色概念表示训练样本中隐含提示概念之间的信号，红色箭头表示示例间低概率转移的噪声。
- 训练样本提供信号：训练样本提供贝叶斯推理的信号，样本内单词转移有助于LM学习共享的隐含概念。这种转移来自输入分布、输出分布、输出格式以及输入-输出映射。
- 训练样本之间的转移可以是低概率噪声：训练样本独立同分布，放在一起后，成了非自然、低概率的样本转移，产生推理噪声
- ICL对噪声鲁棒：如果提示里包含的信号超过噪声，语言模型能成功执行ICL任务：随着训练样本数趋近无穷，预测准确率不断上升。
  - 信号由提示里的概念和其他概念之间的KL散度来衡量。
  - 噪声源自示例之间的转移概率
- 小规模ICL数据集 GINC dataset：We found that pretraining on GINC causes in-context learning to emerge for both Transformers and LSTMs, the main effect comes from the structure in the pretraining data.

### ICL 介绍

In Context Learning（ICL）的核心思想：<span style='color:blue'>从类比中学习</span>。
- 综述: 【2023-2-8】[A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234.pdf)
- ![](https://cdnimg.redian.news/mmbiz_png/VBcD02jFhgktluYFHRVjOLXEEuQEIUiaQhWrL0xDIF0LaIZYAuh3sqSEGhw3iadFuGmrEH2UsiaP0Qvoo1JAz3FOg/640?wx_fmt=png)

一个描述语言模型如何使用ICL进行决策的例子。
- ![img](https://pic2.zhimg.com/80/v2-16eadb0b8f3e1c03bd5a642b83a61279_1440w.webp)
- 首先，ICL需要一些示例, 形成一个`演示上下文`。这些示例通常是用自然语言模板编写的。
- 然后，ICL将查询的`问题`（即input）和一个`上下文演示`（一些相关的cases）连接在一起，形成带有提示的输入，并将其输入到语言模型中进行预测。

注意：
- 与训练阶段用反向梯度更新模型参数的监督学习不同，`ICL`<span style='color:red'>不需要参数更新</span>，并直接对预先训练好的语言模型进行预测
  - 这是`ICL`与`prompt`，传统`demonstration learning`不同的地方，ICL不需要在下游 P-tuning 或 Fine-tuning。
- 希望该模型学习隐藏在演示中的模式，并据此做出正确的预测。

Works like magic!
- No parameter tuning need
- Only need few examples for downstream tasks
- GPT-3 improved SOTA on LAMBADA by 18%!

We don’t know how models in-context learn
- Learns to do a downstream task by conditioning on **input-output** examples
- No weight update and model is not explicitly pre-trained to learn from examples

Model needs to figure out：
- input distribution (financial or general news)
- output distribution (Positive/Negative or topic)
- input-output mapping (sentiment or topic classification)
- formatting


结论1：
- ICL中 Ground Truth 信息无关紧要
- ICL的性能收益主要来自独立规范的 `输入空间` 和 `标签空间` ，以及正确一致的演示格式

模型是否在Test阶段学习到了知识？
- 如果对学习进行严格的定义，即学习在训练数据中给出的输入**标签对**，那么，lm在测试时不学习新的任务。
- 然而，学习一项新任务更广泛地解释：可能包括适应特定的输入和标签分布以及演示的格式，并最终更准确地做出预测。
- 有了定义，该模型确实可以从演示中学习任务。实验表明，该模型确实利用了演示的各个方面，并实现了性能的提高。

演示是如何让 In-context learning 在不同的任务中产生性能增益的，而且随着 fine-tune 阶段的**黑盒化**，很多文章也提出 fine-tune 阶段可能让模型丧失了泛化性，那么ICL这种不fine tune的方法既节省时间与资源开销，且能提升效果，应该会在大模型林立的时代被人关注，并迅速火起来。

## 上下文学习原理

### ICL 原理

[How does in-context learning work?](http://ai.stanford.edu/blog/understanding-incontext/)
- [In-context learning综述](https://arxiv.org/pdf/2301.00234.pdf)
- [github](https://github.com/dtsip/in-context-learning)
- [In-Context Paperlist](https://github.com/dongguanting/In-Context-Learning_PaperList)

大型预训练语言模型其中一个重要特点：`上下文学习`（In-Context Learning，`ICL`）能力，即通过一些示范性的 **\<输入-标签>** 对，就可以在不更新参数的情况下对新输入的标签进行预测。

性能虽然上去了，但大模型的`ICL`能力到底从何而来仍然是一个开放的问题。为了更好地理解ICL的工作原理，清华大学、北京大学和微软的研究人员共同发表了一篇论文，将语言模型解释为`元优化器`（meta-optimizer），并将ICL理解为一种隐性的（implicit）微调。

情境学习三种分类的定义和示例如下：
- `few-shot learning` **多个示例**
  - 输入：“这个任务要求将中文翻译为英文。你好->hello，再见->goodbye，购买->purchase，销售->”
  - 要求模型预测下一个输出应该是什么，正确答案应为“sell”。
- `one-shot learning` **一个示例**
  - 输入：“这个任务要求将中文翻译为英文。你好->hello，销售->”
  - 要求模型预测下一个输出应该是什么，正确答案应为“sell”。
- `zero-shot learning`
  - 输入：“这个任务要求将中文翻译为英文。销售->”
  - 要求模型预测下一个输出应该是什么，正确答案应为“sell”。

### ICL 与 GPT

GPT-3 中Few-shot Learning没有 fine-tune，直接当做 GPT model 的输入，没有调整模型
- [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- GPT首先根据示范实例产生元梯度，然后将这些元梯度应用于原始的GPT，建立ICL模型。
- ICL在预测层面、表征层面和注意行为层面的表现与显式微调类似。
此外，受到元优化理解的启发，通过与基于动量的梯度下降算法的类比，文中还设计了一个基于动量的注意力，比普通的注意力有更好的表现，从另一个方面再次支持了该理解的正确性，也展现了利用该理解对模型做进一步设计的潜力。

【2023-2-8】[In-Context Learning（上下文学习）相关分享](https://zhuanlan.zhihu.com/p/603650082)

## 示例讲解

### 拷贝输出

一个很简单的任务，让模型直接复制输入的内容。

首先示例个数设置为 5个，每个示例输入包含 5 个不同的小写单词（从字母表前 8 个小写字母中随机选5个得到），这些单词用逗号分隔，输出直接拷贝的输入，比如：

```s
Input: g, c, b, h, d
Output: g, c, b, h, d
Input: b, g, d, h, a
Output: b, g, d, h, a
Input: f, c, d, e, h
Output: f, c, d, e, h
Input: c, f, g, h, d
Output: c, f, g, h, d
Input: e, f, b, g, d
Output: e, f, b, g, d
Input: a, b, c, d, e
Output:
```

期待模型的输出是：
- a, b, c, d, e

接着对于5个字母顺序的**所有**可能情况 (8!/3!=6720，从8个样本中选5个总的组合数)也就是最后 input 的位置将 6720 个情况都测试了
- GPT-3 模型的准确率是 100%。
- GPT-3 系列最小的模型 text-ada-001 来做这个任务，获得了 6705/6720 = 99.78% 的准确率

一定程度上证明了<span style='color:red'>模型规模的重要性</span>。

### 格式化日期

复杂一些的任务。
- 对日期做格式化，将 **年-月-日** 格式的输入格式转化成 **!月!日!年!**，其中年份四位数，月份和日子是两位数，比如：
- ![](https://pic4.zhimg.com/80/v2-ba83f805fcd0ea4c5b98c768f9e1e807_1440w.webp)
- 源自：斯坦福[in-context-learning](https://ai.stanford.edu/blog/in-context-learning/)

示例个数是3，最后是待测试的日期：2005-07-23。

为什么选择日期格式化这个任务呢？
- 足够简单，日期包含三个随机变量（年月日），长度固定，而且设定的输出格式也不是正常格式，所以训练数据中不太可能包含类似的样本，也排除了模型可能只是将训练数据都**记忆**了下来。

接下来看看测试结果
- GPT-3 全系列的模型，包括 text-ada-001,text-babbage-001,text-curie-001 和 text-davinci-003，模型参数量依次从小到大排列。并通过设置不同的**上下文示例个数**（对于每个示例个数的设置，都有2000个测试样本），记录各个模型的预测准确率，测试结果如下：
- ![](https://pic2.zhimg.com/80/v2-fe5d963c6fba070092190a8e9b8ee2b9_1440w.webp)

分析：
- 固定横坐标示例个数，则模型越大准确率也越高，**模型越大**准确率曲线也就更加的陡峭。
- 而对于每个模型来说，**增加示例个数**也能有效提升准确率。
- 不过，即使增大示例个数和模型，模型的精确度也只是无限接近 100% 但还是达不到。

GPT-3 预测错误的样本都包含哪些类型。
- 随着上下文示例个数的增加，预测错误的样本个数也在下降。
- 2019 年份的输入，模型是最容易预测错误的，这也能理解因为训练数据中 2019 年份的数据不多

### 标签重映射

任务描述
- 将实体做一个不正常的重新分类

```s
volleyball: animal
onions: sport
broccoli: sport
hockey: animal
kale: sport
beet: sport
golf: animal
horse: plant/vegetable
corn: sport
...
```

输入示例中包含了 \[animal（动物）, plant/vegetable（植物/蔬菜）, sport（运动）\] 三种类型标签。

现在将原来的标签映射打乱，将**动物**映射为**植物**（duck: plant/vegetable），将**运动**映射为**动物**（golf: animal），将**植物**映射为**运动**（beans: sport）。

测试 GPT-3 能否根据仅有的示例学会预测新的映射：GPT-3 能正确输出映射关系。

## ICL能力成因

为什么 LLM 能够具备该能力？上下文学习的原理究竟是怎样的呢？

微软研究院发布的文章，对于上下文学习能力来源的探究。
- 论文：[Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we figure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit finetuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit finetuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more importantly, it shows the potential to utilize our understanding for future model designing.
- 关键在于 LLM 中的`注意力层`（attention layers），在推理过程实现了一个隐式参数优化过程，这和 fine-tuning 的时候通过梯度下降法显式优化参数的过程是类似的。

基于梯度下降法的优化过程和注意力层的联系
- 一个线性注意力层其实和基于梯度下降法优化的全连接层是互为**对偶**的形式

上下文学习怎么实现隐式 finetuning
- 略，详见：[一文理解“上下文学习”----大语言模型突现能力](https://zhuanlan.zhihu.com/p/606788655)



# 思维链（CoT：Chain-of-thoughts）


【2023-2-19】[思维链（Chain-of-thoughts）作为提示](https://zhuanlan.zhihu.com/p/493533589)
- “让我们一步步思考。”
- Let's think step by step

【2023-2-27】[Learn Prompting](https://learnprompting.org/)

- 【2023-3-6】CoT一作 Jason Wei的ppt [New abilities in big language models](https://docs.google.com/presentation/d/1JyvLrfvLOTfGBWrNl7Gk6Mqn6LIgM2NTeRM2d6oyBow/edit#slide=id.g110339e1e35_0_0)，two new abilities of scale 大模型的两项新增能力
- ① Language models follow instructions. **遵从指令**
  - Finetuned language models are zero-shot learners (ICLR 2022). {J. Wei, M. Bosma, V. Zhao, K. Guu}, A. Yu, B. Lester, N. Du, A. Dai, & Q. Le. 
- ② Language models do chain of thought reasoning. **思维链**
  - Chain of thought prompting elicits reasoning in large language models 
- Emergence and reasoning in large language models - Jason Wei (Google)，[ppt](https://drive.google.com/file/d/1j_CM1fwl_EKB63VlreNUnrKMQsbZHagg/view), [youtube](https://www.youtube.com/watch?v=0Z1ZwY2K2-M)

**Chain-of-thought** prompting elicits reasoning in large language models (Wei et al., 2022).
- ○ Self-consistency improves chain-of-thought reasoning in language models (Wang et al., 2022).
- ○ Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022).
- ○ Language models are multilingual chain-of-thought reasoners (Shi et al., 2022).
- ○ Challenging BIG-Bench tasks and whether chain-of-thought can solve them (Suzgun et al., 2022).

- 作者GitHub上的[chain-of-thought-zip](https://github.com/jasonwei20/chain-of-thought-prompting/blob/main/chain-of-thought-zip.zip)
- 【2023-3-18】思维链论文集锦：[Chain-of-ThoughtsPapers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)


## CoT 诞生之初

2021年，`提示学习`（prompt learning）浪潮兴起，以`离散式提示学习`（提示词的组合）为**起点**，`连续化提示学习`（冻结大模型权重+微调较小参数达到等价性能）为**复兴**，几乎是在年末达到了研究的一个巅峰。

2022年开始，逐渐有很多人意识到: `连续化提示学习`其中的一些好处伴随的一些**局限性**，比如**伪资源节约**，**不稳定**等等。很多研究者拒绝陪玩，虽认同提示学习将会带来下一代NLP界的革命，但是认为拒绝做他人大模型的附庸，开始探索大模型的训练技术，并且训练自己的大模型；而手头暂时没有掌握资源的研究者开始再次将研究重心从`连续化学习`转移到`离散式提示学习`上去，将研究聚焦于特定的`大模型GPT3`上。

此时，距离175B的GPT3模型被发布和上下文学习被发现过去了不到2年，热度经历了高潮与低谷，经历了深度学习流派关于`连接学派`和`符号学派`的辩论和是否具有**意识**和**推理能力**的讨论，一些基础的玩法在被开发之后就被搁置了一段时间直到提示学习的兴起。

2022年1月，OpenAI通过`强化学习`调试模型，使用强化学习调试更新了他们的模型到了第二代，LLM肉眼可见地变得更好提示，很多任务的性能也显著提升，尤其是一些之前没有办法很好进行的任务被显著地提高了起来。

思维链系列工作就是在这样一个大环境下产生的。

## CoT 介绍

2022年5月，谷歌年度开发者大会（Google I/O）上对CoT技术进行了宣传，还有谷歌的540B系数大模型PaLM和Pixel系列手机手表等等。
- ![](https://pic2.zhimg.com/80/v2-50bd83f418275a6db5afeb32c1be86a9_1440w.webp)

思维链的主要思想是通过向大语言模型展示一些少量的 exemplars，在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。

[思维链提示过程](https://learnprompting.org/zh-Hans/docs/intermediate/chain_of_thought) Wei等人
> - “思维链仅在使用**∼100B**参数的模型时才会产生性能提升”。

较小的模型编写了不合逻辑的思维链会导致精度比标准提示更差。

`思维链`(CoT)提示过程 是一种最近开发的提示方法，它鼓励大语言模型解释其推理过程
- ![](https://learnprompting.org/zh-Hans/assets/images/chain_of_thought_example-37c925a2720c9c4bb4c823d237bc72c8.png)
- 常规提示过程 vs 思维链提示过程

思维链已被证明对于算术、常识和符号推理等任务的结果有所改进。
- 特别是，在GSM8K2基准测试上，PaLM 540B3的提示达到了**57%**的解决率准确性。
- ![](https://learnprompting.org/zh-Hans/assets/images/prompted_palm-20fba06418ed8569b51f0dd376c03b41.png)

### CoT 提出

22年1月, 谷歌大脑研究员[Jason Wei](https://twitter.com/_jasonwei)放到arxiv上面的文章，提出了`思维链`这个概念。
- 论文：[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- 2023年2月15日，一作[Jason Wei](https://www.jasonwei.net/),[twitter](https://twitter.com/_jasonwei),[github](https://github.com/jasonwei20), [personal page](https://jasonwei20.github.io/personal/), 离开Google Brain，加入OpenAI
- 2020年，达特茅斯本科毕业的ABC，I graduated with an AB from Dartmouth College in 2020.

<img src="https://images.squarespace-cdn.com/content/v1/633a6c0dd119b45af0d898c2/1226b6e8-1232-4d1f-aeb2-7c4a61657610/taverna.jpeg?format=500w" width=300 heigth="100%">

> We explore how generating a **chain of thought** (`CoT`) -- a series of intermediate reasoning steps -- significantly improves the ability of **large language models** (LLM) to perform complex reasoning (复杂推理). 
>- In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called **chain of thought prompting**, where a few chain of thought demonstrations are provided as exemplars in prompting. 
>- Experiments on three large language models show that **chain of thought prompting** improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a **540B**-parameter language model with just **eight** chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.

`思维链`是一种`离散式提示学习`
- 大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习，即通过 x1,y1,x2,y2,....x_test 作为输入来让大模型补全输出 y_test，思维链多了中间的一些闲言碎语絮絮叨叨，以下面这张图为例子
- ![CoT](https://pic3.zhimg.com/80/v2-e1b5adff46170f633e8ed635e7a57646_1440w.webp)

例子选择自一个数据集叫`GSM8K`，每一个样例大概就是一个小学一二年级的看几句话（基本都是三句）写算式然后算答案的难度，但是`GPT-3`通过刚刚说的最简单的提示方法曾经只能在这个数据集上做到6%左右的准确度。
- 由此可见，直接预测y不太可行。

思维链的絮絮叨叨，即不直接预测y，而是将y的“思维过程”r（学术上统称为relationale）也要预测出来。当然最后不需要这些“思维过程”，这些只是用来**提示**，获得更好的答案，只选择最后的答案即可。作者对不同的数据集的原本用于上下文学习的提示标注了这些思维链然后跑了实验，发现这么做能够**显著**提升性能（左图），且这种性能的提升是具有类似于**井喷性质**（右图）的（后来称这种性质叫`涌现性`）。
- ![](https://pic1.zhimg.com/80/v2-a5e20815867c458a74b5e30a13f3b53c_1440w.webp)

## CoT 原理


### 如何生成CoT提示？

【2023-3-24】[Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)
- 思维链有两种范例
- ① 用 “让我们一步步思考” 来辅助LLM回答问题时逐步思考
- ② 一系列人工示例，每个示例由 $< 问题, 推理链 >$ 组成

Providing intermediate reasoning steps for prompting demonstrations is called chain-of-thought (`CoT`) prompting. CoT prompting has two major paradigms. 
- One leverages a simple prompt like "<span style='color:blue'>Let's think step by step</span>" to facilitate step-by-step thinking before answering a question. 
- The other uses a few **manual** demonstrations one by one, each composed of a **question** and a **reasoning chain** that leads to an answer. 

The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with **mistakes**. 

To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an **automatic** CoT prompting method: `Auto-CoT`. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with `GPT-3`, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. 



## CoT 改进

- (1) 多数投票显著提高CoT性能
  - 2023年，[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)，[Jason Wei](https://www.jasonwei.net/) 二作
- (2) 提出了一种boost方法，让**中小模型**也可以通过训练具有思维链能力
  - STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning
- (3) Zero-Shot 推理
  - Large Language Models are Zero-Shot Reasoners
  - “Let's do it step by step“
- (4) 其它文章
  - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models
  - On the Advance of Making Language Models Better Reasoners
  - Rationale-Augmented Ensembles in Language Models

### 多数投票显著提高CoT性能

2022年3月在arxiv上放出来
- 论文：[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
- 改进思维链解码效率：新的解码策略（自一致性）替换原生的贪心解码

>**Chain-of-thought prompting** combined with pre-trained large language models has achieved encouraging results on **complex reasoning** tasks. In this paper, we propose a new **decoding strategy**, self-consistency, to replace the **naive greedy decoding** used in chain-of-thought prompting. 
>- It first samples a diverse set of **reasoning paths** instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. 
>- Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.
>- Our extensive empirical evaluation shows that **self-consistency** boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).

思维链初代文章很快的一个跟进工作，是思维链系列文章版图的重要一步。
- 这篇文章几乎用的和初代思维链文章完全一样的数据集和设置，主要改进是使用了对答案进行了**多数投票**（majority vote），并且发现其可以显著地提高思维链方法的性能。
- 将greedy search变成了sample+vote
- ![](https://pic1.zhimg.com/80/v2-82f1234d53f8c0623d783eccdff272e4_1440w.webp)
- 一开始不如cot（因为temperature），后面大幅度超过
- ![](https://pic3.zhimg.com/80/v2-bc59df588bc49b11c82a46dab77379de_1440w.webp)
- 将`贪婪搜索`（greedy search），即将GPT模型的 temperature 从0设置为某个数值，比如说0.4，然后sample多个按照y进行投票，会显著地提升性能

### 自洽性

`自洽性`（Self-consistency）是对 CoT 的补充，它生成**多个**思路链，然后取**多数答案**作为最终答案。

在下面的图中，左侧的提示是使用少样本思路链范例编写的。使用这个提示，独立生成多个思路链，从每个思路链中提取答案，通过“边缘化推理路径”来计算最终答案。实际上，这意味着取多数答案。
- ![](https://learnprompting.org/zh-Hans/assets/images/self_consistency-3db2492237f38cf1567b25e0c902e3f5.png)

结论
- 自洽性可以提高算术、常识和符号推理任务的结果。
- 即使普通的思路链提示被发现无效2，自洽性仍然能够改善结果。

### 知识生成

生成的知识方法（Generated Knowledge Approach）要求 LLM 在生成响应之前生成与问题相关的可能有用的信息。

该方法由两个中间步骤组成：知识生成 和 知识集成
- 知识生成: 要求 LLM 生成有关问题的一组事实。大语言模型将以 few-shot 方式进行提示，如下所示。使用相同提示生成 M 个不同的完成。
  - ![](https://learnprompting.org/zh-Hans/assets/images/gen_k_p1-13cb7c5d4bcd11edceda40a150b4f2cd.png)
- 知识集成:

问题: “大多数袋鼠有 \<mask\> 肢体”。假设在知识生成步骤中，生成了 2 个知识（M=2）：
>- 知识1：“袋鼠是生活在澳大利亚的有袋动物。”
>- 知识2：“袋鼠是有 5 条肢体的有袋动物。”

将每个知识与问题连接起来，生成知识增强的问题：
>- 知识增强问题1：“大多数袋鼠有 \<mask\> 肢体。袋鼠是生活在澳大利亚的有袋动物。”
>- 知识增强问题2：“大多数袋鼠有 \<mask\> 肢体。袋鼠是有 5 条肢体的有袋动物。”

然后，用这些知识增强的问题提示 LLM，并获得最终答案的提案：
>- 答案1：“4”
>- 答案2：“5”

选择概率最高的答案作为最终答案。最高概率可能是答案令牌的 softmax 概率，或答案令牌的对数概率。

### 最少到最多提示过程 Least to Most prompting

`最少到最多提示过程` (Least to Most prompting, LtM) 将 `思维链提示过程` (CoT prompting) 进一步发展
- 首先将问题分解为**子问题**，然后逐个解决。

受针对儿童的现实教育策略的启发, 发展出的一种技术。

与`思维链提示过程`类似，需要解决的问题被分解成一组建立在彼此之上的子问题。
- 在第二步中，这些子问题被逐个解决。
- 与思维链不同的是，先前子问题的解决方案被输入到提示中，以尝试解决下一个问题。
- ![LtM](https://learnprompting.org/zh-Hans/assets/images/least_to_most_formal-23db97bcd5ecbabe7d5db9b0d0645741.png)

[结论](https://learnprompting.org/zh-Hans/docs/intermediate/least_to_most): LtM 带来了多项提升：
- 相对于思维链提高了准确性
- 在难度高于提示的问题上提升了泛化能力
- 在组合泛化方面的性能得到了显著提高，特别是在SCAN基准测试3中
- 使用 text-davinci-002（论文模型）的标准提示解决了 6% 的 SCAN 问题，而 LtM 提示则取得了惊人的 76% 的成功率。在 code-davinci-002 中，结果更为显著，LtM 达到了 99.7% 的成功率。

### Zero-Shot 思维链

`零样本思维链`（Zero Shot Chain of Thought，Zero-shot-CoT）提示过程 是对 CoT prompting 的后续研究，它引入一种非常简单的零样本提示。
- 在问题结尾附加“<font style='color:blue'>让我们一步步思考</span>。” 这几个词，大语言模型能够生成一个回答问题的思维链。
- 从这个思维链中，他们能够提取更准确的答案。
- ![](https://learnprompting.org/zh-Hans/assets/images/zero_shot-1af9e1cb88412f9fdefa3b07b67c4193.png)

从技术上讲，完整的零样本思维链过程涉及两个单独的提示/补全结果。下图中，左侧顶部气泡生成一个思维链，而右侧顶部气泡接收来自第一个提示（包括第一个提示本身）的输出，并从思维链中提取答案。这个第二个提示是一个 自我增强 的提示。
- ![](https://learnprompting.org/zh-Hans/assets/images/zero_shot_example-89065990663d4ef044011844ff77f9af.png)

结论
- 零样本思维链也有效地改善了算术、常识和符号推理任务的结果。然而，通常不如思维链提示过程有效。在获取思维链提示的少量示例有困难的时候，零样本思维链可以派上用场。

CoT 使用 few shot 让 llm 说出来 CoT ，然后得到答案，分成两部分：
- 首先, 加一个“<font style='color:blue'>Lets think step by step</span>”，得到一个CoT的 murmur
- 然后, 后面接上一句“so the anwser is”，然后得到答案。

性能远超原来的zero shot逼近few shot和CoT的few shot性能。

## CoT 应用

应用
- [多项选择题](https://learnprompting.org/zh-Hans/docs/applied_prompting/mc_tutorial)
- [解答讨论性问题](https://learnprompting.org/zh-Hans/docs/applied_prompting/short_response): 通过正确的提示，GPT-3非常擅长写作短格式回答。
- [GPT-3构建ChatGPT](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatgpt)
- [聊天机器人+知识库](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatbot_from_kb)

【2023-3-19】MathPrompter: Mathematical Reasoning using Large Language Models，微软的这篇论文通过prompt设计提高了数学计算效果

说到底，人脑也就是参数量够了才有能顾及那么繁多细节的思考能力。其实不止是transformer，就算是最原始的nn用这个算力效果也不会太差

### 多项选择

[多项选择题](https://learnprompting.org/zh-Hans/docs/applied_prompting/mc_tutorial)
- LSAT（Law School Admission Test）是美国法学院用于评估潜在学生的批判性思维和分析推理能力的标准化考试
- 引入 CoT 技术，可以让 GPT-3 完成 LSAT 的多项选择题
- GPT与计算器等外部工具 [MRKL](https://learnprompting.org/docs/advanced_applications/mrkl)

#### ReAct

- ReAct 系统是具有推理和行动能力的MRKL系统
- ReAct(reason, act)是一种使用自然语言推理解决复杂任务的语言模型范例，旨在用于允许LLM执行某些操作的任务。MRKL系统中，LLM可以与外部API交互以检索信息。

顶部框中的问题来自HotPotQA，一个需要复杂推理的问答数据集。 
- ReAct能够首先通过推理`问题`（Thought 1），然后执行一个`动作`（Act 1）来向Google发送查询来回答问题。
- 然后收到了一个`观察`（Obs 1），并继续进行这个`思想`，`行动`，`观察`循环，直到达到结论（Act 3）
- ![](https://learnprompting.org/zh-Hans/assets/images/react_qa-cfd65cdd3036e7d29f8331eb90dd939a.png)

具有强化学习知识的读者可能会认为，这个过程类似于经典的RL循环：状态，行动，奖励，状态，...

谷歌在 ReAct的实验中使用了`PaLM` LLM。与标准提示（仅问题）、CoT和其他配置进行比较表明，ReAct在复杂推理任务方面的表现是有希望的。

#### PAL

`程序辅助语言模型`（Program-aided Language Models, `PAL`） 是另一个MRKL系统的例子。[代码推理](https://learnprompting.org/zh-Hans/docs/advanced_applications/pal)
- 给定一个问题，PAL能够编写代码解决这个问题。
- 它将代码发送到编程运行时以获得结果。

PAL的中间推理是`代码`，而CoT的是`自然语言`。

PAL实际上交织了`自然语言`（NL）和`代码`。
- 图中，蓝色的是PAL生成的自然语言推理。虽然图中没有显示，PAL实际上在每行自然语言推理前生成'#'，以便编程运行时将其解释为注释。
- ![](https://learnprompting.org/zh-Hans/assets/images/pal-102def844edd97ac78e9c26cbc2564f9.png)

PAL解决数学问题的例子
- langchain，一个用于链接LLM功能的Python包。

```py
#!pip install langchain==0.0.26
#!pip install openai
from langchain.llms import OpenAI
import os

os.environ["OPENAI_API_KEY"] = "sk-YOUR_KEY_HERE"
llm = OpenAI(model_name='text-davinci-002', temperature=0)

MATH_PROMPT = '''
Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?

# solution in Python:
"""There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?"""
computers_initial = 9
computers_per_day = 5
num_days = 4  # 4 days between monday and thursday
computers_added = computers_per_day * num_days
computers_total = computers_initial + computers_added
result = computers_total
return result


Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?

# solution in Python:
"""Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?"""
toys_initial = 5
mom_toys = 2
dad_toys = 2
total_received = mom_toys + dad_toys
total_toys = toys_initial + total_received
result = total_toys


Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?

# solution in Python:
"""Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?"""
jason_lollipops_initial = 20
jason_lollipops_after = 12
denny_lollipops = jason_lollipops_initial - jason_lollipops_after
result = denny_lollipops

Q: {question}

# solution in Python:
'''

llm_out = llm(MATH_PROMPT.format(question=question))
print(llm_out)
exec(llm_out) # 传递给Python执行
print(result) # 210
```

输出

```sh
"""Emma took a 60 minute plane ride to seattle. She then took a 2 hour train ride to portland, and then a 30 minute bus ride to vancouver. How long did it take her to get to vancouver?"""
plane_ride = 60
train_ride = 2 * 60  # 2 hours in minutes
bus_ride = 30
total_time = plane_ride + train_ride + bus_ride
result = total_time
```

### 讨论性问题

[解答讨论性问题](https://learnprompting.org/zh-Hans/docs/applied_prompting/short_response): 通过正确的提示，GPT-3非常擅长写作短格式回答。
- 讨论性问题的回答通常约为100到700字。更长的内容可能会有些棘手，因为语言模型的记忆有限，并且难以理解他们所写的内容全局。
- 一个好的提示应该给出**具体格式**和**内容指令**。
- 许多讨论性问题并不适合提示
  - Bad: 内战是一场关于扩张的冲突吗？同意还是不同意，为什么？
  - Good(明确指令): 解释内战的原因以及扩张是否在冲突中起了作用。附上支持您论点的证据
  - Better(具体格式): 写一篇高度详细的讨论回答，按照论文结构回答以下提示: 解释内战的原因以及扩张是否在冲突中起了作用。附上支持您论点的证据。

### GPT-3构建ChatGPT

- [GPT-3构建ChatGPT](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatgpt)
  - 与GPT-3.5系列模型一样， ChatGPT 是使用RLHF训练，但大部分效果来自于使用了**好的提示**。
  - ![](https://learnprompting.org/zh-Hans/assets/images/chatgpt_ui_diagram-87b55966a74fe72526d9e2c4b86c6650.png)
  - 如何记忆？将上文附加到下一个提示中，包括ChatGPT在内GPT-3模型，组合提示和生成响应的标记限制为4097个（约3000个单词）。

### 聊天机器人+知识库

[聊天机器人+知识库](https://learnprompting.org/zh-Hans/docs/applied_prompting/build_chatbot_from_kb)
- 传统聊天机器人基于意图识别，以 **< 一组问题, 回复 >** 形式存在， 问题：域外意图无法回答
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_intents-26defe08be3840898a001f744580bebd.png)
- 知识库 Knowledge Base 是存储为结构化和非结构化数据的信息，可用于分析或推断。
- 每个意图与文档相关联，而不是一组问题和特定答案, 这样意图可以更加广泛
- 当用户询问有关登录的问题时，将“登录问题”**文档**传递给 GPT-3 作为上下文信息，并生成特定的响应。
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_gpt3-b1b71b5a76270f0ce2e14716f7546d33.png)
- 这种方法减少了需要处理的意图数量，并允许更好地适应每个问题的答案。此外，如果与意图关联的文档描述了不同的流程（例如“在网站上登录”的流程和“在移动应用程序上登录”的流程），GPT-3可以在给出最终答案之前自动询问用户以获得更多的上下文信息。
- GPT-3这样的LLM模型, 最大提示的长度约为**4k令牌**（text-davinci-003模型），不足以将整个知识库馈入单个提示中。 LLM由于计算原因具有最大提示的限制，因为生成文本涉及多个计算，随着提示大小的增加，计算量也会迅速增加。未来的LLM可能不会有这种限制，同时保留文本生成能力。
- 用GPT-3构建一个聊天机器人, 两个步骤
  - 为用户问题选择适当意图，即从知识库中检索正确的文档。--- 语义搜索 semantic search, 如 sentence-transformers 库中的预训练模型为每个文档分配一个分数。分数最高的文档将用于生成聊天机器人答案。
  - 有了正确的文档，就可以用 GPT-3 生成适当答案；--- 用temperature为0.7的text-davinci-003模型。
- 注意
  - 只要有正确的上下文信息，GPT-3就可以进行消歧义。
  - 当用户问题可以在上下文中找到答案时，GPT-3很少生成虚假信息。由于用户问题通常是短小模糊的文本，不能总是依赖语义搜索步骤来检索正确的文档，因此容易受到**虚假信息生成**的影响。
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_gpt3_organized-7a4e195a1ae2dfc507fb58a6a987f2b6.png)

提示制作
- 角色提示: 一种启发式技术，为AI分配特定的角色。
- 相关的知识库信息, 即在语义搜索步骤中检索到的文档。
- 用户和聊天机器人之间最后一次交换的消息. 这对于用户发送的未指定整个上下文的消息非常有用。我们将在后面的例子中看到它。请查看此示例 了解如何使用GPT-3管理对话。
- 最后, 用户的问题.
- ![](https://learnprompting.org/zh-Hans/assets/images/chatbot_from_kb_prompt-8f462a976480059dc1dc16128205efb5.png)


ChatGPT 的 Prompt 构造
- `system`: 系统设置，默认是高级聊天机器人
- `conversation`: 会话历史
- `user`: 用户输入

```py
chatbot_prompt = """
    作为一个高级聊天机器人，你的主要目标是尽可能地协助用户。这可能涉及回答问题、提供有用的信息，或根据用户输入完成任务。为了有效地协助用户，重要的是在你的回答中详细和全面。使用例子和证据支持你的观点，并为你的建议或解决方案提供理由。

    <conversation history>

    User: <user input>
    Chatbot:"""
```


## cot实践

auto-cot
- [Auto-CoT: Automatic Chain of Thought Prompting in Large Language Models (ICLR 2023)](https://github.com/amazon-science/auto-cot)
- Official implementation for "Automatic Chain of Thought Prompting in Large Language Models" (stay tuned & more will be updated)
- ![](https://user-images.githubusercontent.com/22279212/194787183-a1f8dff8-a0ad-43a1-827f-819671503860.png)

#### Prompt DEMO

[Dyno](https://trydyno.com/)：Prompt Engineering IDE
- [trydyno](https://embed.trydyno.com/), prompt工具包，支持自定义gpt-3模型、token长度、温度、top p等参数调节

<script defer="defer" src="https://embed.trydyno.com/embedder.js"></script>
<link href="https://embed.trydyno.com/embedder.css" rel="stylesheet" />

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="你多大" initial-response="" max-tokens="256" box-rows="3" model-temp="0.7" top-p="1">
    <noscript>Failed to load Dyno Embed: JavaScript must be enabled</noscript>
</div>



# 结束
