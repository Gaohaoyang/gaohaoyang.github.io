---
layout: post
title:  "深度学习框架（Pytorch）学习笔记"
date:   2019-05-11 12:00:00
categories: 编程语言
tags: Pytorch Tensorflow Python 深度学习
excerpt: Pytorch 编程技能汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

【2020-8-31】[30天吃掉那只TensorFlow2](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)

# 深度学习框架

## 发展历史

- 【2021-3-3】[深度学习框架编年史：搞深度学习的那帮人，不是疯子，就是骗子！](https://mp.weixin.qq.com/s/PRg2P_7TtebhLAXwplEUdA)
- 2002年，瑞士是物理和数学领域的领跑者。也在2002年，瑞士戴尔莫尔感知人工智能（Idiap）研究所诞生了第一个机器学习库Torch。
  - 机器学习库Torch，出自“葡萄酒产区”研究所的一份研究报告（三位作者分别是：Ronan Collobert、Samy Bengio、Johnny Mariéthoz）。其中一位作者姓本吉奥（Bengio），没错，这位眉毛粗粗的科学家，就是深度学习三巨头之一，约舒亚·本吉奥（Yoshua Bengio）的兄弟。2007年他跳槽去了谷歌。
- 2007年，加拿大蒙特利尔大学开发了第一个深度学习框架Theano（行业祖师爷）。框架和图灵奖获得者颇有渊源，约舒亚·本吉奥（Yoshua Bengio）和伊恩·古德费洛（Ian Goodfellow）都有参与Theano。 
  - 库和框架的不同之处，在于境界。库是兵器库，框架则是一套武林绝学的世界观，程序员在这个世界观的约束下去练（编）拳（程）法（序），结果被框架所调用。框架接管了程序的主控制流。有了框架，才能做到只关注算法的原理和逻辑，不用去费事搞定底层系统、工程的事。
- 2013年，著名的Caffe框架诞生，发音和“咖啡”相似，是“快速特征提取的卷积框架”论文的英文简称。贾扬清第一个C++项目
  - 贾扬清已经在美国加州大学伯克利分校攻读博士学位，开启了计算机视觉的相关研究。那时候，他常被一个问题困扰：怎样训练和设计深度学习的网络？为此，贾扬清想造一个通用工具。
- 2013年，Parameter Server（参数服务器）的两位著名教授走向台前，邢波（Eric Xing）教授和Alex Smola教授，现在两位均在美国卡内基梅隆大学（CMU）任教。
  - 参数服务器是个编程框架，也支持其他AI算法，对深度学习框架有重要影响。
  - 高校实验室善于技术创新，深度学习框架的很多精髓创意源于此地。但是，深度学习框架复杂性高、工程量极大，长期负责复杂产品，高校并不擅长。多年后，高校出生的深度学习框架，都以某种方式“进入”企业，或者被企业赶超
- 2015年11月，谷歌大脑团队开发的TensorFlow开源，原创者之一是谷歌天才科学家，杰夫·迪恩（Jeff Dean）。谷歌的搜索、油管、广告、地图、街景和翻译的背后，都有其身影。 
  - TensorFlow直译，张量（tensor）在图中流动（flow）。由此也可获知，数据流图是框架的重要技术。数据流图由算子组成，算子又分为大算子和小算子。Caffe是大算子抽象，TensorFlow是小算子抽象。小算子好处是灵活，坏处是性能优化难。
  - 在2000年下半年的时候，Jeff Dean的代码速度突然激增了40倍，原因是他把自己的键盘升级到了USB 2.0。编译器从来不会给Jeff Dean警告，但Jeff Dean会警告编译器。—— 段子
- 2015 年是一个重要的年份，何恺明等人的研究成果，突破了边界，在准确率上再创新高，风头一时无二。
- 2015年，谷歌AI研究员弗朗索瓦·乔莱特（Francois Chollet）几乎是独自完成了著名的Keras 框架的开发，为谷歌再添一条护城河，大有“千秋万代，一统江湖”的势头。
- 2016年，微软CNTK（Cognitive Toolkit）伸手接过女神（内部孵化项目Minerva）的接力棒，可惜魔障难消，用的人少，没有推广开，于2019年停止维护。
- 2016年，贾扬清从谷歌TensorFlow团队离职，跳槽到了Facebook公司。与谷歌挥手道别，四载光阴（实习两年，工作两年），往事依稀，他的内心充满感怀。
- 2016年5月，陈天奇读博二开发的MXNet（读作“mixnet”，mix是中文“混合”之意）开源，浓缩了当时的精华，合并了几个原来有的项目，陈天奇cxxnet、参数服务器、智慧女神、颜水成学生林敏的purine2。仅一年时间里，就做出了完整的架构。团队中还有一位闻名遐迩的大神，李沐（现任亚马逊公司资深主任科学家，principal scientist）。2017年9月，MXNet被亚马逊选为官方开源平台。
- 2017年，祖师爷Theano官宣退休。贾扬清借鉴谷歌TensorFlow框架里面的一些新思想，实现了一个全新的开源Caffe2。三十而立的他，成长为遍历世界级产品的第一高手。
- 2018年，PyTorch接纳Caffe2后，意外崛起，上演令谷歌框架王冠落地的戏剧性一幕。易用性确实可以抢客户，但谷歌没有想到脸书抢了这么多。后来者确实可以居上，但谷歌没有想到脸书仅用如此短的时间。
  - 谷歌出发最早，为何没有独坐钓鱼台？为什么是脸书抢了市场？
  - 谷歌野心非常大，初期想做很大很全的工具。虽然完备性很强，但是，系统过度复杂。虽然以底层操作为主，有很多基础的功能，但是这些功能没能封装得很好，需要开发者自己解决（定义），手动工作过多。
  - 仅仅是丢市场还不够惨，PyTorch框架带火了背后的技术（动态执行等），脸书开始左右技术趋势。
- 微软在智慧女神和CNTK两次滑铁卢之后，依然斗志昂扬准备第三次入局。微软思路清奇地设计了ONNX（全称Open Neural Network Exchange），一种开放式深度学习神经网络模型的格式，用于统一模型格式标准。
- 2016 年8月，百度PaddlePaddle开源，PaddlePaddle作为国内唯一的开源深度学习框架，此后两年多，都是孤家寡人。2013年，百度第一位T11徐伟，同时也是百度深度学习框架PaddlePaddle的原创者和奠基人。
- 2019年，百度PaddlePaddle有了中文名，名叫“飞桨”。国外产品连个中文名都懒得起。
- 2019年2月，一流科技获得千万级Pre-A轮投资，袁进辉是创始人兼CEO。邢波教授团队和袁进辉团队双剑合璧开发了OneFlow
  - 2014年，微软亚研院副院长马维英（现任清华大学智能产业研究院讲席教授、首席科学家）找到一位研究员，名叫袁进辉，他是清华大学计算机专业的博士，师从张钹院士。
  - 来自美国CMU的教授，名叫邢波，此时任微软亚研院顾问一职，他擅长的领域包括大规模计算系统。
- 2020年，国产深度学习框架井喷。国产深度学习框架的“元年”。
  - 3月20日，清华大学计图（Jittor）。
  - 3月25日，旷视科技天元（MegEngine） 。
  - 3月28日，华为MindSpore。
  - 7月31日，一流科技OneFlow。OneFlow有两个创新点：一会自动安排数据通信。二把数据通信和计算的关系协调好，让整体效率更高。
- 守旧的经验是，既然国外开源了，就抓紧学。既然人家成了事实工业标准，就尽力参与。总是慢了好几拍，Linux这轮就是这样。
- 引用某游戏厂商的经典台词是：“别催了，在抄了，在抄了。”
- 可惜竞争从来不是游戏。
- 深度学习框架的台词是：“不能照抄，不能舔狗，舔到最后，一无所有。”
- 这世界上唯一能够碾压国内一线城市房价增速的，只有AI模型的规模，虽然硬件和软件的进步已经将每年的训练成本降低了37%；但是，AI模型越来越大，以每年10倍的速度增长。
- 前美国国防部咨询顾问，史蒂夫·马奎斯的说法是：“开源项目，来源于最纯粹的竞争。如果一个开源项目在商业世界获得了成功，那决不会是出于侥幸，决不会是因为其它竞争者恰好被规章制度所累、被知识产权法约束、被人傻钱多的金主拖垮。一个开源项目胜出了，背后只会有一个原因——它真的比其他竞争者都要好。”

## Tensorflow v.s. Pytorch

结论：
- 如果是**工程师**，应该优先选TensorFlow2.
- 如果是**学生**/**研究**人员，应该优先选择Pytorch.
- 如果时间足够，最好TensorFlow2和Pytorch都要学习掌握。
- 理由如下：
  - 1，在工业界最重要的是模型落地，目前国内的大部分互联网企业只支持TensorFlow模型的在线部署，不支持Pytorch。 并且工业界更加注重的是模型的高可用性，许多时候使用的都是成熟的模型架构，调试需求并不大。
  - 2，研究人员最重要的是快速迭代发表文章，需要尝试一些较新的模型架构。而Pytorch在易用性上相比TensorFlow2有一些优势，更加方便调试。 并且在2019年以来在学术界占领了大半壁江山，能够找到的相应最新研究成果更多。
  - 3，TensorFlow2和Pytorch实际上整体风格已经非常相似了，学会了其中一个，学习另外一个将比较容易。两种框架都掌握的话，能够参考的开源模型案例更多，并且可以方便地在两种框架之间切换。
- Keras库在2.3.0版本后将不再更新，用户应该使用tf.keras

总结：
- （1）**模型**可用性：pytorch更好
- （2）**部署**便捷性：TensorFlow更好
  - Serving 和 TFLite 比 PyTorch 的同类型工具要稳健一些。而且，将 TFLite 与谷歌的 Coral 设备一起用于本地 AI 的能力是许多行业的必备条件。相比之下，PyTorch Live 只专注于移动平台，而 TorchServe 仍处于起步阶段。
  - 既想用 TensorFlow 的部署基础设施，又想访问只能在 PyTorch 中使用的模型，作者推荐使用 ONNX 将模型从 PyTorch 移植到 TensorFlow。
- （3）**生态系统**对比：TensorFlow 胜出

[2022年了，PyTorch和TensorFlow你选哪个？](https://www.toutiao.com/i7043969108496515597)

选择指南：
- （1）工程师
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRNg3967KOj?from=pc)
  - TensorFlow 强大的部署框架和端到端的 TensorFlow Extended 平台是很珍贵的。能在 gRPC 服务器上进行轻松部署以及模型监控和工件跟踪是行业应用的关键
  - 仅在 PyTorch 中可用的 SOTA 模型，那也可以考虑使用 PyTorch（TorchServe）
  - 移动应用，也可以用pytorch live
  - 音频或视频输入，应该使用 TensorFlow
  - 用 AI 的嵌入式系统或 IoT 设备，鉴于 TFLite + Coral 生态系统，用tf
- （2）研究者
  - 大概率会使用 PyTorch，坚持使用，大多数 SOTA 模型都适用于 PyTorch；
  - 但强化学习领域的一些研究应该考虑使用 TensorFlow。原生 Agents 库，并且 DeepMind 的 Acme框架和Sonnet
  - 想用TPU训练，但不想用TensorFlow，那考虑探索谷歌的JAX。JAX 本身不是神经网络框架，而是更接近于具有自动微分能力的 GPU/TPU 的 NumPy 实现。
  - 不用TPU 训练，那最好是坚持使用 PyTorch
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRO25bBNaV1?from=pc)
- （3）教授，因课程重点而已
  - 培养具备行业技能的深度学习工程师，胜任整个端到端深度学习任务，而不仅仅是掌握深度学习理论，那应该使用 TensorFlow
  - 深度学习理论和理解深度学习模型的底层原理/高级课程/研究，那应该使用 PyTorch。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmROWHLI567V?from=pc)
- （4）职业转型
  - 对框架完全不熟悉，请使用 TensorFlow，因为它是首选的行业框架。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRQcBgRnxW0?from=pc)
- （5）业余爱好者
  - 大项目，部署到物联网/嵌入式设备，TensorFlow + TFLite
  - 了解深度学习，PyTorch
  - 入门即可，keras


HuggingFace使得深度学习从业者仅借助几行代码就能将训练、微调好的 SOTA 模型整合到其 pipeline 中。
- HuggingFace中大约有85%的模型只能在PyTorch上用，剩下的模型还有一半也可以在 PyTorch 上用。
- 相比之下，只有16%的模型能在 TensorFlow 上用，只有 8% 是 TensorFlow 所独有的。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQr91qOhSid?from=pc)
- Top 30 个模型中，能在 TensorFlow 上用的还不到 2/3，但能在 PyTorch 上用的却达到了 100%，没有哪个模型只能在 TensorFlow 上用。
- 8个顶级研究期刊论文中框架采用情况：PyTorch 的采用率增长迅速，几年时间就从原来的 7% 长到了近 **80%**。 很多转向 PyTorch 的研究者都表示 TensorFlow 1 太难用了。尽管 2019 年发布的 TensorFlow 2 改掉了一些问题，但彼时，PyTorch 的增长势头已经难以遏制。
- ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQtFATPcRfG?from=pc)
- 2018 年还在用 TensorFlow 的论文作者中，有 55% 的人在 2019 年转向了 PyTorch，但 2018 年就在用 PyTorch 的人有 85% 都留了下来。
- Papers with Code 本季度创建的 4500 个库中，有 60% 是在 PyTorch 中实现的，只有 11% 是在 TensorFlow 中实现的。相比之下，TensorFlow 的使用率在稳步下降，2019 年 TensorFlow 2 的发布也没有扭转这一趋势。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQtlBjJz4JO?from=pc)
大公司：
- Google AI：谷歌发布的论文自然会用 TensorFlow。鉴于在论文方面谷歌比 Facebook 更高产，一些研究者可能会发现掌握 TensorFlow 还是很有用的。
- DeepMind：DeepMind 也用 TensorFlow，而且也比 Facebook 高产。他们创建了一个名叫 Sonnet 的 TensorFlow 高级 API，用于研究目的。有人管这个 API 叫「科研版 Keras」，那些考虑用 TensorFlow 做研究的人可能会用到它。此外，DeepMind 的 Acme 框架可能对于强化学习研究者很有用。
- OpenAI：OpenAI 在 2020 年宣布了全面拥抱 PyTorch 的决定。但他们之前的强化学习基线库都是在 TensorFlow 上部署的。基线提供了高质量强化学习算法的实现，因此 TensorFlow 可能还是强化学习从业者的最佳选择。
- JAX：谷歌还有另一个框架——JAX，它在研究社区中越来越受欢迎。与 PyTorch 和 TensorFlow 相比，JAX 的开销要小得多。但同时，JAX 和前两个框架差别也很大，因此迁移到 JAX 对于大多数人来说可能并不是一个好选择。目前，有越来越多的模型 / 论文已经在用 JAX，但未来几年的趋势依然不甚明朗。

模型部署

TensorFlow Serving：
- TensorFlow Serving 用于在服务器上部署 TensorFlow 模型，无论是在内部还是在云上，并在 TensorFlow Extended（TFX）端到端机器学习平台中使用。Serving 使得用模型标记（model tag）将模型序列化到定义良好的目录中变得很容易，并且可以选择在保持服务器架构和 API 静态的情况下使用哪个模型来进行推理请求。
- Serving 可以帮用户轻松地在 gRPC 服务器上部署模型，这些服务器运行谷歌为高性能 RPC 打造的开源框架。gRPC 的设计意图是连接不同的微服务生态系统，因此这些服务器非常适合模型部署。Serving 通过 Vertex AI 和 Google Cloud 紧密地集成在一起，还和 Kubernetes 以及 Docker 进行了集成。

TensorFlow Lite：
- TensorFlow Lite 用于在移动或物联网 / 嵌入式设备上部署 TensorFlow 模型。TFLite 对这些设备上的模型进行了压缩和优化，并解决了设备上的 AI 的 5 个约束——延迟、连接、隐私、大小和功耗。可以使用相同的 pipeline 同时导出基于标准 Keras 的 SavedModels（和 Serving 一起使用）和 TFLite 模型，这样就能比较模型的质量。
- TFLite 可用于 Android、iOS、微控制器和嵌入式 Linux。TensorFlow 针对 Python、Java、C++、JavaScript 和 Swift 的 API 为开发人员提供了广泛的语言选项。

PyTorch 用户需要使用 Flask 或 Django 在模型之上构建一个 REST API，但现在他们有了 TorchServe 和 PyTorch Live 的本地部署选项。

TorchServe：
- TorchServe 是 AWS 和 Facebook 合作的开源部署框架，于 2020 年发布。它具有端点规范、模型归档和指标观测等基本功能，但仍然不如 TensorFlow。TorchServe 同时支持 REST 和 gRPC API。

PyTorch Live：
- PyTorch 于 2019 年首次发布 PyTorch Mobile，旨在为部署优化的机器学习模型创建端到端工作流，适用于 Android、iOS 和 Linux。
- PyTorch Live 于 12 月初发布，以移动平台为基础。它使用 JavaScript 和 React Native 来创建带有相关 UI 的跨平台 iOS 和 Android AI 应用。设备上的推理仍然由 PyTorch Mobile 执行。Live 提供了示例项目来辅助入门，并计划在未来支持音频和视频输入。

## pytorch设计思路

Pytorch 的哲学是 **解决当务之急**，也就是说即时构建和运行计算图。

# 安装

## Pytorch

安装：

```shell
pip install torch torchvision
```
验证：

```python
python -c "import torch;x = torch.rand(5, 3);print(x)"
```

## tensorflow

```shell
pip install --upgrade tensorflow
```
验证：

```python
python -c "import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
```

# 生态系统


## pytorch生态

- Hub：
  - PyTorch Hub 作为面向研究的官方平台，用于与预训练模型共享存储库。Hub 拥有广泛类别的模型，包括用于音频、视觉、NLP 任务的模型，还有用于生成任务的 GAN 模型。
- SpeechBrain：
  - SpeechBrain 是 PyTorch 的官方开源语音工具包。SpeechBrain 能够完成自动语音识别（ASR）、说话人识别、验证和分类等任务。如果你不想构建任何模型，而是想要一个具有情感分析、实体检测等功能的即插即用工具，你可以选择使用 AssemblyAI 的 Speech-to-Text API。
  - 当然，PyTorch 的工具页面还有很多其他有用的库，包括为计算机视觉和自然语言处理量身定制的库，例如 fast.ai。
- TorchElastic：
  - TorchElastic 是 AWS 和 Facebook 2020 年联合发布的分布式训练工具，可管理工作进程并协调重启行为，以便用户在计算节点集群上训练模型，这些节点可以动态变化而不会影响训练。因此，TorchElastic 可防止因服务器维护或网络问题等导致的灾难性故障，不会丢失训练进度。TorchElastic 具有与 Kubernetes 集成的特性，并已集成到 PyTorch 1.9+ 中。
- TorchX：
  - TorchX 是一个用于快速构建和部署机器学习应用程序的 SDK。TorchX 包括 Training Session Manager API，可在支持的调度程序上启动分布式 PyTorch 应用程序。TorchX 负责启动分布式作业，同时原生支持由 TorchElastic 局部管理的作业。
- Lightning：
  - PyTorch Lightning 有时被称为 PyTorch 的 Keras。虽然这种类比并不准确，但 Lightning 的确是简化 PyTorch 中模型工程和训练过程的有用工具，自 2019 年首次发布以来已经逐渐趋于成熟。Lightning 以面向对象的方式处理建模过程，定义了可重用和可跨项目使用的可共享组件。

## TensorFlow生态

- Hub：
  - TensorFlow Hub 是一个经过训练的机器学习模型库，可以进行微调，让用户只需几行代码就能使用像 BERT 这样的模型。Hub 包含适用于不同用例的 TensorFlow、TensorFlow Lite 和 TensorFlow.js 模型，可用于图像、视频、音频和文本处理。
- Model Garden：
  - 如果现成的预训练模型不适用于用户的应用，那么 TensorFlow 的存储库 Model Garden 可以提供 SOTA 模型的源代码。对于想要深入了解模型工作原理，或根据自己的需要修改模型的用户，Model Garden 将非常有用。
  - Model Garden 包含谷歌维护的官方模型、研究人员维护的研究模型和社区维护的精选社区模型。TensorFlow 的长期目标是在 Hub 上提供来自 Model Garden 的模型的预训练版本，并使 Hub 上的预训练模型在 Model Garden 中具有可用的源代码。
- Extended（TFX）：
  - TensorFlow Extended 是 TensorFlow 用于模型部署的端到端平台。该平台的功能强大，包括：加载、验证、分析和转换数据；训练和评估模型；使用 Serving 或 Lite 部署模型；跟踪 artifact 及其依赖项。TFX 还可以与 Jupyter 或 Colab 一起使用，并且可以使用 Apache Airflow/Beam 或 Kubernetes 进行编排。TFX 与 Google Cloud 紧密集成，可与 Vertex AI Pipelines 一起使用。
- Vertex AI：
  - Vertex AI 是 Google Cloud 今年刚刚发布的统一机器学习平台，旨在统一 GCP、AI Platform 和 AutoML，成为一个平台。Vertex AI 能够以无服务器方式编排工作流，帮助用户自动化、监控和管理机器学习系统。Vertex AI 还可以存储工作流的 artifact，让用户可以跟踪依赖项和模型的训练数据、超参数和源代码。
- Coral：
  - 尽管有各种各样的 SaaS 公司依赖基于云的人工智能，但许多行业对本地人工智能的需求也在不断增长，Google Coral 就是为了满足这一需求而创建的。Coral 是一个完整的工具包，可以使用本地 AI 构建产品。Coral 于 2020 年发布，解决了部署部分 TFLite 中提到的实现板载 AI 的问题，克服了隐私和效率等方面的困难。
  - Coral 提供了一系列用于原型设计、生产和传感的硬件产品，其中一些本质上是增强型的树莓派，专为 AI 应用程序创建，能够利用 Edge TPU 在低功耗设备上进行高性能推理。Coral 还提供用于图像分割、姿态估计、语音识别等任务的预编译模型，为希望创建本地 AI 系统的开发人员提供支持。创建模型的基本步骤如下面的流程图所示。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRMl2J55OqA?from=pc)
- TensorFlow.js：
  - TensorFlow.js 是一个用于机器学习的 JavaScript 库，允许用户使用 Node.js 在浏览器和服务器端训练和部署模型。
- Cloud：
  - TensorFlow Cloud 是一个可以将本地环境连接到 Google Cloud 的库，它的 API 旨在弥补本地机器上模型构建和调试与 GCP 上分布式训练和超参数调整之间的差距，而无需使用 Cloud Console。
- Colab：
  - Google Colab 是一个基于云的 notebook 环境，与 Jupyter 非常相似。Colab 易于连接到 Google Cloud 进行 GPU 或 TPU 训练，并且 Colab 还可以和 PyTorch 一起使用。
- Playground：
  - Playground 是一个小而精致的可视化工具，用于帮助用户理解神经网络的基础知识。要户可以更改 Playground 内置神经网络的层数和大小，以实时查看神经网络是如何学习特征的，用户还可以看到改变学习率和正则化强度等超参数如何影响不同数据集的学习过程。Playground 允许实时播放学习过程，以高度直观的方式查看输入在训练过程中是如何转换的。Playground 还提供了一个开源的小型神经网络库，是它自身的构建基础，用户能够查看其源代码的具体细节。
- Datasets：
  - 谷歌研究院的 Datasets 是谷歌定期发布的数据集的整合资源。谷歌还提供了数据集搜索以访问更广泛的数据集资源。当然，PyTorch 用户也可以利用这些数据集。


## TorchServe

- 【2020-4-27】PyTorch 1.5发布，与AWS联手推出TorchServe
- PyTorch 1.5 发布，升级了主要的 torch**vision** (视觉)，torch**text**（文本） 和 torch**audio**（语音） 库，并推出将模型从 Python API 转换为 C++ API 等功能。
- 除此之外，Facebook 还和 Amazon 合作，推出了两个重磅的工具：TorchServe 模型服务框架 和 TorchElastic Kubernetes 控制器。
  - TorchServe 旨在为大规模部署 PyTorch 模型推理，提供一个干净、兼容性好的工业级路径。
  - TorchElastic Kubernetes 控制器，可让开发人员快速使用 Kubernetes 集群，在 PyTorch 中创建容错分布式训练作业。
- 【2021-1-19】pytorch模型部署工具[TorchServe](https://github.com/pytorch/serve/blob/master/README.md#serve-a-model)
- [如何评价 PyTorch 在 2020 年 4 月推出的 TorchServe？](https://www.zhihu.com/question/389731764)
- TorchServe 旨在为大规模部署 PyTorch 模型推理，提供一个干净、兼容性好的工业级路径。其主要的特点包括有：
    - 原生态 API：支持用于预测的推理 API，和用于管理模型服务器的管理 API。
    - 安全部署：包括对安全部署的  HTTPS 支持。
    - 强大的模型管理功能：允许通过命令行接口、配置文件或运行时 API 对模型、版本和单个工作线程进行完整配置。
    - 模型归档：提供执行「模型归档」的工具，这是一个将模型、参数和支持文件打包到单个持久工件的过程。使用一个简单的命令行界面，可以打包和导出为单个「 .mar」文件，其中包含提供 PyTorch 模型所需的一切。该 .mar 文件可以共享和重用。
    - 内置的模型处理程序：支持涵盖最常见用例，如图像分类、对象检测、文本分类、图像分割的模型处理程序。TorchServe 还支持自定义处理程序。
    - 日志记录和指标：支持可靠的日志记录和实时指标，以监视推理服务和端点、性能、资源利用率和错误。还可以生成自定义日志并定义自定义指标。
    - 模型管理：支持同时管理多个模型或同一模型的多个版本。你可以使用模型版本回到早期版本，或者将流量路由到不同的版本进行 A/B 测试。
    - 预构建的图像：准备就绪后，可以在基于 CPU 和 NVIDIA GPU 的环境中，部署 TorchServe 的 Dockerfile 和 Docker 镜像。
- 综上可知，这次的 TorchServe  在推理任务上，将会有很大的使用空间，对于广大开发者来说是一件好事。这一点可以留着以后去慢慢验证。对于这次 Facebook 和 AWS 合作，明显可以看出双方在各取所长，试图打造一个可以反抗谷歌 TensorFlow 垄断的方案。
- 问题：
    - 为什么TorchServe采用Java开发，而没有像TensorFlow Serving一样采用更好性能的C++，抑或是采用Golang?
- TorchServe Architecture
    - ![](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)

- Model Server for PyTorch Documentation

**Basic Features**

* [Serving Quick Start](../README.md#serve-a-model) - Basic server usage tutorial
* [Model Archive Quick Start](../model-archiver#creating-a-model-archive) - Tutorial that shows you how to package a model archive file.
* [Installation](../README.md##install-torchserve) - Installation procedures
* [Serving Models](server.md) - Explains how to use `torchserve`.
  * [REST API](rest_api.md) - Specification on the API endpoint for TorchServe
  * [gRPC API](grpc_api.md) - Specification on the gRPC API endpoint for TorchServe
* [Packaging Model Archive](../model-archiver/README.md) - Explains how to package model archive file, use `model-archiver`.
* [Logging](logging.md) - How to configure logging
* [Metrics](metrics.md) - How to configure metrics
* [Batch inference with TorchServe](batch_inference_with_ts.md) - How to create and serve a model with batch inference in TorchServe
* [Model Snapshots](snapshot.md) - Describes how to use snapshot feature for resiliency due to a planned or unplanned service stop

 **Advanced Features**

* [Advanced settings](configuration.md) - Describes advanced TorchServe configurations.
* [Custom Model Service](custom_service.md) - Describes how to develop custom inference services.
* [Unit Tests](../ts/tests/README.md) - Housekeeping unit tests for TorchServe.
* [Benchmark](../benchmarks/README.md) - Use JMeter to run TorchServe through the paces and collect benchmark data

**Default Handlers**

* [Image Classifier](../ts/torch_handler/image_classifier.py) - This handler takes an image and returns the name of object in that image
* [Text Classifier](../ts/torch_handler/text_classifier.py) - This handler takes a text (string) as input and returns the classification text based on the model vocabulary
* [Object Detector](../ts/torch_handler/object_detector.py) - This handler takes an image and returns list of detected classes and bounding boxes respectively
* [Image Segmenter](../ts/torch_handler/image_segmenter.py) - This handler takes an image and returns output shape as [CL H W], CL - number of classes, H - height and W - width


# Pytorch教程

Pytorch的哲学是解决当务之急，也就是说即时构建和运行计算图。

## 快速入门

[PyTorch经验分享：新手如何搭建PyTorch程序](https://blog.csdn.net/jiongnima/article/details/103324839)

深度学习任务中，简单的PyTorch程序主要包含以下这4个模块。
- 网络模型定义文件，比如名称为network.py。
- 数据读取文件，比如名称为dataset.py。
- 训练接口程序，比如名称为train.py。
- 测试接口程序，比如名称为evaluate.py。
- ![](https://img-blog.csdnimg.cn/20191130161751508.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppb25nbmltYQ==,size_16,color_FFFFFF,t_70)
- 包含之前提到的 network，dataset，train 和 evaluate。当然也包含定制的模块，比如
  - cal_metric s用来计算实验指标
  - cfg 用来设置某些超参数
  - image_io_and_process 用来对图像进行预处理和后处理
  - loss_functions 用来定义与实现某些损失
  - utils 用来承载一些其他操作，比如读写txt文档。

详解：
1. 网络**模型定义**文件，比如名称为network.py。
  - 定义一个Python类，并继承 torch.nn.Module 这个类。
  - 重写构造函数 \__init__ 来定义网络中使用的层，在定义层或者模块的时候，经常会使用到 torch.nn 这个库。
  - 再重写父类的 forward 函数，在其中使用 \__init__ 函数中定义的层实现网络的前传过程
2. **数据读取**文件，比如名称为dataset.py。
  - PyTorch与其他框架具有鲜明差异的一点，就是数据读取**接口非常规范**。
  - PyTorch通过继承 torch.utils.data.Dataset 这个类实现自己的数据集读取。
  - 继承时，主要需要重写三个函数：__init__函数，__getitem__函数和__len__函数。
    - __**init**__函数：初始化，比如读取一下所需的记录数据的txt或者xml文件，传入一些预处理参数等等。
    - __**getitem**__函数：规定每个batch训练给网络喂数据的时候，应该采用怎样的数据读取方式，以及做怎样的预处理。
    - __**len**__函数：数据集中有多少数据
  - 训练程序中，只需要使用 torch.utils.data.**DataLoader** 开始在每一次训练中对数据集进行读取与遍历就行
  - train_data_loader = torch.utils.data.**DataLoader**(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,timeout=0, worker_init_fn=None)
  - 除去dataset，用的比较多的是batch_size，shuffle，num_workers和pin_memory这几个参数。
    - batch_size 指定了一个批次的数据容量
    - shuffle 指定了是否打乱数据
    - num_workers 指定了用多少个线程对数据进行读取
    - 而pin_memory 则指定了是否将数据放入显存。
3. **训练**接口程序，比如名称为train.py。与其他深度学习框架类似，都是先搭建图(模型)，然后在图上进行训练
  - 四个步骤，引用数据、引用模型、定义损失函数、定义优化器：
    - 引用之前定义的**数据读取接口**。
    - 引用model，即网络，需要的话就载入预训练参数。
    - 定义loss。
    - 定义优化器。
  - 然后就可以开始在for循环中进行可训练参数更新了。
  - 最后，在PyTorch训练程序中，还需要注意
    1. 第一点是**保存模型**和**加载预训练参数**。
      - 保存模型的已训练参数(也可以保存整个模型)，通过 torch.save 函数完成；
      - 加载预训练参数，通过model的父类，即 torch.nn.Module 的 load_state_dict 完成。里面的“strict”参数表示是否需要将预训练模型里面的参数与model里面的完全对齐。
    2. 第二点是使用**GPU训练**
      - 如果需要使用GPU训练时，需要使用到 to(device) 函数，意思就是将数据或者模型放到GPU上面。
    3. 第三点是**参数更新**过程
      - 在进行参数更新时，一共分成三步：
        - 将可训练参数**梯度置零** optimizer.zero_grad()
        - 根据损失值求**梯度** loss.backward()
        - **更新**可训练参数 optimizer.step()
    4. 第四点是将model置为训练模式
      - 对应代码中的model.train()
      - 训练模式会对某些定义的网络模块有影响，比如使用dropout层，在训练时会被激活。
4. **测试**接口程序，比如名称为evaluate.py。
  - 测试程序比训练程序简单很多，主要先进行两个步骤：
    - 定义网络
    - 载入已训练参数
  - 然后就可以读取测试样本进行前传了。需要注意的是，在进行模型的测试时，需要将model置为测试模式，对应代码中的model.eval()

PyTorch程序比较规范，简洁与清晰。并且在PyTorch中还使用到了非常多的面向对象的规范，有许多操作都是通过继承Python类进行实现的。

代码示例

```python
# （1）模型定义
class My_Network(nn.Module):
    # 继承自nn.Module
    def __init__(self): # 重写构造函数定义网络结构
        super(network, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) # 3×3卷积
        self.bn1 = nn.BatchNorm2d(64) # BatchNorm
        self.relu = nn.ReLU() # 激活
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) # 最大池化
        self.fc = nn.Linear(64 * 14 * 14, 512) # 全连接，将[n, (64*14*14)]变成[n, 512]
        for m in self.modules(): # 对可训练参数进行初始化（上面定义的所有模块都存储在此）
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x): # 重写forward实现前传
#前传，输入x尺寸为[n, c, 28, 28]
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(x.size(0), -1) #将四维Tensor变成两维，作为全连接的输入
        x = self.fc(x)
        return x

# （2）数据读取
class My_Dataset(torch.utils.data.Dataset):

    def __init__(self, data_txt_path):
        data_list = read_txt(data_txt_path) #读取一下记录数据与标签的txt
        self.data_list = np.random.permutation(data_list) #打乱一下txt
        self.transform = T.ToTensor() #预处理，将数据转化成[n, c, h, w]的形式，并归一化到[0,1]

    def __getitem__(self, index):
        sample = self.data_list[index] #读取txt中的一行，是一个数据对(image_path, label)
        image_path = sample.split(' ')[0]
        label = int(sample.split(' ')[-1])
        resized_image = read_image(image_path) #读取一下图像
        image = self.transform(resized_image) #转化成Tensor
        return image.float(), label 

    def __len__(self):
        return len(self.data_list)

# （3）训练
from __future__ import print_function
import torch.nn
from torch.nn import DataParallel
import torch.optim
import torch.utils.data
import argparse

#import 所需的各种模块
parser = argparse.ArgumentParser(description="")
#添加各种自定义参数
args = parser.parse_args()
def main():
    device = torch.device("cuda")
    train_dataset = My_Dataset(txt_path) 
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle, num_workers)
    model = My_Network()
    #若有预训练参数，可以载入预训练参数
    model.load_state_dict(torch.load(ckpt_path), strict=False)
    #定义损失，比如criterion = torch.nn.CrossEntropyLoss()
    criterion = My_Loss
    model.to(device)
    #可以将模型训练放在多张GPU上并行
    model = DataParallel(model)
    optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay)
    model.train() #将模型置为训练模式
     
    for i in range(epoch):
        for ii, data in enumerate(train_data_loader):
            data_input, label = data
            data_input = data_input.to(device)
            label = label.to(device).long()
            output = model(data_input)
            loss = criterion(output, label)
            print("loss =: ", loss)
            optimizer.zero_grad() #首先梯度置零
            loss.backward() #然后求梯度
            optimizer.step() #通过梯度更新参数
            iters = i * len(train_data_loader) + ii
            if iters % save_interval == 0:
                torch.save(model.state_dict(), save_path)
 
if __name__ == "__main__":
    main()

# （4）测试
from __future__ import print_function
import torch.nn
from torch.nn import DataParallel
import torch.optim
import torch.utils.data
import argparse
#import 所需的各种模块
 
parser = argparse.ArgumentParser(description="")
#添加各种自定义参数
args = parser.parse_args()
 
def main():
    device = torch.device("cuda")
    model = My_Network()
    model = DataParallel(model)
    model.load_state_dict(torch.load(snapshot_path)) #载入参数
    model.to(device)
    model.eval() #将模型置为测试模式
 
    for eval_data_path in eval_data_list:
        eval_data = read_image(eval_data_path)
        data = torch.from_numpy(eval_data) #将data转化为Tensor
        data = data.to(device)
        output = model(data) #前传得到结果
        #自定义操作，比如计算精度
 
if __name__ == "__main__":
    main(）
```


#  pytorch内部机制

[核心开发者全面解读Pytorch内部机制](https://zhuanlan.zhihu.com/p/240938895)

斯坦福大学博士生与 Facebook 人工智能研究所研究工程师 Edward Z. Yang 是 PyTorch 开源项目的核心开发者之一。他在 5 月 14 日的 PyTorch 纽约聚会上做了一个有关 PyTorch 内部机制的演讲，本文是该演讲的长文章版本。
 
选自ezyang博客，作者：Edward Z. Yang，机器之心编译，参与：panda。
- ![](https://pic4.zhimg.com/80/v2-ad42651fcc9d372bed03232338790aef_720w.jpg)
 
大家好！今天我想谈谈 PyTorch 的内部机制。
 
这份演讲是为用过 [PyTorch](https://mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)并且有心为 PyTorch 做贡献但却被 PyTorch 那庞大的 C++ 代码库劝退的人提供的。没必要说谎：PyTorch 代码库有时候确实让人难以招架。
 
本演讲的目的是为你提供一份导航图：为你讲解一个「支持自动微分的张量库」的基本概念结构，并为你提供一些能帮你在代码库中寻路的工具和技巧。我预设你之前已经写过一些 PyTorch，但却可能还没有深入理解机器学习软件库的编写方式。
- ![](https://pic2.zhimg.com/80/v2-bd7a5d2782f171ea8c51f06bae691481_720w.jpg)
 
本演讲分为两部分：
- 在第一部分中，我首先会全面介绍张量库的各种概念。我首先会谈谈你们知道且喜爱的[张量](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)数据类型，并详细讨论这种数据类型究竟能提供什么，这能让我们更好地理解其内部真正的实现方式。
  - 如果你是PyTorch高级用户，可能已经熟悉其中大部分材料了。我们也会谈到「扩展点（extension points）」的三个概念：**布局**（layout）、**设备**（device）和**数据类型**（dtype），这能引导我们思考张量类的扩展的方式。在 PyTorch 纽约聚会的现场演讲中，我略过了有关**自动梯度**（autograd）的幻灯片，但我在这里会进行一些讲解。
- 第二部分会阐述用PyTorch写代码时所涉及的基本细节。如何在 autograd 代码中披荆斩棘、什么代码是真正重要的以及怎样造福他人，我还会介绍 PyTorch 为你写核（kernel）所提供的所有炫酷工具。
 
## 概念
 
### 张量
 
张量是 PyTorch 中的核心数据结构。张量是一种包含某种标量类型（比如浮点数和整型数等）的 n 维数据结构。可以将张量看作是由一些数据构成的，还有一些元数据描述了张量的大小、所包含的元素的类型（dtype）、张量所在的设备（CPU 内存？CUDA 内存？）
- ![](https://pic3.zhimg.com/80/v2-7ee4489394b8e34d19009e40623382f2_720w.jpg)
 
另外还有一个没那么熟悉的元数据：**步幅**（stride）。stride 实际上是 PyTorch 最别致的特征之一，所以值得稍微多讨论它一些。
- ![](https://pic2.zhimg.com/80/v2-d7c311ed86d5171c35d74ec26dbe0d8d_720w.jpg)

张量是一个数学概念。但要在我们的计算机中表示它，我们必须为它们定义某种物理表示方法。最常用的表示方法是在内存中相邻地放置张量的每个元素（这也是术语「contiguous（**邻接**）」的来源），即将每一行写出到内存，如上所示。在上面的案例中，我已经指定该张量包含 32 位的整型数，这样你可以看到每一个整型数都位于一个物理地址中，每个地址与相邻地址相距 4 字节。为了记住张量的实际维度，我们必须将规模大小记为额外的元数据。
 
所以这幅图与步幅有什么关系？
- ![](https://pic3.zhimg.com/80/v2-561af40cdd43ef4557c4136900f46836_720w.jpg)
 
假设我想要读取我的逻辑表示中位置张量 \[0,1\] 的元素。我该如何将这个逻辑位置转译为物理内存中的位置？步幅能让我们做到这一点：要找到一个张量中任意元素的位置，我将每个索引与该维度下各自的步幅相乘，然后将它们全部加到一起。在上图中，我用蓝色表示第一个维度，用红色表示第二个维度，以便你了解该步幅计算中的索引和步幅。进行这个求和后，我得到了 2（零索引的）；实际上，数字 3 正是位于这个邻接数组的起点以下 2 个位置。
 
（后面我还会谈到 TensorAccessor，这是一个处理索引计算的便利类（convenience class）。当你使用 TensorAccessor 时，不会再操作原始指针，这些计算过程已经为你隐藏了起来。）
 
步幅是我们为 PyTorch 用户讲解方法的基本基础。举个例子，假设我想取出一个表示以上张量的第二行的张量：
 
![](https://pic2.zhimg.com/80/v2-8bb5e51f47b14b6571642c7dd8962029_720w.jpg)
 
使用高级的索引支持，我只需写出张量 \[1, :\] 就能得到这一行。重要的是：当我这样做时，不会创建一个新张量；而是会返回一个基于底层数据的不同域段（view）的张量。这意味着，如果我编辑该视角下的这些数据，它就会反映在原始的张量中。
 
在这种情况下，了解如何做到这一点并不算太困难：3 和 4 位于邻接的内存中，我们只需要记录一个说明该（逻辑）张量的数据位于顶部以下 2 个位置的偏移量（offset）。（每个张量都记录一个偏移量，但大多数时候它为零，出现这种情况时我会在我的图表中省略它。）
 
> 演讲时的提问：如果我取张量的一个域段，我该如何释放底层张量的内存？  
> 答案：你必须制作该域段的一个副本，由此断开其与原始物理内存的连接。你能做的其它事情实际上并不多。另外，如果你很久之前写过 Java，取一个字符串的子字符串也有类似的问题，因为默认不会制作副本，所以子字符串会保留（可能非常大的字符串）。很显然，Java 7u6 将其固定了下来。
 
如果我想取第一列，还会更有意思：
- ![](https://pic4.zhimg.com/80/v2-5c729a8c611af9f9060b956c56ee66ff_720w.jpg)
 
当我们查看物理内存时，可以看到该列的元素不是相邻的：两者之间有一个元素的间隙。步幅在这里就大显神威了：我们不再将一个元素与下一个元素之间的步幅指定为 1，而是将其设定为 2，即跳两步。（顺便一提，这就是其被称为「步幅（stride）」的原因：如果我们将索引看作是在布局上行走，步幅就指定了我们每次迈步时向前多少位置。）
 
步幅表示实际上可以让你表示所有类型的张量域段；如果你想了解各种不同的可能做法，请参阅 [https://ezyang.github.io/stride-visualizer/index.html](https://link.zhihu.com/?target=https%3A//ezyang.github.io/stride-visualizer/index.html)
 
我们现在退一步看看，想想我们究竟如何实现这种功能（毕竟这是一个关于内部机制的演讲）。如果我们可以得到张量的域段，这就意味着我们必须解耦张量的概念（你所知道且喜爱的面向用户的概念）以及存储张量的数据的实际物理数据的概念（称为「存储（storage）」）：
- ![](https://pic4.zhimg.com/80/v2-2b8476f6266dff15de357d5df53981c3_720w.jpg)
 
也许会有多个张量共享同一存储。存储会定义张量的 dtype 和物理大小，同时每个张量还会记录大小、步幅和偏移量，这定义的是物理内存的逻辑解释。
 
有一点需要注意：总是会存在一个张量-存储对，即使并不真正需要存储的「简单」情况也是如此（比如，只是用 torch.zeros(2, 2) 划配一个邻接张量时）。
 
> 顺便一提，我们感兴趣的不是这种情况，而是有一个分立的存储概念的情况，只是将一个域段定义为有一个基张量支持的张量。这会更加复杂一些，但也有好处：邻接张量可以实现远远更加直接的表示，而没有存储造成的间接麻烦。这样的变化能让 PyTorch 的内部表示方式更接近 Numpy。
 
上面已经介绍了一些张量的数据布局，但还是有必要谈谈如何实现对张量操作。在最抽象的层面上，当你调用 torch.mm 时，会发生两次调度：
- ![](https://pic2.zhimg.com/80/v2-dd78d289f622b459dab2068c96a6365d_720w.jpg)
-  第一次调度基于**设备类型**和**张量布局**：比如是 CPU 张量还是 [CUDA](https://mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)张量，是有步幅的张量还是稀疏的张量。这个调度是动态的：这是一个虚函数（virtual function）调用（这个虚函数调用究竟发生在何处是本演讲后半部分的主题）。
  - 这里需要做一次调度应该是合理的：CPU 矩阵乘法的实现非常不同于 CUDA 的实现。这里是动态调度的原因是这些核（kernel）可能位于不同的库（比如 libcaffe2.so 或 libcaffe2_gpu.so），这样你就别无选择：如果你想进入一个你没有直接依赖的库，你必须通过动态调度抵达那里。
- 第二次调度是在所涉**dtype**上的调度。这个调度只是一个简单的 switch 语句，针对的是核选择支持的任意 dtype。这里需要调度的原因也很合理：CPU 代码（或 CUDA 代码）是基于 float 实现乘法，这不同于用于 int 的代码。这说明你需要为每种 dtype 都使用不同的核。

如果想理解 PyTorch 中算子的调用方式，这可能就是你头脑中应有的最重要的知识。后面当我们更深入代码时还会回到这里。
- ![](https://pic2.zhimg.com/80/v2-03d0a22840b411e55e2803e61e622549_720w.jpg)
 
花点时间谈谈张量扩展。毕竟，除了密集的 CPU 浮点数张量，还有其它很多类型的张量，比如 XLA 张量、量化张量、MKL-DNN 张量；而对于一个张量库，还有一件需要思考的事情：如何兼顾这些扩展？
- ![](https://pic2.zhimg.com/80/v2-2a547c95739059be38b4dfba62637309_720w.jpg)
 
当前的用于扩展的模型提供了张量的四个扩展点。首先，有三个独立地确定张量类型的配套参数：
*   device（**设备**）：实际存储张量的物理内存，比如在 CPU、英伟达 GPU（cuda）、AMD GPU（hip）或 TPU（xla）上。设备之间各不相同的特性是有各自自己的分配器（allocator），这没法用于其它设备。   
*   layout（**布局**）：对物理内存进行逻辑解读的方式。最常用的布局是有步幅的张量（strided tensor），但稀疏张量的布局不同，其涉及到一对张量，一个用于索引，一个用于数据；MKL-DNN 张量的布局更加奇特，比如 blocked layout，仅用步幅不能表示它。  
*   dtype（**数据类型**）：张量中每个元素实际存储的数据的类型，比如可以是浮点数、整型数或量化的整型数。

如果你想为 PyTorch 张量添加一种扩展，你应该思考扩展这些参数中的哪几种。这些参数的笛卡尔积定义了你可以得到的所有可能的张量。现在，并非所有这些组合都有核（谁为 FPGA 上的稀疏量化张量用核?），但原则上这种组合可能有意义，因此我们至少应该支持表达它。
 
要为张量的功能添加「扩展」，还有最后一种方法，即围绕能实现的目标类型的 PyTorch 张量编写一个 wrapper（包装）类。这可能听起来理所当然，但有时候人们在只需要制作一个 wrapper 类时却跑去扩展那三个参数。wrapper 类的一个突出优点是开发结果可以完全不影响原来的类型（out of tree）。
 
你何时应该编写张量 wrapper，而不是扩展 PyTorch 本身？关键的指标是你是否需要将这个张量传递通过 autograd（自动梯度）反向通过过程。举个例子，这个指标告诉我们稀疏张量应该是一种真正的张量扩展，而不只是一种包含一个索引和值张量的 Python 对象：当在涉及嵌入的网络上执行优化时，我们想要嵌入生成稀疏的梯度。
- ![](https://pic2.zhimg.com/80/v2-0c99ed0b1afed1fb80e2a0b3330f0871_720w.jpg)
我们对扩展的理念也会影响张量本身的数据布局。对于我们的张量结构，我们真正想要的一件事物是固定的布局：我们不想要基本操作（这个说法很常见），比如「一个张量的大小是多少？」来请求虚调度。
 
所以当你查看一个张量的实际布局时（定义为 TensorImpl 结构），会看到所有字段的一个公共前缀——我们认为所有类似「张量」的东西都会有；还有一些字段仅真正适用于有步幅的张量，但它们也很重要，所以我们将其保留在主结构中；然后可以在每个张量的基础上完成有自定义字段的后缀。比如稀疏张量可将其索引和值存储在这个后缀中。
 
### 自动梯度（autograd）
 
我已经说明了张量，但如果 PyTorch 仅有这点把戏，这就只不过是 Numpy 的克隆罢了。PyTorch 的显著特性是其在最初发布时就已提供对张量的自动微分（现在我们还有 TorchScript 等炫酷功能，但那时候就只有这个！）
 
自动微分是做啥？这是负责运行神经网络的机制：
- ![](https://pic2.zhimg.com/80/v2-f5e23bfa1dea5367c0d3fd2693d757f9_720w.jpg)
 
……以及填充实际计算你的网络的梯度时所缺少的代码：
- ![](https://pic4.zhimg.com/80/v2-c4a5fdaa3035cfaa5c4f3c1c3a73a863_720w.jpg)
 
花点时间看看这幅图。其中有很多东西需要解读，我们来看看：
*   首先将你的目光投向红色和蓝色的变量。PyTorch 实现了反向模式自动微分，这意味着我们可以「反向」走过前向计算来有效地计算梯度。查看变量名就能看到这一点：在红色部分的底部，我们计算的是损失（loss）；然后在这个程序的蓝色部分，我们所做的第一件事是计算 grad_loss。loss 根据 next_h2 计算，这样我们可以计算出 grad_next_h2。从技术上讲，我们加了 grad_ 的变量其实并不是梯度，它们实际上左乘了一个向量的雅可比矩阵，但在 PyTorch 中，我们就称之为 grad，基本上所有人都知道这是什么意思。
*   如果代码的结构保持一样，而行为没有保持一样：来自前向的每一行都被替换为一个不同的计算，其代表了前向运算的导数。举个例子，tanh 运算被转译成了 tanh_backward 运算（这两行用图左边一条灰线连接）。前向和反向运算的输入和输出交换：如果前向运算得到 next_h2，反向运算就以 grad_next_h2 为输入。
 
autograd 的意义就在于执行这幅图所描述的计算，但却不用真正生成这个源。PyTorch autograd 并不执行源到源的变换（尽管 PyTorch JIT 确实知道如何执行符号微分（symbolic differentiation））。
- ![](https://pic4.zhimg.com/80/v2-7569e7e29e486ec433ac05561677804b_720w.jpg)
 
要做到这一点，我们需要在张量上执行运算时存储更多元数据。让我们调整一下我们对张量数据结构的图：现在不只是一个指向存储的张量，我们还有一个包装这个张量的变量，而且也存储更多信息（AutogradMeta），这是用户在自己的 PyTorch 脚本中调用 loss.backward() 执行 autograd 时所需的。

这张幻灯片的内容在不久的将来就会过时。Will Feng 在简单融合了 PyTorch 的前端端口之后，正在推动 C++ 中变量和张量的融合：[https://github.com/pytorch/pytorch/issues/13638](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/issues/13638)。
 
我们也必须更新上面关于调度的图：
- ![](https://pic3.zhimg.com/80/v2-988995aebdd2ad936375ac78ebf9e082_720w.jpg)
 
在我们调度到 CPU 或 CUDA 实现之前，还有另一个对变量的调度，其负责打开（unwrap）变量，调用底层实现（绿色），然后再重新将结果包装进变量并为反向过程记录必需的 autograd 元数据。
 
某些实现不会 unwrap；它们只是调用其它变量实现。所以你可能要在变量宇宙中花些时间。但是，一旦你 unwrap 并进入了非变量张量宇宙，你就到达终点了；你再也不用退回变量（除非从你的函数返回）。
 
在我的纽约聚会演讲中，我跳过了以下七页幻灯片。对它们的文本介绍还要等一段时间。
 
- ![](https://pic2.zhimg.com/80/v2-e6c5031d9228be6b8c5027b7541a6941_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-0d0fe340af795be2ddeaf9fc8a06b637_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-f4b195ac1878091a9d8d5d36aa527883_720w.jpg)
- ![](https://pic1.zhimg.com/80/v2-9f5021c96a11f5d1e58041af2f6aef30_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-49c891574145d47790990be81fdc627b_720w.jpg)
- ![](https://pic3.zhimg.com/80/v2-9bf72d5867b3ed5f75c1163a486bcffe_720w.jpg)
- ![](https://pic2.zhimg.com/80/v2-2ae211b8a779315611524f861c73213d_720w.jpg)
 
## 工程开发
 
说够了概念，我们来看看代码。
 
### 找到你的路径
 
PyTorch 有大量文件夹，在 CONTRIBUTING.md 文档中有对它们的非常详细的描述，但实际上你只需知晓 4 个目录：
- ![](https://pic1.zhimg.com/80/v2-3fdf3a5e08851a39b2dc1e5a37e63bf4_720w.jpg)

*   首先，torch/ 包含你最熟悉的东西：你导入和使用的实际的 Python 模块。这些东西是 Python 代码而且易于操作（只需要进行修改然后查看结果即可）。但是，如果太过深入……
*   torch/csrc/：实现了你可能称为 PyTorch 前端的 C++ 代码。用更描述性的术语讲，它实现了在 Python 和 C++ 间转换的绑定代码（binding code）；另外还有一些相当重要的 PyTorch 部分，比如 autograd 引擎和 JIT 编译器。它也包含 C++ 前端代码。
*   aten/：这是「A Tensor Library」的缩写（由 Zachary DeVito 命名），是一个实现张量运算的 C++ 库。如果你检查某些核代码所处的位置，很可能就在 ATen。ATen 本身就分为两个算子区域：「原生」算子（算子的现代的 C++ 实现）和「传统」算子（TH、THC、THNN、THCUNN），这些是遗留的 C 实现。传统的算子是其中糟糕的部分；如果可以，请勿在上面耗费太多时间。
*   c10/：这是「Caffe2」和「A"Ten"」的双关语，包含 PyTorch 的核心抽象，包括张量和存储数据结构的实际实现。
 
找代码需要看很多地方；我们应该简化目录结构，就是这样。如果你想研究算子，你应该在 aten 上花时间。
 
我们看看在实践中是如何分离这些代码的：
- ![](https://pic1.zhimg.com/80/v2-009019cd631544337cad4bb9a4bbffd8_720w.jpg)
 
当你调用一个函数时，比如 torch.add，会发生什么？如果你记得我们的有关调度的讨论，你脑中应该已有了这些基础：
*   我们必须从 Python 国度转换到 C++ 国度（Python 参数解析）。
*   我们处理变量调度（VariableType—Type，顺便一提，和编程语言类型并无特别关联，只是一个用于执行调度的小工具）。
*   我们处理设备类型/布局调度（Type）。
*   我们有实际的核，这要么是一个现代的原生函数，要么是传统的 TH 函数。
 
其中每一步都具体对应于一些代码。让我们开路穿过这片丛林。
 
![](https://pic4.zhimg.com/80/v2-5beac1027d7b80501938675f01d81e23_720w.jpg)
 
我们在 C++ 代码中的起始着陆点是一个 Python 函数的 C 实现，我们已经在 Python 那边见过它，像是 torch.\_C.VariableFunctions.add。THPVariable\_add 就是这样一个实现。
 
对于这些代码，有一点很重要：这些代码是自动生成的。如果你在 GitHub 库中搜索，你没法找到它们，因为你必须实际 build PyTorch 才能看到它们。另外一点也很重要：你不需要真正深入理解这些代码是在做什么，你应该快速浏览它，知道它的功能。
 
我在上面用蓝色标注了最重要的部分：你可以看到这里使用了一个 PythonArgParser 类来从 Python args 和 kwargs 取出 C++ 对象；然后我们调用一个 dispatch_add 函数（红色内联）；这会释放全局解释器锁，然后调用在 C++ 张量自身上的一个普通的旧方法。在其回来的路上，我们将返回的 Tensor 重新包装进 PyObject。
 
（这里幻灯片中有个错误：我应该讲解变量调度代码。我这里还没有修复。某些神奇的事发生了，于是……）
- ![](https://pic4.zhimg.com/80/v2-1ec1bba8299e71dd66d78180e122b773_720w.jpg)
 
当我们在 Tensor 类上调用 add 方法时，还没有虚调度发生。相反，我有一个内联方法，其调用了一个内联方法，其会在「Type」对象上调用一个虚方法。这个方法是真正的虚方法（这就是我说 Type 只是一个让你实现动态调度的「小工具」的原因）。
 
在这个特定案例中，这个虚调用会调度到在一个名为 TypeDefault 的类上的 add 的实现。这刚好是因为我们有一个对所有设备类型（CPU 和 CUDA）都一样的 add 的实现；如果我们刚好有不同的实现，我们可能最终会得到 CPUFloatType::add 这样的结果。正是这种虚方法的实现能让我们最终得到实际的核代码。
 
也希望这张幻灯片很快过时；Roy Li 正在研究使用另一种机制替代 Type 调度，这能让我们更好地在移动端上支持 PyTorch。
 
值得再次强调，一直到我们到达核，所有这些代码都是自动生成的。
- ![](https://pic3.zhimg.com/80/v2-2b2f01593f6a7f655b112883ca882246_720w.jpg)
 
道路蜿蜒曲折，一旦你能基本上把握方向了，我建议你直接跳到核部分。
 
### 编写核（kernel）
 
PyTorch 为有望编写核的人提供了大量有用工具。在这一节我们会了解其中一些。但首先，编写核需要什么？
- ![](https://pic4.zhimg.com/80/v2-05de33363937928c105692c56d4554c7_720w.jpg)
 
我们一般将 PyTorch 中的核看作由以下部分组成：
 
*   首先有一些我们要写的有关核的元数据，这能助力代码生成并让你获取所有与 Python 的捆绑包，同时无需写任何一行代码。
*   一旦你到达了核，你就经过了设备类型/布局调度。你首先需要写的是错误检查，以确保输入的张量有正确的维度。（错误检查真正很重要！不要吝惜它！）
*   接下来，我们一般必须分配我们将要写入输出的结果张量。
*   该到写核的时候了。现在你应该做第二次 dtype 调度，以跳至其所操作的每个 dtype 特定的核。（你不应该过早做这件事，因为那样的话你就会毫无用处地复制在任何情况下看起来都一样的代码。）
*   大多数高性能核都需要某种形式的并行化，这样就能利用多 CPU 系统了。（CUDA 核是「隐式」并行化的，因为它们的编程模型构建于大规模并行化之上。）
*   最后，你需要读取数据并执行你想做的计算！
    
 
在后面的幻灯片中，我将介绍 PyTorch 中能帮你实现这些步骤的工具。
- ![](https://pic2.zhimg.com/80/v2-3a7e22aac7c39d5956b295ad37b9481d_720w.jpg)
 
要充分利用 PyTorch 的代码生成能力，你需要为你的算子写一个模式（schema）。这个模式能提供你的函数的 mypy 风格类型，并控制是否为 Tensor 上的方法或函数生成捆绑包。你还可以告诉模式针对给定的设备-布局组合，应该调用你的算子的哪种实现。
 
有关这种格式的更多信息，请参阅：[https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md)
- ![](https://pic4.zhimg.com/80/v2-7136af67a793fb520f3b3b61549952c3_720w.jpg)
 
你可能也需要为你在 derivatives.yaml 中的操作定义一个导数。
- ![](https://pic1.zhimg.com/80/v2-697b73fb84f4b295c78be40b1fd713b0_720w.jpg)
 
错误检查可以在低层 API 完成，也能通过高层 API 实现。低层 API 只是一个宏 TORCH_CHECK，其接收的是一个布尔值，然后还有任意数量的参数构成错误字符串（error string）以便得出结论看该布尔值是否为真。
 
这个宏有个很好的地方：你可以将字符串与非字符串数据混合起来；每一项都使用它们的 operator<< 实现进行格式化，PyTorch 中大多数重要的数据类型都有 operator<< 实现。
 
高层 API 能让你免于反复编写重复的错误消息。其工作方法是；你首先将每个张量包装为 TensorArg，这包含有关张量来处的信息（比如其参数名称）。然后它提供了一些预先装好的用于检查多种属性的函数；比如 checkDim() 测试的是张量的维度是否是一个固定数值。如果不是，该函数就基于 TensorArg 元数据提供一个用户友好的错误消息。
- ![](https://pic1.zhimg.com/80/v2-9777d783126b775be72ffc9b3ad96c98_720w.jpg)
 
在用 PyTorch 写算子时，有一点很重要：你往往要注册三个算子：abs\_out（其操作的是一个预分配的输出，其实现了 out= keyword 参数）、abs\_（其操作的是 inplace）、abs（这只是一个算子的普通的旧功能版本）。
 
大部分时间，abs\_out 是真正的主力，abs 和 abs\_ 只是围绕 abs_out 的薄弱 wrapper；但有时候也可为每个案例编写专门的实现。
- ![](https://pic4.zhimg.com/80/v2-5a61eb8540dbe8d65cb802860680b22f_720w.jpg)
 
要执行 dtype 调度，你应该使用 AT\_DISPATCH\_ALL_TYPES 宏。这会获取你想要进行调度操作的张量的 dtype，并还会为可从该宏调度的每个 dtype 指定一个 lambda。通常而言，这个 lambda 只是调用一个模板辅助函数。
 
这个宏不只是「执行调度」，它也会决定你的核将支持的 dtype。这样，这个宏实际上就有相当多一些版本，这能让你选取不同的 dtype 子集以生成特定结果。大多数时候，你只需要 AT\_DISPATCH\_ALL_TYPES，但也要关注你可能需要调度其它更多类型的情况。
- ![](https://pic3.zhimg.com/80/v2-ff270ff78311334460fc463e616cf752_720w.jpg)
 
在 CPU 上，你通常需要并行化你的代码。过去，这通常是通过直接在你的代码中添加 OpenMP pragma 来实现。
 
![](https://pic4.zhimg.com/80/v2-0564a9d4e55d03274a23d308b6fdccf7_720w.jpg)
 
某些时候，你必须真正访问数据。PyTorch 为此提供了相当多一些选择。
 
*   如果你只想获取某个特定位置的值，你应该使用 TensorAccessor。张量存取器就像是一个张量，但它将张量的维度和 dtype 硬编码为了模板参数。当你检索一个存取器时，比如 x.accessor();，我们会做一次运行时间测试以确保张量确实是这种格式；但那之后，每次存取都不会被检查。张量存取器能正确地处理步幅，因此你最好使用它们，而不是原始的指针访问（不幸的是，很多传统的核是这样做的）。另外还有 PackedTensorAccessor，这特别适用于通过 CUDA launch 发送存取器，这样你就能从你的 CUDA 核内部获取存取器。（一个值得一提的问题：TensorAccessor 默认是 64 位索引，这比 CUDA 中的 32 位索引要慢得多！）
*   如果你在用很常规的元素存取编写某种算子，比如逐点运算，那么使用远远更高级的抽象要好得多，比如 TensorIterator。这个辅助类能为你自动处理广播和类型提升（type promotion），相当好用。
*   要在 CPU 上获得真正的速度，你可能需要使用向量化的 CPU 指令编写你的核。我们也有用于这方面的辅助函数！Vec256 类表示一种标量向量，并提供了一些能在它们上一次性执行向量化运算的方法。然后 binary\_kernel\_vec 等辅助函数能让你轻松地运行向量化运算，然后结束那些没法用普通的旧指令很好地转换成向量指令的东西。这里的基础设施还能在不同指令集下多次编译你的核，然后在运行时间测试你的 CPU 支持什么指令，再在这些情况中使用最佳的核。
- ![](https://pic3.zhimg.com/80/v2-fc3c0a6d19882db22d0280ffbb2fd29a_720w.jpg)
 
PyTorch 中大量核都仍然是用传统的 TH 风格编写的。（顺便一提，TH 代表 TorcH。这是个很好的缩写词，但很不幸被污染了；如果你看到名称中有 TH，可认为它是传统的。）传统 TH 风格是什么意思呢？
 
*   它是以 C 风格书写的，没有（或很少）使用 C++。
*   其 refcounted 是人工的（使用了对 THTensor_free 的人工调用以降低你使用张量结束时的 refcounts）。
*   其位于 generic/ 目录，这意味着我们实际上要编译这个文件很多次，但要使用不同的 #define scalar_t

这种代码相当疯狂，而且我们讨厌回顾它，所以请不要添加它。如果你想写代码但对核编写了解不多，你能做的一件有用的事情：将某些 TH 函数移植到 ATen。
 
### 工作流程效率
 
![](https://pic1.zhimg.com/80/v2-8bbb39c785b6d86ceeb8652d19e39240_720w.jpg)
 
最后我想谈谈在 PyTorch 上的工作效率。如果 PyTorch 那庞大的 C++ 代码库是阻拦人们为 PyTorch 做贡献的第一只拦路虎，那么你的工作流程的效率就是第二只。如果你想用 Python 习惯开发 C++，那可能会很艰辛：重新编译 PyTorch 需要大量时间，你也需要大量时间才能知道你的修改是否有效。
 
如何高效工作本身可能就值得做一场演讲，但这页幻灯片总结了一些我曾见过某些人抱怨的最常见的反模式：「开发 PyTorch 很困难。」
 
*   如果你编辑一个 header，尤其是被许多源文件包含的 header（尤其当被 CUDA 文件包含时），可以预见会有很长的重新 build 时间。尽量只编辑 cpp 文件，编辑 header 要审慎！
*   我们的 CI 是一种非常好的零设置的测试修改是否有效的方法。但在获得返回信号之前你可能需要等上一两个小时。如果你在进行一种将需要大量实验的改变，那就花点时间设置一个本地开发环境。类似地，如果你在特定的 CI 配置上遇到了困难的 debug 问题，就在本地设置它。你可以将 Docker 镜像下载到本地并运行：[https://github.com/pytorch/ossci-job-dsl](https://link.zhihu.com/?target=https%3A//github.com/pytorch/ossci-job-dsl)
*   贡献指南解释了如何设置 ccache：[https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md%23use-ccache) ；强烈建议这个，因为这可以让你在编辑 header 时幸运地避免大量重新编译。当我们在不应该重新编译文件时重新编译时，这也能帮你覆盖我们的 build 系统的漏洞。
*   最后，我们会有大量 C++ 代码。如果你是在一台有 CPU 和 RAM 的强大服务器上 build，那么会有很愉快的体验。特别要说明，我不建议在笔记本电脑上执行 CUDA build。build CUDA 非常非常慢，而笔记本电脑往往性能不足，不足以快速完成。
    
 
### 参与进来！
 
![](https://pic4.zhimg.com/80/v2-0facf85450c2875e3acb821298f5411b_720w.jpg)
 
这就是我们旋风一般的 PyTorch 内核之旅了！其中省略了很多很多东西；但希望这里的描述和解释至少能帮你消化其代码库中相当大一部分。
 
接下来该做什么？你能做出怎样的贡献？我们的问题跟踪器是个开始的好地方：[https://github.com/pytorch/pytorch/issues](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/issues)。
 
从今年开始，我们一直在分类鉴别问题；标注有「triaged」的问题表示至少有一个 PyTorch 开发者研究过它并对该问题进行了初步评估。你可以使用这些标签找到我们认为哪些问题是高优先级的或查看针对特定模块（如 autograd）的问题，也能找到我们认为是小问题的问题。（警告：我们有时是错的！）
 
即使你并不想马上就开始写代码，也仍有很多其它有用的工作值得去做，比如改善文档（我很喜欢合并文档 PR，它们都很赞）、帮助我们重现来自其他用户的 bug 报告以及帮助我们讨论问题跟踪器上的 RFC。没有我们的开源贡献者，PyTorch 不会走到今天



# 结束


