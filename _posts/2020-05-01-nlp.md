---
layout: post
title:  "自然语言处理-NLP"
date:   2020-05-01 21:50:00
categories: 自然语言处理
tags: 深度学习 NLP 自然语言处理 文本摘要 唐杰 指代消解 省略补全 kenlm fasttext 文本分类 多模态 句法分析 OOV 分词 纠错
excerpt: NLP各类知识点及工程落地方法汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2021-9-21】[NLP为什么难系列](https://mp.weixin.qq.com/s/3f2XUn9959krRDohNfTLfg)
- 【2020-6-11】阿里员工开发的[论文知识图谱](https://www.connectedpapers.com/)
   - ![](http://p1.pstatp.com/large/pgc-image/37054b2db9b64394a73feecfa9ad024d)
- 【2020-9-5】[文本摘要综述](https://github.com/xcfcode/What-I-Have-Read/blob/master/slides/presentation/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E7%AE%80%E8%BF%B0.pdf)
   - ACL 2020论文：Extractive Summarization as Text Matching，[文本摘要新框架，抽取式摘要“轻松”取得SOTA](https://zhuanlan.zhihu.com/p/133096909)，打破原有的解决抽取式摘要的思路，这里提出了一个全新的范式：将抽取式摘要任务转化为一个语义匹配的问题。代码[MatchSum](https://github.com/maszhongming/MatchSum)，eight Tesla-V100-16G GPUs to train our model, the training time is about 30 hours
   - [Text Summarization on WikiHow](https://paperswithcode.com/sota/text-summarization-on-wikihow)，第一名[代码](https://github.com/nlpyang/PreSumm)
- 【2020-8-9】[NLP 中的各种不正确打开方式](https://www.bilibili.com/video/BV1ha4y1J71Q/)（反向模式 Anti-Pattern），总结的真好。[原文](https://docs.google.com/presentation/d/e/2PACX-1vSINTutKveUSVpj-FwpTz1Kkau7JC7d6ZKmGRqHyWf81Lj7dQkcr9dy9eL7HACD9NhvSH37lMtmqelh/pub?slide=id.g8a66028a04_0_50 )，也推荐作者的[博客](http://medium.com/modern-nlp)​​​​
- 【2021-1-3】Google 的AI团队在2020年底出品的《高性能自然语言处理》，综述了NLP的基础技术、核心技术、案例实践。
   - [EMNLP_2020_Tutorial__High_Performance_NLP](http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf)
- 【2021-1-23】Python Autocomplete：基于Transformer和LSTM的Python代码自动补全】’[Python Autocomplete](https://github.com/lab-ml/python_autocomplete) - Use Transformers and LSTMs to learn Python source code' by LabML
- 【2021-1-23】拼写纠错工具[autocorrect](https://github.com/fsondej/autocorrect)，基于seq2seq的文本纠错实现[deep-text-corrector](https://github1s.com/atpaino/deep-text-corrector)
- 【2021-8-20】[常见30种NLP任务的练手项目](https://zhuanlan.zhihu.com/p/51279338)，类似论文实现那样的demo级的，也不是传统的工程实现，用的方法一般比工业界的高端，非常适合练手用。涉及：
  - 分词 Word Segmentation: [chqiwang/convseg](https://github.com/chqiwang/convseg) ，基于CNN做中文分词，提供数据和代码。对应的论文Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation IJCNLP2017.
  - 词预测 Word Prediction: [Kyubyong/word_prediction](https://github.com/Kyubyong/word_prediction) ，基于CNN做词预测，提供数据和代码。
  - 文本蕴涵 Textual Entailment: [Steven-Hewitt/Entailment-with-Tensorflow](https://github.com/Steven-Hewitt/Entailment-with-Tensorflow)，基于Tensorflow做文本蕴涵，提供数据和代码。
  - 词性标注（POS）、 命名实体识别（NER）、 句法分析（parser）、 语义角色标注（SRL） 等。
    - [HIT-SCIR/ltp](https://github.com/HIT-SCIR/ltp)， 包括代码、模型、数据，还有详细的文档，而且效果还很好
  - 词干 Word Stemming: [snowballstem/snowball](https://github.com/snowballstem/snowball)， 实现的词干效果还不错。
  - 机器翻译 Machine Translation: [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)， 基于PyTorch的神经机器翻译，很适合练手。
  - 复述生成 Paraphrase Generation: [vsuthichai/paraphraser](https://github.com/vsuthichai/paraphraser)，基于Tensorflow的句子级复述生成，适合练手。
  - 关系抽取 Relationship Extraction: [ankitp94/relationship-extraction](https://github.com/ankitp94/relationship-extraction)，基于核方法的关系抽取。
  - 句子边界消歧 Sentence Boundary Disambiguation: [Orekhov/SentenceBreaking](https://github.com/Orekhov/SentenceBreaking)
  - 事件抽取 Event Extraction: [liuhuanyong/ComplexEventExtraction](https://github.com/liuhuanyong/ComplexEventExtraction)， 中文复合事件抽取，包括条件事件、因果事件、顺承事件、反转事件等事件抽取，并形成事理图谱。
  - 词义消歧 Word Sense Disambiguation: [alvations/pywsd](https://github.com/alvations/pywsd)
  - 命名实体消歧 Named Entity Disambiguation: [dice-group/AGDISTIS](https://github.com/dice-group/AGDISTIS)
  - 实体链接 Entity Linking: [hasibi/EntityLinkingRetrieval-ELR](https://github.com/hasibi/EntityLinkingRetrieval-ELR)
  - 指代消歧 Coreference Resolution: [huggingface/neuralcoref](https://github.com/huggingface/neuralcoref)
  - 复述检测 Paraphrase Detection 和 问答 Question Answering: [Paraphrase-Driven Learning for Open Question Answering](https://link.zhihu.com/?target=http%3A//knowitall.cs.washington.edu/paralex/)， 基于复述驱动学习的开放域问答。
  - 自动摘要 Automatic Summarisation: [PKULCWM/PKUSUMSUM](https://github.com/PKULCWM/PKUSUMSUM)，北大万小军老师团队的自动摘要方法汇总，包含了他们大量paper的实现，支持单文档摘要、多文档摘要、topic-focused多文档摘要。
  - 文本纠错 Text Correct: [atpaino/deep-text-corrector](https://github.com/atpaino/deep-text-corrector)，基于深度学习做文本纠错，提供数据和代码。
  - 音汉互译 Pinyin-To-Chinese: [Kyubyong/neural_chinese_transliterator](https://github.com/Kyubyong/neural_chinese_transliterator)，基于CNN做音汉互译
  - 字音转换 Grapheme to Phoneme: [cmusphinx/g2p-seq2seq](https://github.com/cmusphinx/g2p-seq2seq)，基于网红transformer做， 提供数据和代码。
  - 语言识别 Language Identification: [saffsd/langid.py](https://github.com/saffsd/langid.py)，语言识别比较好的开源工具
  - 语音识别 Automatic Speech Recognition: [buriburisuri/speech-to-text-wavenet](https://github.com/buriburisuri/speech-to-text-wavenet)，基于DeepMind WaveNet和Tensorflow做句子级语音识别。
  - 手语识别 Sign Language Recognition: [Home-SignAll](https://www.signall.us/)， 该项目在手语识别做的非常成熟

## 能力总结

- 【2021-2-27】[中文NLP实话实话](https://mp.weixin.qq.com/s/vsnkFr11i4rde7ixBrsIfg)：中文自然语言处理，目前在AI泡沫之下，真假难辨，实战技术与PPT技术往往存在着很大的差异。目前关于AI或者自然语言处理，做的人与讲的人往往是两回事。
- 深度学习在自然语言处理当中，除了在**分类**问题上能够取得较好效果外（如单选问题：情感分类、文本分类、正确答案分类问题等），在信息抽取上，尤其是在**元组**抽取上基本上是一塌糊涂，在工业场景下很难达到实用水准。
- 目前各种评测集大多是人为标注的，人为标注的大多为干净环境下的较为规范的文本，而且省略了真实生产环节中的多个环节。在评测环节中达到的诸多state-of-art方法，在真实应用场景下泛化能力很差，大多仅仅是为了刷榜而刷榜。
- 目前关于知识图谱的构建环节中，数据大多数都还是来自于结构化数据，半结构化信息抽取次之，非结构化数据抽取最少。半结构化信息抽取，即表格信息抽取最为危险，一个单元格错误很有可能导致所有数据都出现错误。非结构化抽取中，实体识别和实体关系识别难度相当大。
- 工业场景下**命名实体识别**，标配的BILSTM+CRF实际上只是辅助手段，工业界还是以领域实体字典匹配为主，大厂中往往在后者有很大的用户日志，这种日志包括大量的实体信息。因此，生产环节中的实体识别工作中，基础性词性的构建和扩展工作显得尤为重要。
- 目前关于知识图谱**推理**问题，严格意义上不属于推理的范畴，最多只能相当于是知识补全问题，如评测中的知识推理任务，是三元组补全问题。
- 目前舆情分析还是处于初级阶段。目前舆情分析还停留在以表层计量为主，配以浅层句子级情感分析和主题挖掘技术的分析。对于深层次事件演化以及对象级情感分析依旧还处于初级阶段。
  - 【2021-3-14】情感分析五要素：（entity，aspect，opinion，holder，time）, entity + aspect -> target
    - 示例：我觉得华为手机拍照非常牛逼。 → （华为手机，拍照，正面，我，\）
    - ![](https://p1.pstatp.com/large/tos-cn-i-0022/8bd0117028624224811facc091b2ded2)
- Bert本质上仅仅是个编码器，是word2vec的升级版而已，不是无所不能，仅仅是编码能力强，向量表示上语义更为丰富，然而大多人都装糊涂。
- 学界和业界最大的区别在于
  - 学术界以**探索前沿**为目的，提新概念，然后搭个草图就结束，目光并不长远，打完这一战就不知道下一战打什么，下一战该去哪里打，什么时候打，或者打一枪换个阵地再打。
  - 而工业界，往往面临着生存问题，需要考虑实际问题，还是以**解决实际问题**为主，因此没必要把学界的那一套理念融入到生产环节中，要根据实际情况制定自己的方法。
- 利用结构化数据，尤其是百科类infobox数据，采集下来，存入到Neo4j图数据库中，就称自己建立了知识图谱的做法是伪知识图谱做法。基于这类知识图谱，再搞个简单的问答系统，就标榜自己是基于知识图谱的智能问答，实际上很肤浅。
- 知识图谱不是结构化知识的可视化（不是两个点几条边）那么简单，那叫知识的可视化，不是知识图谱。知识图谱的核心在于知识的图谱化，特点在于知识的表示方法和图谱存储结构，前者决定了知识的抽象表示维度，后者决定了知识运行的可行性，图算法(图遍历、联通图、最短路径)。基于图谱存储结构，进行知识的游走，进行知识表征和未知知识的预测。
- 物以稀为贵，大家都能获取到的知识，往往价值都很低。知识图谱也是这样，只有做专门性的具有数据壁垒的知识图谱，才能带来商业价值。
- 目前智能问答，大多都是人工智障，通用型的闲聊型问答大多是个智障，多轮对话缺失，答非所问等问题层出不穷。垂直性的问答才是出路，但真正用心做的太少，大多都是处于demo级别。
- 大多数微信自然语言处理软文实际上都不可不看，纯属浪费时间。尤其是在对内容的分析上，大多是抓语料，调包统计词频，提取关键词，调包情感分析，做柱状图，做折线图，做主题词云，分析方法上千篇一律。应该从根本上去做方法上的创新，这样才能有营养，从根本上来说才能有营养可言。文本分析应该从浅层分析走向深层分析，更好地挖掘文本的语义信息。
- 目前百科类知识图谱的构建工作有很多，重复性的工作不少。基于开放类百科知识图谱的数据获取接口有复旦等开放出来，可以应用到基本的概念下实体查询，实体属性查询等，但目前仅仅只能做到一度。
- 基于知识图谱的问答目前的难点在于两个方面
  - 1）多度也称为多跳问题，如姚明的老婆是谁，可以走14条回答，但姚明的老婆的女儿是谁则回答不出来，这种本质上是实体与属性以及实体与实体关系的分类问题。
  - 2）多轮问答问题。多轮分成两种，一种是**指代补全**问答， 如前一句问北京的天气，后者省略“的天气”这一词，而只说“北京”，这个需要进行意图判定并准确加载相应的问答槽。另一种是**追问式**多轮问答，典型的在天气查询或者酒店预订等垂直性问答任务上。大家要抓住这两个方面去做。
- 关系挖掘是信息抽取的重要里程碑，理解了实体与实体、实体与属性、属性与属性、实体与事件、事件与事件的关系是解决真正语义理解的基础，但目前，这方面，在工业界实际运用中，特定领域中模板的性能要比深度学习多得多，学界大多采用端到端模型进行实验，在这方面还难以超越模版性能。

## NLP难点

【2021-8-21】[NLP≠NLU，机器学习无法理解人类语言](https://mp.weixin.qq.com/s/BYm54a6r1h4VqCSgxldsiQ)，详情见机器之心的[不同于NLP，数据驱动方法与机器学习无法攻克NLU，原因有三点](https://mp.weixin.qq.com/s/OWygqcLUa618Ys-wxsLrRw)：[英文原文](https://thegradient.pub/machine-learning-wont-solve-the-natural-language-understanding-challenge/)
- 长期以来，我们一直在与机器沟通：**编写代码--创建程序--执行任务**。然而，这些程序并非是用人类“自然语言“编写的，像Java、Python、C和C ++语言，始终考虑的是"**机器能够轻松理解和处理吗**？"
- “自然语言处理”(Natural Language Processing，NLP)的目的与此相反，它不是以**人类顺应机器**的方式学习与它们沟通，而是**让机器具备智力**，学习人类的交流方式。
- 自然语言处理，实际上是人工智能和语言学的交叉领域，但多年来，仅在语音转录、语音命令执行、语音关键词提取的工作上兢兢业业，规规矩矩，应用到人机交互，就显得十分吃力。NLP的核心步骤：分词→词干提取→词形还原→词性标注→命名实体识别→分块
- 因为在语料预处理阶段，NLP通常直接给出“断句”，比如 "订一张明天从北京到杭州的机票，国航头等舱"，经过NLP模型处理后，机器给出的输出出发地、目的地等信息，准确率高但这背后，我们并不知道机器理解了什么。由于足够好用，人们也就不多问了。在更加复杂的任务中，比如机器翻译，基于深度学习的编码、解码架构会将原句子转换成我们根本不熟悉的样子，也就是在无穷维空间中的点。研究人员试图向神经网络添加参数以提高它们在语言任务上的表现，然而，语言理解的根本问题是“<font color='red'>理解词语和句子下隐藏的含义“</font>
- 伦斯勒理工学院的两位科学家撰写了一本名为《人工智能时代语言学》的书，探讨了目前的人工智能学习方法在自然语言理解(Natural Language Understanding，NLU)中的瓶颈，并尝试探索更先进的智能体的途径。
- （1）AI必须从“**处理**”自然语言到“**理解**”自然语言
  - 机器“记录”了数据并不意味着“理解”了数据。近几十年来，机器学习算法一直尝试完成从NLP 到 NLU 的转型，机器学习曾长期承载着转型使命的荣光。机器学习模型是一种知识精益系统，它试图通过统计词语**映射**来回答上下文关系。在这些模型中，上下文是由词语序列之间的统计关系形成的，而非词语背后的含义。因而数据集越大、示例越多样化，机器对上下文关系的理解越精确。
  - 机器学习终将失宠，因为它们需要**太多的算力和数据**来自动设计特征、创建词汇结构和本体，以及开发将所有这些部分结合在一起的软件系统，而且机器人也不知道自己在做什么，以及为什么这样做。它们解决问题的方法不依赖与世界、语言或自身的互动（人类）。因此，它们无法理解两个人长时间对话中对同一件事情的描述越来越简短的情景，也就是文本缺失现象。巨大人工成本使机器学习陷入瓶颈，并迫使人们寻求其他方法来处理自然语言， 并导致了自然语言处理中经验主义范式（认为语言理解起源于感觉）的出现。
- 具有“感觉”的人工智能，或许会在自然语言处理上有三个突破：
  - 通过语言交流激活感觉模型，并以此承载记忆，从而可以应对人类之间交流时的文本缺失现象，实现“默契”（正确）的解码；
  - 理解语言的上下文相关含义，并从单词和句子的歧义中找到合适的理解，以及从感觉世界中寻找更强的约束和限制；
  - 向它们的人类合作者解释它们的想法、行动和决策；
- 机器也需要在世界互动时保持终身学习。而机器学习由于将可压缩性和可学习性对等起来，并且限于表面的符号统计关系理解，以及不可解释性等原因，不可避免丢失背景信息，而做不到上述层次的理解。总之，机器要理解自然语言，感觉经验是必不可少的。
- 书中指出：“ 语言理解不能与**整体的认知过程**区分开来，启发机器人理解语言也要运用其他感知（例如视觉、触觉）。”正如在现实世界中，人类也是利用他们丰富的体态动作来填补语言表达的空白。
- NLP 中广泛使用数据驱动的经验方法有以下原因：**符号和逻辑方法未能产生可扩展的 NLP 系统**，导致 NLP （`EMNLP`，指数据驱动、基于语料库的短语，统计和机器学习方法）中所谓的经验方法的兴起。EMNLP的先驱之一肯尼思·丘奇（Kenneth Church）所解释的，NLP数据驱动和统计方法的拥护者对解决简单的语言任务感兴趣，其动机从来不是暗示语言就是这样运作的，而是"做简单的事情总比什么都不做好"。这种转变的动机被严重误解，他们以为这个“**可能大致正确的**”（  Probably Approximately Correct ，`PAC`）范式将扩展到完全自然的语言理解。
- 是时候重新思考NLU方法了，对 NLU 的"大数据"方法不仅在心理上、认知上甚至计算上都是难以操作的，而且这种盲目的数据驱动 NLU 方法在理论和技术上也有缺陷。
  - “新一代和当代的NLP研究人员在语言学和NLP的理解上有差别，因此，这种被误导的趋势导致了一种不幸的状况：坚持使用"大语言模型"（LLM）构建NLP系统，这需要巨大的计算能力，并试图通过记住海量数据来接近自然语言。
  - 这几乎是徒劳的尝试。我们认为，这种伪科学方法不仅浪费时间和资源，而且引诱新一代年轻科学家认为语言只是数据。更糟糕的是，这种方法会阻碍自然语言理解（NLU）的任何真正进展。
- 为什么NLP与NLU不同？
- **语言理解不承认任何程度的误差**，它们要充分理解一个话语或一个问题。
  - 常见的"下游 NLP"任务：综述--主题提取--命名实体识别（NER）--（语义）搜索--自动标记--聚类
  - 上述所有任务都符合所有机器学习方法的基础**可能大致正确**（PAC） 范式。
  - 对普通口语的真正理解与单纯的文本（或语言）处理是完全不同的问题。在文本（或语言）处理中，我们可以接受近似正确的结果。

### NLU为什么难

为什么 NLU 很困难：
- （1）**文本容易丢失**："缺失文本现象"（MTP）自然语言理解中所有挑战的核心。
  - 场景：演讲者将思想“编码”为某种自然语言中的话语，然后听众将话语“解码”为演讲者打算/希望传达的思想。"解码"过程是NLU中的"U"--即理解话语背后的思想。
  - MTP：媒体传输协议，Media Transfer Protocol
- （2）"解码"过程中需要没有任何误差，才能从说话者的话语中，找出唯一一种意在传达的含义。这正是NLU困难的原因。
  - 两种优化通信的方案：
    - 1.说话者可以压缩（和最小化）在思想编码中发送的信息量，并且听者做一些额外的工作解码（解压缩）话语；
    - 2.演讲者多做一部分工作，把所有想要传达的思想信息告诉听者，减少听者的工作量；
- NLU的问题所在：机器不知道我们遗漏了什么，它们不知道我们都知道什么。如果它们不能以某种方式"整理"我们话语的所有的含义，那么软件程序将永远不能完全理解我们话语背后的想法。
  - 为了有效地沟通，人们在交流中通常**不会说对方知道的信息**。这也正是为什么都倾向于忽略相同的信息——因为都了解每个人都知道的，而这正是所谓的**共同背景知识**。人类在大约 20 万年的进化过程中，发展出的这一天才优化过程非常有效。
  - NLU的挑战，并不是解析，阻止，POS标记，命名实体识别等, 而是解释或揭示**那些缺失的信息**。并隐含地假定为**共享和共同的背景知识**。
- 在此背景下，整理了三个原因，说明为什么机器学习和数据驱动的方法不会为自然语言理解提供解决方案。
  - 1、ML 方法甚至与 NLU 无关：ML 是压缩的，语言理解需要**解压缩**：
    - 机器的自然语言理解由于MTP而变得困难，因为日常口语被高度压缩，因此"理解"的挑战在于未压缩（或发现）缺失的文本。
    - MTP 现象正是为什么数据驱动和机器学习方法虽然在某些 NLP 任务中可能很有用，但与 NLU 甚至不相关的原因。
    - 机器可学习性（ML） 和可压缩性（COMP）之间的等价性已在数学上建立。即已经确定，只有在数据高度可压缩（未压缩的数据有大量冗余）时，才能从数据集中学习，反之亦然。
    - 机器学习是关于发现将大量数据概括为单一函数。另一方面，由于MTP，自然语言理解需要智能的"不压缩"技术，可以发现所有缺失和隐含的假定文本。因此，机器学习和语言理解是不兼容的——事实上，它们是矛盾的。
  - 2、ML 方法甚至与 NLU 无关：统计上的**无意义**
    - ML 本质上是一种基于在数据中找到一些**模式**（**相关性**）的范式。因此，该范式的希望是在捕捉自然语言中的各种现象时，发现它们存在统计上的显着差异。
    - 但是当"小"和"大"（或"打开"和"关闭"等）等反义词/反义词以相同的概率出现在相同的上下文中，**统计分析不能建模**（甚至不能近似）语义——就这么简单！
    - 有人说，收集足够的例子，一个系统可以确立统计学意义。但是，需要多少个示例来"学习"如何解决结构中的引用呢？机器学习/数据驱动系统需要看到上述 40000000 个变体，以学习如何解决句子中的引用。如果有的话，这在计算上是不可信的。正如Fodor和Pylyshyn曾经引用著名的认知科学家乔治.米勒（ George Miller），为了捕捉 NLU 系统所需的所有句法和语义变化，神经网络可能需要的特征数量超过宇宙中的原子数量！这里的寓意是：**统计无法捕捉（甚至不能近似）语义**。
  - 3、ML 方法甚至与 NLU 无关：**意图**
    - 逻辑学家们长期以来一直在研究一种语义概念，试图用语义三角形（**符号**、**概念**和**语义**）解释什么是"内涵"。
    - 一个符号用来指代一个概念，概念可能有实际的对象作为实例，但有些概念没有实例，例如，神话中的独角兽只是一个概念，没有实际的实例独角兽。类似地，"被取消的旅行"是对实际未发生的事件的引用，或从未存在的事件等。
    - 每个"事物"（或认知的每一个对象）都有三个部分：
      - 一个符号，符号所指的概念以及概念具有的具体实例。
      - 一个概念（通常由某个符号/标签所指）是由一组属性和属性定义，也许还有额外的公理和既定事实等。
      - 然而，概念与实际（不完美）实例不同，在数学世界中也是如此。
- 内涵决定外延，但外延本身并不能完全代表概念。
  - 自然语言充斥着内涵现象，因为语言具有不可忽视的内涵。但是机器学习/数据驱动方法的所有变体都纯粹是延伸的——它们以物体的数字（矢量/紧张）表示来运作，而不是它们的象征性和结构特性，因此在这个范式中，我们不能用自然语言来模拟各种内涵。顺便说一句，神经网络纯粹是延伸的，因此不能表示内涵，这是它们总是容易受到对抗性攻击的真正原因

人类在传达自己的想法时，其实是在传递高度压缩的语言表达，需要用大脑来解释和"揭示"所有缺失但隐含假设的背景信息。
语言是承载思想的人工制品，因此，在构建越来越大的语言模型时，机器学习和数据驱动方法试图在尝试找到数据中甚至不存在的东西时，徒劳地追逐无穷大。

【2021-8-21】[NLP≠NLU，机器学习无法理解人类语言](https://mp.weixin.qq.com/s/BYm54a6r1h4VqCSgxldsiQ)：[英文原文](https://thegradient.pub/machine-learning-wont-solve-the-natural-language-understanding-challenge/)


### 如何处理OOV

[NLP 研究主流目前如何处理 out of vocabulary words？](https://www.zhihu.com/question/55172758)

解决OOV问题的集中方法
- 更换更复杂的字符级模型，字符有限，以单词为单位为导致组合爆炸
- OOV单词较少时，可以直接替换成<UNK>标签来忽略OOV的问题，尽管实际应用时候并不想这样————总不能给用户输出一个<UNK>吧？

如果OOV单词占比较多（如30%），改怎么办？对数据的处理可操作性更强效果也是特别直观地好。
1.  **Mixed Word/Character Model** 字符模型
- 即把所有的OOV词，拆成**字符**。比如 Jessica，变成<B>_J，<M>_e，<M>s，<M>s，<M>i，<M>c，<E>a。其中<B><M><E>是Begin，Middle，End的标记。这样处理的好处就是消灭了全部的OOV。坏处就是文本序列变得非常长，对于性能敏感的系统，这是难以接受的维度增长。
2. **Wordpiece Model（WPM）** 拆成<font color='red'>子词</font>
- 和上面一样，同样要进行拆词。不同的是，非OOV的词也要拆，并且**非字符**粒度，而是sub-word 子词。还是 Jessica，变成<B>Je，<M>ssi，<E>ca。这类方法最早应用于Google的语音识别系统，现在已经在NLP中遍地开花了。拆词规则可以从语料中自动统计学习到，常用的是`BPE`（Byte Pair Encode）编码，出处在[《Neural Machine Translation of Rare Words with Subword Units》](https://arxiv.org/abs/1508.07909)。
- 和第一种方法相比，虽然序列的长度控制住了，但是在有限词表的情况下，**OOV仍然存在**。
- 另外，sub-word的OOV有一种麻烦，对于Jessica的例子，即使只有<M>ssi是OOV，<B>Je和<E>ca都在词表内，整个Jessica的**单词仍然无法正确表示**。
3. **UNK处理**
- 在训练数据充足的情况下，RNN模型可以轻松支持30k-80k的词表。在大多数情况下，**扩大词表都是首选的方案**。经过WPM处理后，配合词表加大，剩下的OOV都是冷门的长尾词。如果不关注这部分性能，可以直接扔掉OOV词，删掉包含OOV的数据。对于分类型任务，就全部替换成<UNK>标签。对于生成型任务，有不同的细节处理方案，可以看下经典的[《Addressing the Rare Word Problem in Neural Machine Translation》](https://arxiv.org/abs/1410.8206)，里面介绍了Copyable、PosALL和PosUNK三种替换策略。这类策略对于实体类NER的词，有比较好的效果。
4. **中文的处理**
- 英文中包含的姓名、复合词、同源词和外来词，使得WPM的方法效果拔群。
- 在处理中文时，WPM可以有效帮助解决混入的**英文词和阿拉伯数字**等。对于纯中文的句子，分割成子词的**意义不大**。
- 这时候，**扩大词表**仍然是首选。
5. **扩大词表**
- **终极**解决办法。通常情况不使用大词表，一方面是因为训练数据的多样性有限，另一方面是softmax的计算速度受限。对于第一种情况，扩大语料范围。对于第二种情况，相关的加速策略可以将词表扩大10倍而GPU上的预测速度只降低一半（从5W词到50W词）。比如[《On Using Very Large Target Vocabulary for Neural Machine Translation》](https://arxiv.org/abs/1412.2007v1)。tensorflow中有对应的实现 tf.nn.sampled_softmax_loss
6. 模型
- 构建一个固定词表的同时，维护一个动态的词表。这种机制一般称为**Copy/Pointer Mechanism**。这个动态的词表，事实上就是source input里面的词copy过来的。动态词表里面一般是一些罕见词，而固定的词表是你基于某种规则构建你的词表（比如：出现频数大于5）。这样的方法在十分优雅，两个不同词表的合作，就像GPU的缓存和内存之间的合作。



# NLP简介

## NLP基础任务

- `词法分析`（Lexical Analysis）：对自然语言进行词汇层面的分析，是NLP基础性工作
  - **分词**（Word Segmentation/Tokenization）：对没有明显边界的文本进行切分，得到词序列
  - **新词发现**（New Words Identification）：找出文本中具有新形势、新意义或是新用法的词
  - **形态分析**（Morphological Analysis）：分析单词的形态组成，包括词干（Sterms）、词根（Roots）、词缀（Prefixes and Suffixes）等
  - **词性标注**（Part-of-speech Tagging）：确定文本中每个词的词性。词性包括动词（Verb）、名词（Noun）、代词（pronoun）等
  - **拼写校正**（Spelling Correction）：找出拼写错误的词并进行纠正
- `句子分析`（Sentence Analysis）：对自然语言进行句子层面的分析，包括句法分析和其他句子级别的分析任务
  - **组块分析**（Chunking）：标出句子中的短语块，例如名词短语（NP），动词短语（VP）等
  - **超级标签标注**（Super Tagging）：给每个句子中的每个词标注上超级标签，超级标签是句法树中与该词相关的树形结构
  - **成分句法分析**（Constituency Parsing）：分析句子的成分，给出一棵树由终结符和非终结符构成的句法树
  - **依存句法分析**（Dependency Parsing）：分析句子中词与词之间的依存关系，给一棵由词语依存关系构成的依存句法树
  - **语言模型**（Language Modeling）：对给定的一个句子进行打分，该分数代表句子合理性（流畅度）的程度
  - **语种识别**（Language Identification）：给定一段文本，确定该文本属于哪个语种
  - **句子边界检测**（Sentence Boundary Detection）：给没有明显句子边界的文本加边界
- `语义分析`（Semantic Analysis）：对给定文本进行分析和理解，形成能勾够表达语义的形式化表示或分布式表示
  - **词义消歧**（Word Sense Disambiguation）：对有歧义的词，确定其准确的词义
  - **语义角色标注**（Semantic Role Labeling）：标注句子中的语义角色类标，语义角色，语义角色包括施事、受事、影响等
  - **抽象语义表示分析**（Abstract Meaning Representation Parsing）：AMR是一种抽象语义表示形式，AMR parser把句子解析成AMR结构
  - **一阶谓词逻辑演算**（First Order Predicate Calculus）：使用一阶谓词逻辑系统表达语义
  - **框架语义分析**（Frame Semantic Parsing）：根据框架语义学的观点，对句子进行语义分析
  - 词汇/句子/段落的**向量化表示**（Word/Sentence/Paragraph Vector）：研究词汇、句子、段落的向量化方法，向量的性质和应用
- `信息抽取`（Information Extraction）：从无结构文本中抽取结构化的信息
  - **命名实体识别**（Named Entity Recognition）：从文本中识别出命名实体，实体一般包括人名、地名、机构名、时间、日期、货币、百分比等
  - **实体消歧**（Entity Disambiguation）：确定实体指代的现实世界中的对象
  - **术语抽取**（Terminology/Giossary Extraction）：从文本中确定术语
  - **共指消解**（Coreference Resolution）：确定不同实体的等价描述，包括代词消解和名词消解
  - **关系抽取**（Relationship Extraction）：确定文本中两个实体之间的关系类型
  - **事件抽取**（Event Extraction）：从无结构的文本中抽取结构化事件
  - **情感分析**（Sentiment Analysis）：对文本的主观性情绪进行提取
  - **意图识别**（Intent Detection）：对话系统中的一个重要模块，对用户给定的对话内容进行分析，识别用户意图
  - **槽位填充**（Slot Filling）：对话系统中的一个重要模块，从对话内容中分析出于用户意图相关的有效信息
- `顶层任务`（High-level Tasks）：直接面向普通用户，提供自然语言处理产品服务的系统级任务，会用到多个层面的自然语言处理技术
  - **机器翻译**（Machine Translation）：通过计算机自动化的把一种语言翻译成另外一种语言
  - **文本摘要**（Text summarization/Simplication）：对较长文本进行内容梗概的提取
  - **问答系统**（Question-Answering Systerm）：针对用户提出的问题，系统给出相应的答案
  - **对话系统**（Dialogue Systerm）：能够与用户进行聊天对话，从对话中捕获用户的意图，并分析执行
  - **阅读理解**（Reading Comprehension）：机器阅读完一篇文章后，给定一些文章相关问题，机器能够回答
  - **自动文章分级**（Automatic Essay Grading）：给定一篇文章，对文章的质量进行打分或分级
参考：
- [原文链接](https://blog.csdn.net/lz_peter/article/details/81588430)，[视频介绍](https://edu.csdn.net/course/play/8673)


## 工具汇总

- 【2020-8-27】NLP模型可视化工具LIT，为什么模型做出这样的预测？什么时候性能不佳？在输入变化可控的情况下会发生什么？LIT 将局部解释、聚合分析和反事实生成集成到一个流线型的、基于浏览器的界面中，以实现快速探索和错误分析。
   - [谷歌开源NLP模型可视化工具LIT，模型训练不再「黑箱」](https://www.toutiao.com/i6865152251150008844/)
   - 支持多种自然语言处理任务，包括探索情感分析的反事实、度量共指系统中的性别偏见，以及探索文本生成中的局部行为。
   - 此外 LIT 还支持多种模型，包括分类、seq2seq 和结构化预测模型。并且它具备高度可扩展性，可通过声明式、框架无关的 API 进行扩展。
   - [论文地址](https://arxiv.org/pdf/2008.05122.pdf)
   - [项目地址](https://github.com/PAIR-code/lit)，[模块操作细节](https://github.com/PAIR-code/lit/blob/main/docs/user_guide.md)
   - ![](https://p6-tt.byteimg.com/origin/pgc-image/a8703a8db04649b19e12ef152783bb4a?from=pc)
- 【2021-7-26】[5个流行的自然语言处理库及入门用法](https://www.toutiao.com/i6989075362323989024/)
  - Hugging Face Datasets: Hugging Face 的 Datasets 库本质上是一个对公开可用的 NLP 数据集的打包集合，带有一组通用的 API 和数据格式，以及一些辅助功能。Datasets 提供了两大特性：用于许多公共数据集的单行数据加载器，以及高效的数据预处理。安装：pip install datasets
  - TextHero：在其 GitHub 存储库中的介绍很简单：文本预处理、表示和可视化，助你从零迈向大师。安装：pip install texthero
  - spaCy: 专门设计的，其宗旨是成为一个用于实现生产就绪系统的有用库。
  - Hugging Face Transformers：Hugging Face 的 Transformers 库已成为 NLP 实践不可或缺的一部分，Transformers 提供了数千个预训练模型，可以对 100 多种语言的文本执行分类、信息提取、问答、摘要、翻译、文本生成等任务。它的目标是让所有人都更容易使用尖端的 NLP 技术。Transformers由两个最流行的深度学习库 PyTorch 和 TensorFlow 提供支持，它们之间无缝集成，允许你使用一个模型来训练你的模型，然后加载它来推理另一个。

```python
# pip install texthero
def text_texthero():
	import texthero as hero
	import pandas as pd
	df = pd.read_csv("https://github.com/jbesomi/texthero/raw/master/dataset/bbcsport.csv")
	df['pca'] = (
		df['text']
			.pipe(hero.clean)
			.pipe(hero.tfidf)
			.pipe(hero.pca)
		)
	hero.scatterplot(df, 'pca', color='topic', title="PCA BBC Sport news")
# pip install spacy
# python -m spacy download en # 加载语言模型
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
nlp = spacy.load('en')
doc = nlp(sample)

print("Tokens:\n=======)
for token in doc:
    print(token)

print("Stop words:\n===========")
for word in doc:
    if word.is_stop == True:
        print(word)

print("POS tagging:\n============")
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
          token.shape_, token.is_alpha, token.is_stop)

print("Named entities:\n===============")
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

# pip install transformers
from transformers import pipeline
# Allocate a pipeline for sentiment-analysis
classifier = pipeline('sentiment-analysis')
# Classify text
print(classifier('I am a fan of KDnuggets, its useful content, and its helpful editors!'))
# [{'label': 'POSITIVE', 'score': 0.9954679012298584}]
# Allocate a pipeline for question-answering
question_answerer = pipeline('question-answering')
# Ask a question
answer = question_answerer({
	'question': 'Where is KDnuggets headquartered?',
	'context': 'KDnuggets was founded in February of 1997 by Gregory Piatetsky in Brookline, Massachusetts.'
})
# Print the answer
print(answer)
# {'score': 0.9153624176979065, 'start': 66, 'end': 90, 'answer': 'Brookline, Massachusetts'}
```


## 发展历史

- 微软亚洲研究院成立20周年时表示：[NLP将迎来黄金十年](https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-nlp)。
   - 比尔·盖茨曾说过，“语言理解是人工智能皇冠上的明珠”。自然语言处理（NLP，Natural Language Processing）的进步将会推动人工智能整体进展。
   - NLP的历史几乎跟计算机和人工智能（AI）的历史一样长。
- 回顾基于深度学习的NLP技术的重大进展，从时间轴来看主要包括：
   - NNLM(2003)
   - Word Embeddings(2013)
   - Seq2Seq(2014)
   - Attention(2015)
   - Memory-based networks(2015)
   - Transformer(2017)
   - BERT(2018)
   - XLNet(2019)
- 预训练语言模型发展历史
   - 摘自：[nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)
![](https://pic3.zhimg.com/80/v2-d8fd47547f5a8230372ceaa894e52feb_720w.jpg)

- 【2021-1-17】Impressive progress of deep learning on unsupervised text corpora，[A Review of the Neural History of Natural Language Processing](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
   - 2001 ： Neural language models
   - 2008 ： Multi-task learning
   - 2013 ： Word embeddings，Neural networks for NLP开启
   - 2014 ： Sequence-to-sequence models
   - 2015 ： Attention及Memory-based networks
   - 2018 ： Pretrained language models，BERT系列

# 算法理论


## 自监督学习

- 【2020-6-21】[NLP中的自监督表示学习，全是动图，很过瘾的](https://www.toutiao.com/i6839892851711541764/)，[英文原文](https://amitness.com/2020/05/self-supervised-learning-nlp/)
- 自监督的方法的核心是一个叫做 “pretext task” 的框架，它允许我们使用数据本身来生成标签，并使用监督的方法来解决非监督的问题。这些也被称为“auxiliary task”或“pre-training task“。通过执行此任务获得的表示可以用作我们的下游监督任务的起点。
   - ![](http://p3.pstatp.com/large/pgc-image/47ba10919c1440c781c0b14f1c14de82)
- 【2020-7-12】刘知远新书：[Representation Learning for Natural Language Processing](http://nlp.csai.tsinghua.edu.cn/news/%E4%B8%93%E8%91%97representation-learning-for-natural-language-processing%E6%AD%A3%E5%BC%8F%E5%87%BA%E7%89%88/)，[电子版下载](https://link.springer.com/book/10.1007%2F978-981-15-5573-2)

- 【2020-7-21】自监督学习综述（清华唐杰团队）：Self-supervised Learning: Generative or Contrastive
<iframe src="https://view.officeapps.live.com/op/embed.aspx?src=https%3A%2F%2Fstatic%2Eaminer%2Ecn%3A443%2Fupload%2Fppt%2F332%2F651%2F1679%2F5ee8986f91e011e66831c59b%5F1%2Epptx&amp;wdAr=1.4440104166666667" width="700px" height="500px" frameborder="0">这是嵌入 <a target="_blank" href="https://office.com">Microsoft Office</a> 演示文稿，由 <a target="_blank" href="https://office.com/webapps">Office</a> 提供支持。</iframe>


### 自监督的方案

- 1. 预测中心词
   - 在这个公式中，我们取一定窗口大小的一小块文本，我们的目标是根据周围的单词预测中心单词。
   - ![](http://p1.pstatp.com/large/pgc-image/b217d92b952d4601937a1629bc867642)
   - 例如，在下面的图中，我们有一个大小为1的窗口，因此我们在中间单词的两边各有一个单词。使用这些相邻的词，我们需要预测中心词。
   - ![](http://p9.pstatp.com/large/pgc-image/62516862aa444fed9885c34aaf955b05)
   - 这个方案已经在著名的Word2Vec论文的“Continuous Bag of Words”方法中使用过。

- 2. 预测邻居词
   - 在这个公式中，我们取一定窗口大小的文本张成的空间，我们的目标是在给定中心词的情况下预测周围的词。
   - ![](http://p3.pstatp.com/large/pgc-image/63df64f4f6af401db506372dc06c0b2f)
   - 这个方案已经在著名的Word2Vec论文的“skip-gram”方法中实现。

- 3. 相邻句子的预测
   - 在这个公式中，我们取三个连续的句子，设计一个任务，其中给定中心句，我们需要生成前一个句子和下一个句子。它类似于之前的skip-gram方法，但适- 用于句子而不是单词。
   - ![](http://p1.pstatp.com/large/pgc-image/6c88f0845d7e4ae38ea626bb3bfd9000)
   - 这个方案已经在Skip-Thought Vectors的论文中使用过。

- 4. 自回归语言建模
   - 在这个公式中，我们取大量未标注的文本，并设置一个任务，根据前面的单词预测下一个单词。因为我们已经知道下一个来自语料库的单词是什么，所以- 我们不需要手工标注的标签。
   - ![](http://p1.pstatp.com/large/pgc-image/bf36697cfb06464c8faf6ab3c88a2493)
   - 例如，我们可以通过预测给定前一个单词的下一个单词来将任务设置为从左到右的语言建模。
   - ![](http://p3.pstatp.com/large/pgc-image/1ddb11e50efa4a6f955bac7e14218b1b)
   - 我们也可以用这个方案来通给定未来的单词预测之前的单词，方向是从右到左。
   - ![](http://p1.pstatp.com/large/pgc-image/bbb35ee25289497a9c40e3eeee901fe0)
   - 这个方案已经使用在许多论文中，从n-gram模型到神经网络模型比如神经概率语言模型 (GPT) 。

- 5. 掩码语言建模
   - 在这个方案中，文本中的单词是随机掩码的，任务是预测它们。与自回归公式相比，我们在预测掩码单词时可以同时使用前一个词和下一个词的上下文。
   - ![](http://p1.pstatp.com/large/pgc-image/a9b530083051422f9f88c1613eff489f)
   - 这个方案已经在BERT、RoBERTa和ALBERT的论文中使用过。与自回归相比，在这个任务中，我们只预测了一小部分掩码词，因此从每句话中学到的东西更少。

- 6. 下一个句子预测
   - 在这个方案中，我们取文件中出现的两个连续的句子，以及同一文件或不同文件中随机出现的另一个句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8d58e6c913dc445d895429086cc50ae9)
   - 然后，任务是区分两个句子是否是连贯的。
   - ![](http://p3.pstatp.com/large/pgc-image/f50ff4fe179a4678926ddcc3769289f0)
   - 在BERT的论文中，它被用于提高下游任务的性能，这些任务需要理解句子之间的关系，比如自然语言推理(NLI)和问题回答。然而，后来的研究对其有效性提出了质疑。

- 7. 句子顺序的预测
   - 在这个方案中，我们从文档中提取成对的连续句子。然后互换这两个句子的位置，创建出另外一对句子。
   - ![](http://p3.pstatp.com/large/pgc-image/0638806390584a488e06c82a11832455)
   - 我们的目标是对一对句子进行分类，看它们的顺序是否正确。
   - ![](http://p3.pstatp.com/large/pgc-image/2ba2388f8dfa4778ba4684a8bbbc57fe)
   - 在ALBERT的论文中，它被用来取代“下一个句子预测”任务。

- 8. 句子重排
   - 在这个方案中，我们从语料库中取出一个连续的文本，并破开的句子。然后，对句子的位置进行随机打乱，任务是恢复句子的原始顺序。
   - ![](http://p1.pstatp.com/large/pgc-image/451d926ff39a4ce6b3f2ef4b97d18469)
   - 它已经在BART的论文中被用作预训练的任务之一。

- 9. 文档旋转
   - 在这个方案中，文档中的一个随机token被选择为旋转点。然后，对文档进行旋转，使得这个token成为开始词。任务是从这个旋转的版本中恢复原来的句子。
   - ![](http://p3.pstatp.com/large/pgc-image/8095d77a0fc440978eb825289a83acf6)
   - 它已经在BART的论文中被用作预训练的任务之一。直觉上，这将训练模型开始识别文档。

- 10. 表情符号预测
   - 这个方案被用在了DeepMoji的论文中，并利用了我们使用表情符号来表达我们所发推文的情感这一想法。如下所示，我们可以使用推特上的表情符号作为标签，并制定一个监督任务，在给出文本时预测表情符号。
   - ![](http://p1.pstatp.com/large/pgc-image/501d4092a3c4433298824a230c07eeb1)
   - DeepMoji的作者们使用这个概念对一个模型进行了12亿条推文的预训练，然后在情绪分析、仇恨语言检测和侮辱检测等与情绪相关的下游任务上对其进行微调。

## 机器翻译

- 【2018-10】[独家：“论文致谢刷屏”博士黄国平演讲干货](https://mp.weixin.qq.com/s/RYnJnkz-55qj94hyy4zm2Q),QCon 全球软件开发大会 2018 上海站的演讲[视频](https://time.geekbang.org/dailylesson/detail/100020790)
- 【2020-6-5】[机器翻译：统计建模与深度学习方法](https://opensource.niutrans.com/mtbook/index.html)，[ppt地址](https://github.com/NiuTrans/MTBook/blob/master/slides)
- ![](https://opensource.niutrans.com/guideline.png)
- 【2020-6-10】Google官方示例：[基于注意力的神经机器翻译](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh-cn)
   - ![](https://tensorflow.org/images/spanish-english.png)
- 【2021-1-13】翻车的机器翻译
  - 大数据文摘：[机器翻译古文也翻车？读了20次“苟富贵勿相忘”后，谷歌：没钱的人总会被遗忘](https://mp.weixin.qq.com/s/E2VESXhJLaNmJMlp84sXaA)
  - [谷歌翻译20次鲁迅《狂人日记》中的经典“吃人”片段！极度生草](https://www.bilibili.com/video/BV1nK4y1r75x/?spm_id_from=333.788.recommend_more_video.1)
  - [谷歌翻译20次司马迁《陈涉世家》！ 清朝，瑞士，东罗马，曹魏竟在同一时代](https://www.bilibili.com/video/BV1Jf4y1C7oP?from=search&seid=7681248349324754656)

<iframe src="//player.bilibili.com/player.html?aid=288370813&bvid=BV1Jf4y1C7oP&cid=271241642&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"> </iframe>

- 【2021-1-22】【LibreTranslate：可完全本地化部署的开源机器翻译API服务，基于Argos Translate】’[LibreTranslate](https://github.com/uav4geo/LibreTranslate) - Free and Open Source Machine Translation API. 100% self-hosted, no limits, no ties to proprietary services. Built on top of Argos Translate.' by UAV4GEO，[在线体验Demo](https://libretranslate.com/)
  - windows下安装失败，错误信息
    - ERROR: Could not find a version that satisfies the requirement ctranslate2 (from argostranslate==1.0) (from versions: none)；ERROR: No matching distribution found for ctranslate2 (from argostranslate==1.0)

## 分词

[分词算法深度综述](https://zhuanlan.zhihu.com/p/50444885)

NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：
1.  粒度，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词
2.  歧义，比如“下雨天留人天留我不留”
3.  未登录词，比如“skrrr”、“打call”等新兴词语
 
然而，在真实的应用中往往会因为以上的难点造成分词效果欠佳，进而影响之后的任务。对于追求算法表现的童鞋来说，不仅要会调分词包，也要对这些基础技术有一定的了解，在做真正的工业级应用时有能力对分词器进行调整。这篇文章不是着重介绍某个SOTA成果，而是对常用的分词算法（不仅是机器学习或神经网络，还包括动态规划等）以及其核心思想进行介绍。

### 总结
 
分词作为NLP底层任务之一，既简单又重要，很多时候上层算法的错误都是由分词结果导致的。因此，对于底层实现的算法工程师，不仅需要深入理解分词算法，更需要懂得如何高效地实现。而对于上层应用的算法工程师，在实际分词时，需要根据业务场景有选择地应用上述算法，比如在搜索引擎对大规模网页进行内容解析时，对分词对速度要求大于精度，而在智能问答中由于句子较短，对分词的精度要求大于速度。
 
### 分词算法
 
分词算法根据其核心思想主要分为两种
- 第一种是基于**字典**的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；
- 第二种是基于**字**的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。
归根结底，上述两种方法都可以归结为在图或者概率图上寻找**最短路径**的问题。接下来以“**他说的确实在理**”这句话为例，讲解各个不同的分词算法核心思想。
 
#### 基于词典的分词
 
##### 最大匹配分词算法
 
最大匹配分词寻找最优组合的方式是将匹配到的最长词组合在一起。主要的思路是先将词典构造成一棵Trie树，也称为字典树，如下图：
- ![](https://pic4.zhimg.com/v2-75f7fbfd385f12ec3babad2c45ed8417_b.jpg)
- ![](https://pic4.zhimg.com/80/v2-75f7fbfd385f12ec3babad2c45ed8417_720w.jpg)

Trie树由词的公共前缀构成节点，降低了存储空间的同时提升查找效率。最大匹配分词将句子与Trie树进行匹配，在匹配到根结点时由下一个字重新开始进行查找。比如正向（从左至右）匹配“他说的确实在理”，得出的结果为“他／说／的确／实在／理”。如果进行反向最大匹配，则为“他／说／的／确实／在理”。
 
可见，词典分词虽然可以在O(n)时间对句子进行分词，但是效果很差，在实际情况中基本不使用此种方法。
 
##### 最短路径分词算法

最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式，引用《统计自然语言处理》中的图：
- ![](https://pic4.zhimg.com/v2-0f0d0bdf6d76ff30bff0fa2cb02bd963_b.jpg)
- ![](https://pic4.zhimg.com/80/v2-0f0d0bdf6d76ff30bff0fa2cb02bd963_720w.jpg)
 
我们认为图中每个词的权重都是相等的，因此每条边的权重都为1。

在求解DAG图的最短路径问题时，总是要利用到一种性质：即两点之间的最短路径也包含了路径上其他顶点间的最短路径。比如S->A->B->E为S到E到最短路径，那S->A->B一定是S到B到最短路径，否则会存在一点C使得d(S->C->B)<d(S->A->B)，那S到E的最短路径也会变为S->C->B->E，这就与假设矛盾了。利用上述的最优子结构性质，可以利用贪心算法或动态规划两种求解算法：
 
**最短路径分词算法**
 
基于Dijkstra算法求解最短路径。该算法适用于所有带权有向图，求解源节点到其他所有节点的最短路径，并可以求得全局最优解。Dijkstra本质为贪心算法，在每一步走到当前路径最短的节点，递推地更新原节点到其他节点的距离。针对当前问题，Dijkstra算法的计算结果为：“他／说／的／确实／在理“。可见最短路径分词算法可以满足部分分词要求。但当存在多条距离相同的最短路径时，Dijkstra只保存一条，对其他路径不公平，也缺乏理论依据。
 
**N-最短路径分词算法**
 
N-最短路径分词是对Dijkstra算法的扩展，在每一步保存最短的N条路径，并记录这些路径上当前节点的前驱，在最后求得最优解时回溯得到最短路径。该方法的准确率优于Dijkstra算法，但在时间和空间复杂度上都更大。
 
##### 基于n-gram model的分词算法
 
在前文的词图中，边的权重都为1。而现实中却不一样，常用词的出现频率／概率肯定比罕见词要大。因此可以将求解词图最短路径的问题转化为求解最大概率路径的问题，即分词结果为“最有可能的词的组合“。计算词出现的概率，仅有词典是不够的，还需要有充足的语料。因此分词任务已经从单纯的“算法”上升到了“建模”，即利用统计学方法结合大数据挖掘，对“语言”进行建模。

语言模型的目的是构建一句话出现的概率p(s)，根据条件概率公式我们知道：
- ![[公式]](https://www.zhihu.com/equation?tex=p%28%E4%BB%96%E8%AF%B4%E7%9A%84%E7%A1%AE%E5%AE%9E%E5%9C%A8%E7%90%86%29%3Dp%28%E4%BB%96%29p%28%E8%AF%B4%7C%E4%BB%96%29p%28%E7%9A%84%7C%E4%BB%96%E8%AF%B4%29p%28%E7%A1%AE%7C%E4%BB%96%E8%AF%B4%E7%9A%84%29...p%28%E7%90%86%7C%E4%BB%96%E8%AF%B4%E7%9A%84%E7%A1%AE%E5%AE%9E%E5%9C%A8%29)

而要真正计算“他说的确实在理”出现的概率，就必须计算出上述所有形如 ![[公式]](https://www.zhihu.com/equation?tex=p%28w_n%7Cw_1...w_%7Bn-1%7D%29)  n=1,...,6 的概率，计算量太过庞大，因此我们近似地认为：
- ![[公式]](https://www.zhihu.com/equation?tex=p%28s%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bl%7Dp%28w_i%7Cw_1...w_%7Bi-1%7D%29%5Capprox+%5Cprod_%7Bi%3D1%7D%5E%7Bl%7Dp%28w_i%7Cw_%7Bi-1%7D%29+%5C%5C)
 
其中 ![[公式]](https://www.zhihu.com/equation?tex=s%3Dw_1w_2...w_l) ， ![[公式]](https://www.zhihu.com/equation?tex=w_i) 为字或单词。我们将上述模型成为二元语言模型(2-gram model)。类似的，如果只对词频进行统计，则为一元语言模型。由于计算量的限制，在实际应用中n一般取3。
 
将基于词的语言模型所统计出的概率分布应用到词图中，可以得到词的**概率图**：
- ![](https://pic4.zhimg.com/v2-cf40db0165ab1c241d3632ff2fef18a7_b.jpg)
- ![](https://pic4.zhimg.com/80/v2-cf40db0165ab1c241d3632ff2fef18a7_720w.jpg)
 
对该词图用2.1.2中的算法求解最大概率的路径，即可得到分词结果。
 
#### 基于字的分词
 
与基于词典的分词不同的是，基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。
 
统计机器学习方法通过一系列算法对问题进行抽象，进而得到模型，再用得到的模型去解决相似的问题。也可以将模型看成一个函数，输入X，得到f(X)=Y。另外，机器学习中一般将模型分为两类：生成式模型和判别式模型，两者的本质区别在于X和Y的生成关系。生成式模型以“输出Y按照一定的规律生成输入X”为假设对P(X,Y)联合概率进行建模；判别式模型认为Y由X决定，直接对后验概率P(Y|X)进行建模。两者各有利弊，生成模型对变量的关系描述更加清晰，而判别式模型容易建立和学习。下面对几种序列标注方法做简要介绍。
 
##### 生成式模型分词算法
 
生成式模型主要有n-gram模型、HMM隐马尔可夫模型、朴素贝叶斯分类等。在分词中应用比较多的是n-gram模型和HMM模型。如果将2.1.3中的节点由词改成字，则可基于字的n-gram模型进行分词，不过这种方法的效果没有基于词的效果要好。
 
HMM模型认为在解决序列标注问题时存在两种序列，一种是观测序列，即人们显性观察到的句子，而序列标签是隐状态序列，即观测序列为X，隐状态序列是Y，因果关系为Y->X。因此要得到标注结果Y，必须对X的概率、Y的概率、P(X|Y)进行计算，即建立P(X,Y)的概率分布模型。例句的隐马尔科夫序列如下图：
- ![](https://pic3.zhimg.com/v2-64e500f19aa2e7f0a0c5e04eade941a6_b.jpg)
- ![](https://pic3.zhimg.com/80/v2-64e500f19aa2e7f0a0c5e04eade941a6_720w.jpg)

HMM模型是常用的分词模型，基于Python的jieba分词器和基于Java的HanLP分词器都使用了HMM。要注意的是，该模型创建的概率图与上文中的DAG图并不同，因为节点具有观测概率，所以不能再用上文中的算法求解，而应该使用Viterbi算法求解最大概率的路径。
 
##### 判别式模型分词算法
 
判别式模型主要有感知机、SVM支持向量机、CRF条件随机场、最大熵模型等。在分词中常用的有感知机模型和CRF模型：
 
**平均感知机分词算法**
 
感知机是一种简单的二分类线性模型，通过构造超平面，将特征空间（输入空间）中的样本分为正负两类。通过组合，感知机也可以处理多分类问题。但由于每次迭代都会更新模型的所有权重，被误分类的样本会造成很大影响，因此采用平均的方法，在处理完一部分样本后对更新的权重进行平均。
 
**CRF分词算法**
 
CRF可以看作一个无向图模型，对于给定的标注序列Y和观测序列X，对条件概率P(Y|X)进行定义，而不是对联合概率建模。CRF可以说是目前最常用的分词、词性标注和实体识别算法，它对未登陆词有很好的识别能力，但开销较大。
 
##### 神经网络分词算法
 
在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。双向（Bidirectional）循环神经网络分别从句子的开头和结尾开始对输入进行处理，将上下文信息进行编码，提升预测效果。
 
目前对于序列标注任务，公认效果最好的模型是BiLSTM+CRF。结构如图：
- ![](https://pic1.zhimg.com/v2-6fe27f55d9af16b2dca64fc85278606c_b.jpg)
- ![](https://pic1.zhimg.com/80/v2-6fe27f55d9af16b2dca64fc85278606c_720w.jpg)
 
利用双向循环神经网络BiLSTM，相比于上述其它模型，可以更好的编码当前字等上下文信息，并在最终增加CRF层，核心是用Viterbi算法进行解码，以得到全局最优解，避免B,S,E这种标记结果的出现。
 
### 分词算法中的数据结构
 
前文主要讲了分词任务中所用到的算法和模型，但在实际的工业级应用中，仅仅有算法是不够的，还需要高效的数据结构进行辅助。
 
#### 词典

中文有7000多个常用字，56000多个常用词，要将这些数据加载到内存虽然容易，但进行高并发毫秒级运算是困难的，这就需要设计巧妙的数据结构和存储方式。前文提到的Trie树只可以在O(n)时间完成单模式匹配，识别出“的确”后到达Trie树对也节点，句子指针接着指向“实”，再识别“实在”，而无法识别“确实”这个词。如果要在O(n)时间完成多模式匹配，构建词图，就需要用到Aho-Corasick算法将模式串预处理为有限状态自动机，如模式串是he/she/his/hers，文本为“ushers”。构建的自动机如图：
- ![](https://pic1.zhimg.com/v2-0830487fa64b32d7558129b45b0791f0_b.jpg)
- ![](https://pic1.zhimg.com/80/v2-0830487fa64b32d7558129b45b0791f0_720w.jpg)
 
这样，在第一次到叶节点5时，下一步的匹配可以直接从节点2开始，一次遍历就可以识别出所有的模式串。
 
对于数据结构的存储，一般可以用链表或者数组，两者在查找、插入和删除操作的复杂度上各有千秋。在基于Java的高性能分词器HanLP中，作者使用双数组完成了Trie树和自动机的存储。
 
#### 词图
 
图作为一种常见的数据结构，其存储方式一般有两种：
 
**邻接矩阵**
 
邻接矩阵用数组下标代表节点，值代表边的权重，即d\[i\]\[j\]=v代表节点i和节点j间的边权重为v。如下图：
- ![](https://pic3.zhimg.com/v2-9799f85f1252410610ea06d5cf865702_b.jpg)
- ![](https://pic3.zhimg.com/80/v2-9799f85f1252410610ea06d5cf865702_720w.jpg)
 
用矩阵存储图的空间复杂度较高，在存储稀疏图时不建议使用。
 
**邻接表**
 
邻接表对图中的每个节点建立一个单链表，对于稀疏图可以极大地节省存储空间。第i个单链表中的节点表示依附于顶点i的边，如下图：
- ![](https://pic3.zhimg.com/v2-2bc26ceeb33dad2e7f7857fee735d2c2_b.jpg)
- ![](https://pic3.zhimg.com/80/v2-2bc26ceeb33dad2e7f7857fee735d2c2_720w.jpg)
在实际应用中，尤其是用Viterbi算法求解最优路径时，由于是按照广度优先的策略对图进行遍历，最好是使用邻接表对图进行存储，便于访问某个节点下的所有节点。

## NER 命名实体识别

命名实体识别（NER, Named Entity Recognition），是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。

- 目标：识别序列中的人名、地名、组织机构名等实体。属于序列标注问题。
- [工业界如何解决NER问题？12个trick](https://zhuanlan.zhihu.com/p/152463745)
   - Q1、如何快速有效地提升NER性能（非模型迭代）？
   - Q2、如何在模型层面提升NER性能？
   - Q3、如何构建引入词汇信息（词向量）的NER？
   - Q4、如何解决NER实体span过长的问题？
   - Q5、如何客观看待BERT在NER中的作用？
   - Q6、如何冷启动NER任务？
   - Q7、如何有效解决低资源NER问题？
   - Q8、如何缓解NER标注数据的噪声问题？
   - Q9、如何克服NER中的类别不平衡问题？
   - Q10、如何对NER任务进行领域迁移？
   - Q11、如何让NER系统变得“透明”且健壮？
   - Q12、如何解决低耗时场景下的NER任务？

### 常见NER方法

- 【2021-11-7】[命名实体识别（NER）的过去和现在](https://mp.weixin.qq.com/s/sNXG-K5aPRFFQ3F373-e0A)

#### 基于词典和规则的方法

- 利用**词典**，通过词典的先验信息，匹配出句子中的**潜在实体**，通过一些规则进行筛选。
- 或者利用**句式模板**，抽取实体，例如模板“**播放歌曲${song}**”，就可以将query=“播放歌曲**七里香**”中的 song= 七里香 抽取出来。
具体匹配方法：
- **正向**最大匹配：从前往后依次匹配子句是否是词语，以最长的优先。
- **反向**最大匹配：从后往前依次匹配子句是否是词语，以最长的优先。
- **双向**最大匹配
  - 覆盖 token **最多**的匹配。
  - 句子包含实体和切分后的片段，这种片段+实体个数最少的。

原理比较简单，直接看[代码](https://github.com/InsaneLife/MyPicture/blob/master/NER/ner_rule.py)

#### 机器学习方法 CRF

CRF，原理可以参考：Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data


#### 引入深度学习语义编码器 Bi-LSTM-CRF

（1）BI-LSTM-CRF模型可以有效地利用过去和未来的输入特征。借助 CRF 层, 它还可以使用句子级别的标记信息。BI-LSTM-CRF 模型在 `POS`（词性标注），chunking（语义组块标注）和 `NER`（命名实体识别）数据集上取得了当时的 SOTA 效果。同时 BI-LSTM-CRF 模型是健壮的，相比之前模型对词嵌入依赖更小。

论文[Bidirectional LSTM-CRF Models for Sequence Tagging]()中对比了 5 种模型：LSTM、BI-LSTM、CRF、LSTM-CRF、BI-LSTM-CRF，LSTM：通过输入门，遗忘门和输出门实现记忆单元，能够有效利用上文的输入特征。BI-LSTM：可以获取时间步的上下文输入特征。CRF：使用功能句子级标签信息，精度高。

比较经典的模型，BERT 之前很长一段时间的范式，小数据集仍然可以使用

（2）stack-LSTM & char-embedding

Neural Architectures for Named Entity Recognition
stack-LSTM ：stack-LSTM 直接构建多词的命名实体。Stack-LSTM 在 LSTM 中加入一个栈指针。模型包含 chunking 和 NER（命名实体识别）。

（3）CNN + BI-LSTM + CRF

通过 CNN 获取字符级的词表示。CNN 是一个非常有效的方式去抽取词的形态信息（例如词的前缀和后缀）进行编码的方法

（4）IDCNN

针对 Bi-LSTM 解码速度较慢的问题，本文提出 ID-CNNs 网络来代替 Bi-LSTM，在保证和 Bi-LSTM-CRF 相当的正确率，且带来了 14-20 倍的提速。句子级别的解码提速 8 倍相比于 Bi- LSTM-CRF。

（5）胶囊网络


Joint Slot Filling and Intent Detection via Capsule Neural Networks [7]

Git: https://github.com/czhang99/Capsule-NLU


NLU 中两个重要的任务，Intent detection 和 slot filling，当前的无论 pipline 或者联合训练的方法，没有显示地对字、槽位、意图三者之间的层次关系建模。本文提出将胶囊网络和 dynamic routing-by-agreement 应用于 slot filling 和 intent detection 联合任务。
1. 使用层次话的胶囊网络来封装字、槽位、意图之间的层次关系。
2. 提出 rerouting 的动态路由方案建模 slot filling。

（6）Transformer

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [10]

直说吧，就是 BERT，bert 之前万年 bilstm+crf，bert 之后，基本没它什么事儿了，bert 原理不多赘述，应用在 NER 任务上也很简单，直接看图，每个 token 的输出直接分类即可

#### 深度学习-语义特征

（1）char-embedding

Neural Architectures for Named Entity Recognition，将英文字符拆解为字母，将词语的每个字母作为一个序列编码，编码器可以使用 rnn，cnn 等。

（2）Attending to Characters in Neural Sequence Labeling Models

Attending to Characters in Neural Sequence Labeling Models，使用了单词或字符级别 embedding 组合，并在两种 embedding 之间使用 attention 机制“灵活地选取信息”，而之前模型是直接将两种 embedding concat。char-embedding 学习的是所有词语之间更通用的表示，而 word-embedding 学习的是特特定词语信息。对于频繁出现的单词，可以直接学习出单词表示，二者也会更相似。

（3）Radical-Level Features（中文部首）


Character-Based LSTM-CRF with Radical-LevelFeatures for Chinese Named Entity Recognition 也是一种 char embedding 方法，将每个中文字拆分为各个部首，例如“朝”会被拆分为字符：十、日、十、月。后面结构都类似。

（4） n-gram prefixes and suffixes

Named Entity Recognition with Character-Level Models 提取每个词语的前缀和后缀作为词语的特征，例如：“aspirin” 提取出 3-gram 的前后缀：{"asp", "rin"}. 包含两个参数：n、T。n 表示 n-gram size，T 是阈值，表示该后缀或者前缀至少在语料库中出现过 T 次。

####  多任务联合学习

（1）联合分词学习

Improving Named Entity Recognition for Chinese Social Mediawith Word Segmentation Representation Learning [15]

将中文分词 和 NER 任务联合起来。使用预测的分割标签作为特征作为 NER 的输入之一，为 NER 系统提供更丰富的边界信息。分词语料目前是很丰富的。如果目标域数据量比较小，不妨用分词的语料作为源域，来预训练一个底层编码器，然后再在目标域数据上联合分词任务 fine-tuning。

（2）联合意图学习

slot-gated

Slot-Gated Modeling for Joint Slot Filling and Intent Prediction，slot-gated 这篇文章提出了 slot-gate 将槽位和意图的关系建模，同时使用了 attention 方法，所以介绍这篇文章直接一起介绍 attention，之前 attention 相关的就不介绍了。

（3）BERT for Joint Intent Classification and Slot Filling

BERT for Joint Intent Classification and Slot Filling 原理如图，底层编码器使用了 BERT，token 的输出向量接 softmax 预测序列标签，cls 向量预测意图。

bert 之后，似乎之前的一些优化都变成了奇技淫巧，那么就没有新的方法了吗？bert 之前实体识别都是以序列标注（sequence labeling）来识别，没有其他的解码方式吗？

### 序列标注的几种模式

- 在序列标注中，我们想对一个序列的每一个元素(token)标注一个标签。一般来说，一个序列指的是一个句子，而一个元素(token)指的是句子中的一个词语或者一个字。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。
- 不同的序列标注任务就是将目标句中的字或者词按照需求的方式标记，不同的结果取决于对样本数据的标注，一般序列的标注是要符合一定的标注标准的如([PKU数据标注规范](http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf))。
- 另外, 词性标注、分词都属于同一类问题，他们的区别主要是对序列中的token的标签标注的方式不同。

下面以命名实体识别来举例说明. 我们在进行命名实体识别时，通常对每个字进行标注。中文为单个字，英文为单词，空格分割。

标签类型的定义一般如下：

|定义|	全称|	备注|
|---|---|---|
|B	|Begin	|实体片段的开始|
|I	|Intermediate|	实体片段的中间|
|E	|End	|实体片段的结束|
|S	|Single	|单个字的实体|
|O	|Other/Outside	|其他不属于任何实体的字符(包括标点等)|

### 常见的标签方案

常用的较为流行的标签方案有如下几种：
- IOB1: 标签I用于文本块中的字符，标签O用于文本块之外的字符，标签B用于在该文本块前面接续则一个同类型的文本块情况下的第一个字符。
- IOB2: 每个文本块都以标签B开始，除此之外，跟IOB1一样。
- IOE1: 标签I用于独立文本块中，标签E仅用于同类型文本块连续的情况，假如有两个同类型的文本块，那么标签E会被打在第一个文本块的最后一个字符。
- IOE2: 每个文本块都以标签E结尾，无论该文本块有多少个字符，除此之外，跟IOE1一样。
- START/END （也叫SBEIO、IOBES）: 包含了全部的5种标签，文本块由单个字符组成的时候，使用S标签来表示，由一个以上的字符组成时，首字符总是使用B标签，尾字符总是使用E标签，中间的字符使用I标签。
- IO: 只使用I和O标签，显然，如果文本中有连续的同种类型实体的文本块，使用该标签方案不能够区分这种情况。

其中最常用的是`BIO`, `BIOES`, `BMES`

#### BIO标注模式

将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

命名实体识别中每个token对应的标签集合如下:
>LabelSet = {O, B-**PER**, I-**PER**, B-**LOC**, I-**LOC**, B-**ORG**, I-**ORG**}

其中，PER代表人名， LOC代表位置， ORG代表组织. B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名(位置)首字、地名(位置)非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，O代表该字不属于命名实体的一部分。

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIO.jpg)

对于词性标注, 则可以用{B-NP, I-NP}给序列中的名词token打标签

#### BIOES标注模式

BIOES标注模式就是在BIO的基础上增加了单字符实体和字符实体的结束标识, 即

>LabelSet = {O, B-PER, I-PER, E-PER, S-PER, B-LOC, I-LOC, E-LOC, S-LOC, B-ORG, I-ORG, E-ORG, S-ORG}

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIOES.jpg)

根据标注的复杂度, 还有会在其中添加其他的比如MISC之类的实体, 如

>LabelSet = {O ,B-MISC, I-MISC, B-ORG ,I-ORG, B-PER ,I-PER, B-LOC ,I-LOC]。
其中，一般一共分为四大类：PER（人名），LOC（位置[地名]），ORG（组织）以及MISC(杂项)，而且B表示开始，I表示中间，O表示不是实体。

其他类似的标注方式:

标注方式1:
>LabelSet = {BA, MA, EA, BO, MO, EO, BP, MP, EP, O}
其中，
- BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；
- BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；
- BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。

标注方式2:
> LabelSet = {NA, SC, CC, SL, CL, SP, CP}
其中 NA = No entity, SC = Start Company, CC = Continue Company, SL = Start Location, CL = Continue Location, SP = Start Person, CP = Continue Person

上面两种标注方式与BIO和BIEOS类似, 只是使用不同的标签字符来标识而已.

### BMES标注模式

### 评估方法

序列标注算法一般用conlleval.pl脚本实现，但这是用perl语言实现的。在Python中，也有相应的序列标注算法的模型效果评估的第三方模块，那就是**seqeval**，其[官网网址](https://pypi.org/project/seqeval/0.0.3/), pip install seqeval==0.0.3

seqeval支持BIO，IOBES标注模式，可用于命名实体识别，词性标注，语义角色标注等任务的评估。

```python
# -*- coding: utf-8 -*-
from seqeval.metrics import f1_score
from seqeval.metrics import precision_score
from seqeval.metrics import accuracy_score
from seqeval.metrics import recall_score
from seqeval.metrics import classification_report

y_true = ['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']
y_pred = ['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']

print("accuary: ", accuracy_score(y_true, y_pred))
print("p: ", precision_score(y_true, y_pred))
print("r: ", recall_score(y_true, y_pred))
print("f1: ", f1_score(y_true, y_pred))
print("classification report: ")
print(classification_report(y_true, y_pred))
```

### NER问题及改进

- 【2021-9-8】[ACL2021 一种巧妙解决NER覆盖和不连续问题的方法](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247505432&idx=2&sn=eb7ad917eaa560a444b9efb51f909295&chksm=eb53c28bdc244b9d2b3e765cafff4a7b20f5fafa6e5b0c8cbccc7223138486470d821a27d085&mpshare=1&scene=1&srcid=0908OsHWLx486VoYI2djrIGu&sharer_sharetime=1631096211455&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
- 稍微复杂些问题：一种带有**覆盖**和**不连续**（Overlapped and Discontinuous）的命名实体识别任务。前人只是要么解决**覆盖**问题，要么解决**不连续**问题，但是本文提出一种联合解决这两种问题的**span-based**方法。
- 两个步骤构建模型：
  - 通过列举所有可能的text span来识别出实体片段（entity fragments）；
  - 在这些entity fragments上预测是两种关系overlapping or succession。
- 这样，我们不仅可以识别Discontinuous的实体，同时也可以对Overlapped的实体进行双重检查。
- 通过上述方法轻松将NER装换成RE（Relation Extraction）任务。最终实验在很多数据集上比如CLEF, GENIA and ACE05上展现除了很强劲的性能。


### 参考资料

- [命名实体识别(Name Entity Recognition)综述](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/)



## 短语挖掘

### 关键词提取方案

- 知乎话题：[关键词提取都有哪些方案](https://www.zhihu.com/question/21104071)

- 刘知远的博士论文：[基于文档主题结构的关键词抽取方法研究](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/phd_thesis.pdf)
- 关键词挖掘的方法：（2014年，作者：[zibuyu9](https://www.zhihu.com/question/21104071/answer/24556905)）
   - 1. TFIDF是很强的baseline，具有较强的普适性，如果没有太多经验的话，可以实现该算法基本能应付大部分关键词抽取的场景了。
   - 2. 对于中文而言，中文分词和词性标注的性能对关键词抽取的效果至关重要。
   - 3. 较复杂的算法各自有些问题，如Topic Model，它的主要问题是抽取的关键词一般过于宽泛，不能较好反映文章主题。这在我的博士论文中有专门实验和论述；TextRank实际应用效果并不比TFIDF有明显优势，而且由于涉及网络构建和随机游走的迭代算法，效率极低。这些复杂算法集中想要解决的问题，是如何利用更丰富的文档外部和内部信息进行抽取。如果有兴趣尝试更复杂的算法，我认为我们提出的基于SMT（统计机器翻译）的模型，可以较好地兼顾效率和效果。
      - TextRank源于page-rank，page-rank是谷歌提出的对网页按照影响力进行排序的算法。同样的，text-rank认为文档或句子中相邻的词语重要性是相互影响的，所以text-rank引入了词语的顺序信息。
      - ![](https://pic4.zhimg.com/80/v2-192504d9afdc37816ceb8e6c8e7649f2_720w.jpg)
   - 4. 以上都是无监督算法，即没有事先标注好的数据集合。而如果我们有事先标注好的数据集合的话，就可以将关键词抽取问题转换为有监督的分类问题。这在我博士论文中的相关工作介绍中均有提到。
   - 从性能上来讲，利用有监督模型的效果普遍要优于无监督模型，对关键词抽取来讲亦是如此。在Web 2.0时代的社会标签推荐问题，就是典型的有监督的关键词推荐问题，也是典型的多分类、多标签的分类问题，有很多高效算法可以使用。

- 作者：[小Fan](https://www.zhihu.com/question/21104071/answer/291420205)
- 分两步走：
   - `候选词匹配`：基于关键词词库的多模式匹配得到候选，这里最重要的工作是词库构建，往往会融合多种方法：垂直站点专有名词，百科词条，输入法细胞词库，广告主购买词，基于大规模语料库的自动词库挖掘（推荐韩家炜团队的 shangjingbo1226/SegPhrase ，shangjingbo1226/AutoPhrase 方法）等。这里会涉及大量的数据清洗工作，甚至还可以有一个质量分类器决定哪些词条可以进入词库。
   - `候选词相关性排序`：包括无监督和有监督方法，如下：
      - `无监督方法`：常见的有 TFIDF（需要统计 phrase 级别的 DF）， textrank（优势不明显，计算量大，慎用），topic 相似度（参见 baidu/Familia），embedding 相似度（需要训练或计算 keyword 和 doc embedding），TWE 相似度（参见 baidu/Familia）
      - `有监督方法`：常见的有基于统计机器翻译 SMT 的方法（转换成翻译问题，可以采用 IBM Model 1），基于序列标注模型的方法（转换成核心成分识别问题，类似 NER，状态只有0和1，即是否是核心成分，较适用于短文本），基于排序学习LTR的方法（转换成候选词排序问题，采用 pairwise 方法，或者深度语义匹配方法，如 DSSM），基于传统机器学习分类方法（转换成二元或多元分类问题）。有监督方法依赖一定规模的标注数据，效果通常会显著好于无监督方法。


### [韩家炜团队开源工具](https://www.jianshu.com/p/76197e116835)

- 具体工具如下：
   - `TopMine`：频率模式挖掘+统计分析
   - `SegPhrase`：SegPhrase：弱监督、高质量的 Phrase Mining
      - 论文：Mining Quality Phrases from Massive Text Corpora
      - TopMine 的方法完全是无监督的，如果有少量的 Label 数据可能会在很大程度上提高 Topic Model 的结果。
      - SegPhrase框架只需要有限的培训，但是生成的短语的质量却接近于人类的判断力。而且，该方法具有可伸缩性：随着语料库大小的增加，计算时间和所需空间都会线性增长。论文在大型文本语料库上的实验证明了该新方法的质量和效率。
      - segphrase已有[GitHub开源工具](https://github.com/shangjingbo1226/SegPhrase)
   - `AutoPhrase`：自动的 Phrase Mining
      - 论文：Automated Phrase Mining from Massive Text Corpora
      - AutoPhrase支持多种语言（包含简体中文和繁体中文）基本思想是通过分别在训练和解析过程中添加编码/解码层来重用英语实现。在训练阶段，在中文单词片段和英文字母之间创建一个字典，将输入数据编码为英文伪单词。在解析阶段，在识别出编码后的新文档中的优质短语之后，进行解码以恢复可读的中文。
      - AutoPhrase已有[GitHub开源工具](https://github.com/shangjingbo1226/AutoPhrase)

### AutoPhrase

- 【2020-7-3】数据挖掘之父韩家炜团队的自动短语挖掘论文：[Automated Phrase Mining from Massive Text Corpora]()，提出自动挖掘短语的方法：AutoPhrase，[C++代码](https://github.com/shangjingbo1226/AutoPhrase)，[Python包](https://github.com/CS512-Autophrase-Demo/AutophrasePy), [解说](https://blog.csdn.net/weixin_42363527/article/details/102884088)
- 思路：使用通用知识库（KB）的来构造正样本（应该就是用完全匹配的方式），然后训练一个NER模型（非神经网络的），然后用这个NER模型的预测结果来减少负样本噪声，引入词性信息
- 先从KB里匹配出正样本，其他的词是负样本，然后训练NER/CRF模型，再卡个阈值，筛掉分低的实体，最后出的作为抽取出的短语。

对于自动短语挖掘任务，
- 输入：语料库（特定语言和特定领域的文本单词序列，长度任意）和知识库
- 输出：一个按质量递减排列的短语列表

短语质量 定义为一个单词序列成为一个完整语义单元的概率，满足以下条件：
- `流行度`: 在给定的文档集合中，质量短语应该出现的频率足够高。
- `一致性`: 由于偶然因素，令牌在高质量短语中的搭配出现的概率明显高于预期。
- `信息性`: 如果一个短语表达了一个特定的主题或概念，那么这个短语就是信息性的。
- `完备性`: 长频繁短语及其子序列均满足上述3个条件。当一个短语可以在特定的文档上下文中解释为一个完整的语义单元时，它就被认为是完整的

流程图
- ![](https://img-blog.csdnimg.cn/2020042217323277.png)
- AutoPhrase会根据正池和负池对短语质量进行两次评估，一次在短语分割前，一次在短语分割后。也就是说，POS-Guided短语分割需要一组初始的短语质量分数;我们预先根据原始频率估计分数;然后，一旦特征值被纠正，我们重新估计分数。

- `AutoPhrase`超越了分段短语，进一步摆脱了额外的手工标注工作，提高了性能。
- 主要使用以下两种新技术：
   - `Robust Positive-Only Distant Training`（鲁棒正向远程训练）
      - 即：利用已有的知识库（Wikipedia）做远程监督训练
      - 公共知识库（如维基百科）中的高质量短语，免费并且数量很多。在远程训练中，使用一般知识库中高质量短语，可以避免手工标注。
      - 具体做法是：
         - 从一般知识库中构建积极标签的样本
         - 从给定的领域语料库中构建消极标签的样本
         - 训练大量的基本分类器
         - 将这些分类器的预测聚合在一起
   - `POS-Guided Phrasal Segmentation`. （POS-Guided短语分割）
      - 即：利用词性信息来增加抽取的准确性
      - 语言处理器应该权衡：`性能` 和 `领域独立能力`
         - 对于领域独立能力，如果没有语言领域知识，准确性会受限制
         - 对于准确性，依赖复杂的、训练有素的语言分析器，就会降低领域独立能力
      - 解决办法： 在文档集合的语言中加入一个预先训练的词性标记，以进一步提高性能

## 句法分析

[李宏毅 人类语言处理 独家笔记 成分句法分析](https://zhuanlan.zhihu.com/p/150121527)

NLP 任务中，句法分析有两种
- 一种是**成分**句法分析: 找到一个句子的组成成分
- 另一种是**依存**句法分析。
句法分析不适用于之前的 NLP 任务分类体系。它的输出形式相对来说会比较不一样。

### 成分句法分析

成分句法分析要做的是给定一个句子，句子中每个词汇都是成分。
- 每一个成分都会有一个标签，比如 deep learning 的标签是 NP，very powerful 的标签是 ADJP。
  - ![示例](https://pic2.zhimg.com/80/v2-39f3f7c7e1fe6d4c1f850b4732ef3fd9_1440w.jpg)
  - ![标签集合](https://pic2.zhimg.com/80/v2-643b673197d99d654c4ff9b3a94b5ee5_1440w.jpg)
- 所有词性标注的词项标签也都是可能的标签。
- 相邻的成分可以组合成一个更大的单位。比如 deep 和 learning 可以组合起来成为一个名词短语。最后这个动词短语和名词短语组合起来变成整个句子。
  - ![](https://pic2.zhimg.com/80/v2-70387226d37ea8c85a4f6cb87c84b2ed_1440w.jpg)
![](https://pic2.zhimg.com/80/v2-39694c27836fdd044601164a33b6b405_1440w.jpg)

成分句法分析通用的解决方案
- 一个是 char-based 方案。训练一个分类器，输入是一串 tokens，它决定这个 span 是不是成分。如果确定它是成分，接下来我们要用另一个分类器，对该成分确定它的标签。
  - ![char-based过程](https://pic3.zhimg.com/80/v2-a017a2b1c09cc8b871706e4dab7e1532_1440w.jpg)
- 另一个方法叫作基于转移的方法。它的精神是把产生的句子加入到一个 Buffer 中，加上一连串的操作，就可以做到成分句法解析。
  - ![transition-based](https://pic4.zhimg.com/80/v2-1c38e20988eb2e20ec5bf9dc64fbd67f_1440w.jpg)
- tree2seq：把树状的结构表示为一个序列，比如用深度优先遍历方法，由上而下，由左到右把每个节点按遍历顺序放在序列中。
  - ![](https://pic3.zhimg.com/80/v2-a7376e07fb9049be5a5d5b54a8eb82f2_1440w.jpg)

资料：[成分分析方法综述](https://zhuanlan.zhihu.com/p/45527481)


### 依存句法分析


## 文本分类


### fasttext

- 【2021-5-27】fasttext, [简介](https://blog.csdn.net/qq_32023541/article/details/80839800?spm=1001.2014.3001.5501) ,Library for efficient text classification and representation learning
  - ![](https://fasttext.cc/img/fasttext-logo-color-web.png)

fastText是一个快速文本分类算法，与基于神经网络的分类算法相比优点：
- 1、fastText在保持高精度的情况下**加快**了训练速度和测试速度
- 2、fastText不需要预训练好的**词向量**，fastText会自己训练词向量
- 3、fastText两个重要的优化：Hierarchical Softmax、N-gram


FastText是Facebook开发的一款快速文本分类器，提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。
- 2016年，首次发布C++版的fastText，只支持命令行模式
- Bayu Aldi Yansyah开发了Python版本的工具包fasttext
- Facebook发布包含Python版本的fastText
- 2019年6月25日，Facebook融合两个工具包

[安装使用](https://blog.csdn.net/qq_32023541/article/details/80844036)，[官方模型集合](https://fasttext.cc/docs/en/crawl-vectors.html), [中文版](http://fasttext.apachecn.org/#/doc/zh/pretrained-vectors), 支持 294 种语言
- fasttext 是一个有效的学习字词表达和句子分类的库。建立在现代 Mac OS 和 Linux 发行版上。因为它使用了 C++11 的特性，所以需要一个支持 C++11 的编译器，这些包括：gcc-4.8 或更高版本, clang-3.3 或更高版本；
- 注意
  - 目前fastext仅在CPU上运行
  - 仅用于离散数据集，连续数据集不可用，需要离散化
  - 数据集中少量的拼写错误不影响结果
  - 模型训练中出现NaN，原因可能是学习率太高，需要降低
  - 因为优化算法异步随机梯度下降算法或Hogwild,所以每次得到的结果都会**略有不同**，如果想要fastText运行结果复现，则必须将参数**thread**设置为1
- 参考
  - [python——Fasttext新手学习笔记](https://blog.csdn.net/weixin_39023975/article/details/100180531)
  - [fastText原理和文本分类实战，看这一篇就够了](https://blog.csdn.net/feilong_csdn/article/details/88655927)，源自[fasttext中文文档](http://fasttext.apachecn.org/)


```shell
# 下载后安装
pip install fasttext
# 直接安装（可能失败）
git clone https://github.com/facebookresearch/fastText.git
cd fastText
pip install .

# 编译安装
wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip
unzip v0.1.0.zip
cd fastText-0.1.0
make
# 测试
./fasttext
```

上述的命令包括：
- supervised： 训练一个监督分类器
- quantize：量化模型以减少内存使用量
- test：评估一个监督分类器
- predict：预测最有可能的标签 
- predict-prob：用概率预测最可能的标签
- skipgram：训练一个 skipgram 模型
- cbow：训练一个 cbow 模型
- print-word-vectors：给定一个训练好的模型，打印出所有的单词向量
- print-sentence-vectors：给定一个训练好的模型，打印出所有的句子向量
- nn：查询最近邻居
- analogies：查找所有同类词


代码：
```python
# ----- 官方版本 -------
import fastText
# and call:
fastText.train_supervised
fastText.train_unsupervised
# ----- 非官方版本 -------
import fasttext
# and call:
fasttext.cbow
fasttext.skipgram
fasttext.supervised
# ----- 官方融合版本 -------
import fasttext
# and call:
fasttext.train_supervised
fasttext.train_unsupervised
```

fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。
- **模型架构**: fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。
- **层次softmax**: 在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈夫曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。
  - 注意：分层softmax是完全softmax的一个**近似**，分层softmax在大数据集上高效的建立模型，但通常会以**损失精度**的几个百分点为代价
- **N-gram特征**: fastText 可以用于文本分类和句子分类。不管是文本分类还是句子分类，我们常用的特征是词袋模型。但词袋模型不能考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。

（2）分类过程

fasttext在进行文本分类时，huffmax树叶子节点处是每一个类别标签的词向量。在训练过程中，训练语料的每一个词也会得到响应的词向量。输入为一个window 内的词对应的词向量，隐藏层为这几个词的线性相加。相加的结果作为该文档的向量。再通过softmax层得到预测标签。结合文档真实标签计算 loss，梯度与迭代更新词向量（优化词向量的表达）。

参数方面的建议：
1. loss function 选用 hs（hierarchical softmax）要比 ns（negative sampling）训练速度更快，准确率也更高
2. wordNgram 默认为1，建议设置为 2 或以上更好
3. 如果词数不是很多，可以把 bucket 设置小一些，否则会预留太多的 bucket 使模型太大


功能
- supervised进行模型训练
- quantize量化模型以减少内存使用
- test进行模型测试
- predict预测最可能的标签等

注意：
- 小数据集时，记得调整一个参数 mincount（默认是5），不然会出不来词表。
- 中文的分词之后，用空格隔开，也同样可以放入训练。
- data.txt 是一个以 UTF-8 编码的训练文本文件
- 在默认情况下，单词向量将会考虑到 n-grams 的3 到 6 个字符。
在优化结束时，程序将保存 2 个文件： model.bin 和 model.vec

C++版本

```sh
# 数据准备
# 无监督语料——wiki百科
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
# wiki百科部分数据
mkdir data
wget -c http://mattmahoney.net/dc/enwik9.zip -P data
unzip data/enwik9.zip -d data

# 有监督语料——分类
wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz
tar xvzf cooking.stackexchange.tar.gz
head cooking.stackexchange.txt
# 数据集划分
head -n 12404 cooking.stackexchange.txt > cooking.train # 训练集
tail -n 3000 cooking.stackexchange.txt > cooking.valid # 验证集

# （1）训练
# -input命令选项指示训练数据
# -output选项指示的是保存的模型的位置
# （1.1）------- 无监督训练 --------
# skip-gram/cbow模型
mkdir result
./fasttext skipgram -input data/fil9 -output result/fil9
./fasttext cbow -input data/fil9 -output result/fil9
# 输出两个文件：
#   ① fil9.bin是模型文件
#   ② fil9.vec是词向量文件，每一行对应词汇表中的每个单词（按照词频降序排列）
# 218316 100 # 第一行是单词数/向量维度
# the -0.10363 -0.063669 0.032436 -0.040798 0.53749 0.00097867 0.10083 0.24829 ...
# of -0.0083724 0.0059414 -0.046618 -0.072735 0.83007 0.038895 -0.13634 0.60063 ...
# one 0.32731 0.044409 -0.46484 0.14716 0.7431 0.24684 -0.11301 0.51721 0.73262 ...

# 模型量化压缩
./fasttext quantize -output model

# 打印词向量
echo "asparagus pidgey yellow" | ./fasttext print-word-vectors result/fil9.bin
# asparagus 0.46826 -0.20187 -0.29122 -0.17918 0.31289 -0.31679 0.17828 -0.04418 ...
# pidgey -0.16065 -0.45867 0.10565 0.036952 -0.11482 0.030053 0.12115 0.39725 ...
# 或，文件形式批量转换向量
./fasttext print-sentence-vectors model.bin < text.txt

# 查询单词
echo "enviroment" | ./fasttext print-word-vectors result/fil9.bin


# 最临近词向量查询，通过fastText提供的nn功能来实现，余弦相似度去衡量两个单词之间的相似度
$ ./fasttext nn result/fil9.bin # 启动服务
# Pre-computing word vectors... done.
# 交互查询
# Query word? asparagus
# beetroot 0.812384
# tomato 0.806688
# horseradish 0.805928
# spinach 0.801483
# licorice 0.791697
# lingonberries 0.781507
# asparagales 0.780756
# lingonberry 0.778534
# celery 0.774529
# beets 0.773984

# 单词类比， analogies，柏林与德国的关系，类比到法国与巴黎
$ ./fasttext analogies result/fil9.bin
# Pre-computing word vectors... done.
# Query triplet (A - B + C)? berlin germany france
# paris 0.896462
# bourges 0.768954
# louveciennes 0.765569
# toulouse 0.761916
# valenciennes 0.760251
# montpellier 0.752747
# strasbourg 0.744487
# meudon 0.74143
# bordeaux 0.740635
# pigneaux 0.736122


# 模型调优
# ① 模型中最重要的两个参数是：词向量大小维度、subwords范围的大小，词向量维度越大，便能获得更多的信息但同时也需要更多的训练数据，同时如果它们过大，模型也就更难训练速度更慢，默认情况下使用的是100维的向量，但在100-300维都是常用到的调参范围。subwords是一个单词序列中包含最小(minn)到最大(maxn)之间的所有字符串(也即是n-grams)，默认情况下我们接受3-6个字符串中间的所有子单词，但不同的语言可能有不同的合适范围
./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300
# ② 另外两个参数：epoch、learning rate、epoch根据训练数据量的不同，可以进行更改，epoch参数即是控制训练时在数据集上循环的次数，默认情况下在数据集上循环5次，但当数据集非常大时，我们也可以适当减少训练的次数，另一个参数学习率，学习率越高模型收敛的速度就越快，但存在对数据集过度拟合的风险，默认值时0.05，这是一个很好的折中，当然在训练过程中，也可以对其进行调参，可调范围是[0.01, 1]
./fasttext skipgram -input data/fil9 -output result/fil9 -epoch 1 -lr 0.5
# ③ fastText是多线程的，默认情况下使用12个线程，如果你的机器只有更少的CPU核数，也可以通过如下参数对使用的CPU核数进行调整
./fasttext skipgram -input data/fil9 -output result/fil9 -thread 4


# （1.2）----- 有监督训练 -------
./fasttext supervised -input cooking.train -output model_cooking
# 在训练结束后，文件model_cooking.bin是在当前目录中创建的，model_cooking.bin便是我们保存训练模型的文件
# （1.2.1） 模型优化：模型精准率从 12.4% 提升到了 59.9%
# ① 数据预处理：如 大小写统一，缩小词表规模
# 增加迭代次数
./fasttext supervised -input cooking.train -output model_cooking -epoch 25
# ② 设置学习率，一般 0.1~1.0
./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 
# 组合使用
./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25
# ③ 使用n-gram语言模型：上面默认使用uni-gram，丢失了词序信息，改成bi-gram重新训练
# 句子：Last donut of the night
# unigrams：last,donut,of,the,night
# bigrams：last donut,donut of,of the,the night
./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25 -wordNgrams 2
# ④ 训练提速：层次softmax，参数-loss 
./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 -loss hs
# Read 0M words
# Number of words:  9012
# Number of labels: 734
# Progress: 100.0%  words/sec/thread: 2199406  lr: 0.000000  loss: 1.718807  eta: 0h0m 


# （2）测试：
# 交互式测试，一次一句
# 输入：Which baking dish is best to bake a banana bread ?
# 结果：baking
./fasttext predict model_cooking.bin -
# 批量测试
./fasttext test model_cooking.bin cooking.valid
# N  3000
# P@1  0.124
# R@1  0.0541
# Number of examples: 3000
# 带概率的结果
./fasttext predict-prob model.bin test.txt k

```


```python
import fasttext

# ------- 无监督训练 -------
# ① skipgram model
model = fasttext.train_unsupervised('data.txt', model = 'skipgram')
#model = fasttext.skipgram('data.txt','model') # 旧版非官方版本调用方法
# ② CBOW model
model = fasttext.train_unsupervised('data.txt', model = 'cbow')
#model = fasttext.cbow('data.txt','model')  # 旧版非官方版本调用方法
print(model)   # 存储位置
print(model.get_words())   # 词表
print(model.words) # 字典中的词汇列表
# 单词 'king' 的词向量
print(model['king'])
print(model.get_word_vector('to'))    #  对应词向量 
# 保存模型
model.save_model('model.bin')

# 加载前面训练好的模型 model.bin
model = fasttext.load_model("model.bin")
# ---- 文本分类 ----
# 训练文本分类器
classifier = fasttext.train_supervised('data.train.txt','model')
# 还可以用 label_prefix 指定标签前缀
classifier = fasttext.train_supervised('data.train.txt','model',label_prefix = '__label__')
# 输出两个文件: model.bin 和 model.vec

# 量化压缩模型
model.quantize(input='data.train.txt',retrain=True)   # 数据集太小会报错 ！！！
#接下来展示结果和存储新模型
print_result(*model.test(valid_data))
model.save_model("model.ftz") # model.ftz 比model.bin 的大小 要小很多

# 加载模型
classifier = fasttext.load_model("model.bin",label_prefix = "__label__")
print(classifier.labels)
#  ------ 预测 -------
# （1）单例预测
# model.predict(self,
#  text, utf-8 字符串文本
#  k=1, 返回标签的个数，可以理解为最大K项
#  threshord=0.0) 概率阈值，大于才返回
texts = ["example very long text 1","example very longtext 2"]
labels = classifier.predict(texts[0]) # 单词预测
labels = classifier.predict(texts) # 并行预测
labels = classifier.predict(texts,k = 3) # top k的预测结果
labels = classifier.predict_proba(texts) # 含概率, 无效！
print(labels)
# (2) 模型批量预测
# model.test(self,
#  path, 文件路径
#  k=1) 最大k项
# 返回：[N，p，r] 样本个数，精确率，召回率

# 使用 classifier.test 方法在测试数据集上评估模型
result = classifier.test("test.txt") # 迭代输出返回内容
print("准确率：" , result.precision)
print("召回率：" , result.recall)
print("Number of examples:", result.nexamples)

```

## 文本生成

- 【2021-1-9】[现代自然语言生成](https://item.jd.com/12785661.html) 黄民烈

### 综述

- 文本生成被称为NLG，目标是根据输入数据生成自然语言的文本。
   - NLP领域使用更多的一般是NLU（Nature Language Understanding 自然语言理解）类任务，如文本分类、命名实体识别等，NLU的目标则是将自然语言文本转化成结构化数据。
   - NLU和NLG两者表向上是一对相反的过程，但其实是紧密相连的，甚至目前很多NLU的任务都受到了生成式模型中表示方法的启发，更多只在最终任务上有所区别
- 文本生成，广义上只要输出是自然语言文本的各类任务都属于这个范畴
   - 端到端文本生成应用领域
   ![](https://p0.meituan.net/travelcube/5166cd647d076e3cea26972cbe9a332e75935.png)

### 技术方案

- 文本生成包含文本表示和文本生成两个关键的部分，既可以独立建模，也可以通过框架完成端到端的训练

#### 文本生成
- (1) seq2seq
   - 2014年提出的Seq2Seq Model，是解决这类问题一个非常通用的思路，本质是将输入句子或其中的词Token做Embedding后，输入循环神经网络中作为源句的表示，这一部分称为Encoder；另一部分生成端在每一个位置同样通过循环神经网络，循环输出对应的Token，这一部分称为Decoder。通过两个循环神经网络连接Encoder和Decoder，可以将两个平行表示连接起来。
   - ![](https://p1.meituan.net/travelcube/71ee857a5f58390785c10738d57d5c7970847.png)
- (2) attention
   - 本质思想是获取两端的某种权重关系，即在Decoder端生成的词和Encoder端的某些信息更相关。它也同样可以处理多模态的问题，比如Image2Text任务，通过CNN等将图片做一个关键特征的向量表示，将这个表示输出到类似的Decoder中去解码输出文本，视频语音等也使用同样的方式
   - ![](https://p0.meituan.net/travelcube/c64908b07137477135f9b7aa2927daea170277.png)

- Encoder-Decoder是一个非常通用的框架，它同样深入应用到了文本生成的三种主流方法，分别是规划式、抽取式和生成式，下面看下这几类方法各自的优劣势：
   - 规划式：根据结构化的信息，通过语法规则、树形规则等方式规划生成进文本中，可以抽象为三个阶段。宏观规划解决“说什么内容”，微观规划解决“怎么说”，包括语法句子粒度的规划，以及最后的表层优化对结果进行微调。
   - 抽取式：顾名思义，在原文信息中抽取一部分作为输出。可以通过编码端的表征在解码端转化为多种不同的分类任务，来实现端到端的优化。
      - 优势: 控制力极强、准确率较高，特别适合新闻播报等模版化场景。控制力极强，对源内容相关性好，改变用户行文较少，也不容易造成体验问题，可以直接在句子级别做端到端优化
      - 劣势是很难做到端到端的优化，损失信息上限也不高。
         - 在优化评估上，首先标题创意衡量的主观性很强，线上Feeds的标注数据也易受到其他因素的影响，比如推荐排序本身；其次，训练预测数据量差异造成OOV问题非常突出，分类任务叠加噪音效果提升非常困难。对此，我们重点在语义＋词级的方向上来对点击/转化率做建模，同时辅以线上E&E选优的机制来持续获取标注对，并提升在线自动纠错的能力。
         - 在受限上，抽取式虽然能直接在Seq级别对业务目标做优化，但有时候也须兼顾阅读体验，否则会形成一些“标题党”，亦或造成与原文相关性差的问题。对此，我们抽象了预处理和质量模型，来通用化处理文本创意内容的质控，独立了一个召回模块负责体验保障。并在模型结构上来对原文做独立表示，后又引入了Topic Feature Context来做针对性控制。
      - ![](https://p0.meituan.net/travelcube/94790c9b54cf6ad34fc800c1579416c3130763.png)
   - 生成式：通过编码端的表征，在解码端完成序列生成的任务，可以实现完全的端到端优化，可以完成多模态的任务。其在泛化能力上具有压倒性优势，但劣势是控制难度极大，建模复杂度也很高。
- 目前的主流的评估方法主要基于数据和人工评测。基于数据可以从不同角度衡量和训练目标文本的相近程度，如基于N-Gram匹配的BLUE和ROUGE等，基于字符编辑距离（Edit Distance）等，以及基于内容Coverage率的Jarcard距离等。基于数据的评测，在机器翻译等有明确标注的场景下具有很大的意义，这也是机器翻译领域最先有所突破的重要原因。但对于我们创意优化的场景来说，意义并不大，我们更重要的是优化业务目标，多以线上的实际效果为导向，并辅以人工评测。
- 另外，值得一提的是，近两年也逐渐涌现了很多利用GAN（Generative Adversarial Networks，生成对抗网络）的相关方法，来解决文本生成泛化性多样性的问题。有不少思路非常有趣，也值得尝试，只是GAN对于NLP的文本生成这类离散输出任务在效果评测指标层面，与传统的Seq2Seq模型还存在一定的差距，可视为一类具有潜力的技术方向。

#### 文本表示

- 整个2018年有两方面非常重要的工作进展：
   - Contextual Embedding：该方向包括一系列工作，如最佳论文Elmo(Embeddings from Language Models)，OpenAI的GPT(Generative Pre-Training)，以及谷歌大力出奇迹的BERT(Bidirectional Encoder Representations from Transformers)。解决的核心问题，是如何利用大量的没标注的文本数据学到一个预训练的模型，并通过通过这个模型辅助在不同的有标注任务上更好地完成目标。传统NLP任务深度模型，往往并不能通过持续增加深度来获取效果的提升，但是在表示层面增加深度，却往往可以对句子做更好的表征，它的核心思想是利用Embedding来表征上下文的的信息。但是这个想法可以通过很多种方式来实现，比如ELMo，通过双向的LSTM拼接后，可以同时得到含上下文信息的Embedding。而Transformer则在Encoder和Decoder两端，都将Attention机制都应用到了极致，通过序列间全位置的直连，可以高效叠加多层（12层），来完成句子的表征。这类方法可以将不同的终端任务做一个统一的表示，大大简化了建模抽象的复杂度。我们的表示也经历了从RNN到拥抱Attention的过程。
   
![](https://p1.meituan.net/travelcube/5f1b394d23c4fe5be6dfe885028a8668332821.png)

图5 GPT ELMo BERT模型结构
   - Tree-Based Embedding：另外一个流派则是通过树形结构进行建模，包括很多方式如传统的语法树，在语法结构上做Tree Base的RNN，用根结点的Embedding即可作为上下文的表征。Tree本身可以通过构造的方式，也可以通过学习的方式（比如强化学习）来进行构建。最终Task效果，既和树的结构（包括深度）有关，也受“表示”学习的能力影响，调优难度比较大。在我们的场景中，人工评测效果并不是很好，仍有很大继续探索的空间。

### 信息流中的应用

- 【2020-5-26】[大众点评信息流基于文本生成的创意优化实践](https://tech.meituan.com/2019/03/14/information-flow-creative-optimization-practices.html)
- 核心目标与推荐问题相似，提升包括点击率、转化率在内的通用指标，同时需要兼顾考量产品的阅读体验包括内容的导向性等
- 信息流中落地重点包括三个方向：
   - 文本创意：在文本方面，既包括了面向内容的摘要标题、排版改写等，也包括面向商户的推荐文案及内容化聚合页。它们都广泛地应用了文本表示和文本生成等技术，也是本文的主要方向。
   - 图像创意：图像方面涉及到首图或首帧的优选、图像的动态裁剪，以及图像的二次生成等。
   - 其他创意：包括多类展示理由（如社交关系等）、元素创意在内的额外补充信息。
![](https://p0.meituan.net/travelcube/a94ee4867832c8b91a9bae495d09eaf084451.png)

- 文本创意优化，在业务和技术上分别面临着不同的挑战。
   - 首先业务侧，启动创意优化需要两个基础前提：
      - 第一，衔接好创意优化与业务目标，因为并不是所有的创意都能优化，也不是所有创意优化都能带来预期的业务价值，方向不对则易蹚坑。
      - 第二，创意优化转化为最优化问题，有一定的Gap。其不同于很多分类排序问题，本身相对主观，所谓“一千个人眼中有一千个哈姆雷特”，创意优化能不能达到预期的业务目标，这个转化非常关键。
   - 其次，在技术层面，业界不同的应用都面临不一样的挑战，并且尝试和实践对应的解决方案。对文本创意生成来说，我们面临的最大的挑战包括以下三点：
      - 带受限的生成 生成一段流畅的文本并非难事，关键在于根据不同的场景和目标能控制它说什么、怎么说。这是目前挑战相对较大的一类问题，在我们的应用场景中都面临这个挑战。
      - 业务导向 生成能够提升业务指标、贴合业务目标的内容。为此，对内容源、内容表示与建模上提出了更高的要求。
      - 高效稳定 这里有两层含义，第一层是高效，即模型训练预测的效果和效率；第二层是稳定，线上系统应用，需要具备很高的准确率和一套完善的质量提升方案。

## 指针网络

- 【2021-3-24】Pointer-Generator模型，解决了摘要中存在事实性错误的问题。然后作者又向前走了一步，就是加入了Coverage机制来解决重复问题
- Pointer Networks天生具备输出元素来自输入元素这样的特点，于是它非常适合用来实现“复制”这个功能。Pointer Networks其实特别适合用于解决OOV（out of vocabulary）问题
- 演进路线
![](https://p1.pstatp.com/large/tos-cn-i-0022/f032ede80be342db930db99361fb9337)

## 多轮对话改写

【2021-3-31】指代消解
- 代词是用来代替重复出现的名词
- 例句：
  1. Ravi is a boy. He often donates money to the poor.
    - 先出现主语，后出现代词，所以流动的方向从左到右，这类句子叫**回指**(Anaphora)
  2. He was already on his way to airport.Realized Ravi.
    - 这种句子表达的方式的逆序的，这类句子叫**预指**(Cataphora)

- 【2020-6-19】[浅谈多轮对话改写](https://zhuanlan.zhihu.com/p/145057649)
   - 2000条日常对话数据进行了简单地统计，发现其中33.5%存在指代，52.7%存在省略的情况
   - 人脑具有记忆的能力，能够很好地重建对话历史的重要信息，自动补全或者替换对方当前轮的回复，来理解回复的意思。同样，为了让对话系统具备记忆的能力，通常会通过对话状态跟踪（DST），来存储对话历史中的一些重要信息。
   - DST的主要问题有：
      - 存储的信息量有限，对于长对话的历史信息无法感知；
      - 存储信息不会太丰富，否则会导致冗余信息太多；
      - 存储的信息不太便于对话系统中其他模块的任务使用。
   - rewrite模型分为Pipeline的方式和End2End的方式
      - ![](https://pic4.zhimg.com/80/v2-ba681d520212275b177bd76d845cb2bf_1440w.jpg)
      - Pipeline方法将rewrite任务分成两个子任务，即指代和零指代的检测以及指代消解
      - 基于模型复杂度和性能的考虑，End2End方法的探索一直没有停止，直到2019年出现了几篇比较优秀的工作，本文将其划分为3种类型：基于联合训练的方法、基于序列标注的方法和基于指针网络的方法。
- [ACL 2019 使用表达改写提升多轮对话系统效果](https://www.aminer.cn/research_report/5d527dd4d5e908133c946b07)
   - [代码实现](https://github.com/chin-gyou/dialogue-utterance-rewriter/issues/11#issuecomment-627871399)
   - 【2020-12-02】[优化版](https://github.com/liu-nlper/dialogue-utterance-rewriter)，使用transformer，ACL 2019论文复现，多轮对话重写：[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf)
      - 作者开源的代码是基于LSTM的，论文中基于Transformer的代码并未公布；
      - 论文实验所使用数据与公开的数据不一致，所以给出新的指标以供参考。
![](http://zhengwen.aminer.cn/a1.png)
- 模型框架
![](http://zhengwen.aminer.cn/a2.png)
- 对于任务导向型对话系统和闲聊型对话系统均有效果提升，实现了用更成熟的单轮对话技术解决多轮对话问题。
![](http://zhengwen.aminer.cn/a10.png)
- [Transformer多轮对话改写实践](https://zhuanlan.zhihu.com/p/137127209)
   - 介绍了多轮对话存在指代和信息省略的问题，同时提出了一种新方法-抽取式多轮对话改写，可以更加实用的部署于线上对话系统，并且提升对话效果
- 日常的交流对话中
   - 30%的对话会包含指代词。比如“它”用来指代物，“那边”用来指代地址
   - 同时有50%以上的对话会有信息省略
- 对话改写的任务有两个：1这句话要不要改写；2 把信息省略和指代识别出来。对于baseline论文放出的数据集，有90%的数据都是简单改写，也就是满足任务2，只有信息省略或者指代词。少数改写语句比较复杂，本文训练集剔除他们，但是验证集保留。
![](https://pic1.zhimg.com/80/v2-d80efd57b81c6ece955a247ca7247db4_1440w.jpg)
- Transformers结构可以通过attention机制有效提取指代词和上下文中关键信息的配对，最近也有一篇很好的工作专门用Bert来做指代消岐[2]。经过transformer结构提取文本特征后，模型结构及输出如下图。
![](https://pic4.zhimg.com/80/v2-0c4a789b68c60c8279dbd98fc18b5b2b_1440w.jpg)
- 输出五个指针中：
   - 关键信息的start和end专门用来识别需要为下文做信息补全或者指代的词；
   - 补全位置用来预测关键信息(start-end)插入在待改写语句的位置，实验中用插入位置的下一个token来表示；
   - 指代start和end用来识别带改写语句出现的指代词。
   - 当待改写语句中不存在指代词或者关键信息的补全时，指代的start和end将会指向cls，同理补全位置也这样。如同阅读理解任务中不存在答案一样，这样的操作在做预测任务时，当指代和补全位置的预测最大概率都位于cls时就可以避免改写，从而保证了改写的稳定性。
- 效果评估
   - 准确度
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 性能
   ![](https://pic3.zhimg.com/80/v2-75faf2bed618cf5170efa56d65cd88e2_1440w.jpg)
   - 对数据集的依赖
   ![](https://pic2.zhimg.com/80/v2-b22df166f10a6716b3db01e303f1a721_1440w.jpg)
   - 对负样本(不需要改写样本)的识别.基于指针抽取的方法对负样本的识别效果会更好。同时根据对长文本的改写效果观察，生成式改写效果较差。
   ![](https://pic3.zhimg.com/80/v2-b653c7da5923236991b6b0c5f973703a_1440w.jpg)
- 示例

|A1|B1|A2|算法改写结果|用户标注label|
|---|---|---|---|---|---|
|你知道板泉井水吗 | 知道 | 她是歌手 | 板泉井水是歌手 | 板泉井水是歌手|
|乌龙茶 | 乌龙茶好喝吗 | 嗯好喝 | 嗯乌龙茶好喝 | 嗯乌龙茶好喝|
|武林外传 | 超爱武林外传的 | 它的导演是谁 | 武林外传的导演是谁 | 武林外传的导演是谁|
|李文雯你爱我吗 | 李文雯是哪位啊 | 她是我女朋友 | 李文雯是我女朋友 | 李文雯是我女朋友|
|舒马赫 | 舒马赫看球了么 | 看了 | 舒马赫看了 | 舒马赫看球了|

- 结论
   - 抽取式文本改写和生成式改写效果相当
   - 抽取式文本改写速度上绝对优于生成式
   - 抽取式文本改写对训练数据依赖少
   - 抽取式文本改写对负样本识别准确率略高于生成式
- 参考
  - 2019.07，ACL，微信AI Lab [Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf)
  - 2020.12.21，AAAI 2021，Netease Games AI Lab，[SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration](https://arxiv.org/abs/2008.01474)，
- 几种模型：
   - CopyNet：基于LSTM 的Seq2Seq模型，使用attention 和 copy 机制 （基本的指针网络）
   - PAC：首先使用bert fintune 的 H，U 词向量，然后直接喂给标准的指针生成网络
   - **T-Ptr-λ**：就上面讲的那一篇腾讯论文的方法，主要是利用6层-6层 的Transformer Encoder Decoder结构
   - Seq2Seq-Uni: 这个baseline 是将Transformer 的各个block 层的输出做了统一结合，然后应用到 seq2seq 结构
   - **SARG**：论文提出的模型，超参数使用 α=3，λ=1 

## 小样本学习

- 从「文本增强」和「半监督学习」这两个角度出发，谈一谈如何解决少样本困境
   - ![](https://pic4.zhimg.com/v2-ad143542b083a12c46673589de4f136f_b.jpg)
- NLP中文本增强方法
   - ![](https://pic4.zhimg.com/v2-938b727e9572bb86dce68e37b68db40f_b.jpg)
- 详情参考：
   - [标注样本少怎么办？「文本增强+半监督学习」总结（从PseudoLabel到UDA/FixMatch） - JayLou娄杰的文章](https://zhuanlan.zhihu.com/p/146777068)
   - [一文了解NLP中的数据增强方法-简枫的文章](https://zhuanlan.zhihu.com/p/145521255)
   - 【2020-7-7】【一键中文数据增强工具】'NLP Chinese Data Augmentation' by 425776024 [GitHub](https://github.com/425776024/nlpcda),支持如下功能：
      - 1.随机实体替换
      - 2.近义词
      - 3.近义近音字替换
      - 4.随机字删除（内部细节：数字时间日期片段，内容不会删）
      - 5.新增：NER类 BIO 数据增强
      - 6.新增 随机置换邻近的字：研表究明，汉字序顺并不定一影响文字的阅读理解<<是乱序的
      - 7.新增百度中英翻译互转实现的增强
      - 8.新增中文等价字替换（1 一 壹 ①，2 二 贰 ②）

- 【2020-7-7】创新奇智有关少样本学习（Few-shot Learning）的研究论文《Prototype Rectification for Few-Shot Learning》被全球计算机视觉顶会ECCV 2020接收为Oral论文，入选率仅2%。[参考地址](https://www.toutiao.com/i6846586199994270222/)
- 本文提出一种简单有效的少样本学习方法，通过减小类内偏差和跨类偏差进行原型校正，从而显著提高少样本分类结果，并且给出理论推导证明本文所提方法可以提高理论下界，最终通过实验表明本方法在通用数据集中达到了最优结果，论文被ECCV 2020 接收为Oral。

- 【2020-8-19】[少样本学习中的深度学习模型综述](https://arxiv.org/abs/2008.06365v2)

- 【2020-11-12】Few-shot Learning最新进展调研
- 【2021-1-20】**少样本学习**可以使一个模型快速承接各种任务。但是为每个任务更新整个模型的权重是很浪费的。最好进行局部更新，让更改集中在一小部分参数里。有一些方法让这些微调变得更加有效和实用，包括：
   - 使用 adapter（Houlsby et al., 2019、Pfeiffer et al., 2020a、Üstün et al., 2020）
   - 加入稀疏参数向量（Guo et al., 2020）
   - 仅修改偏差值（Ben-Zaken et al., 2020）
- 能够仅基于几个范例就可以让模型学会完成任务的方法，大幅度降低了机器学习、NLP 模型应用的门槛。
- 这让模型可以适应新领域，在数据昂贵的情况下为应用的可能性开辟了道路。
- 对于现实世界的情况，我们可以收集上千个训练样本。模型同样也应该可以在少样本学习和大训练集学习之间无缝切换，不应受到例如文本长度这样的限制。在整个训练集上微调过的模型已经在 SuperGLUE 等很多流行任务中实现了超越人类的性能，但如何增强其少样本学习能力是改进的关键所在。


## 多模态学习

【2021-8-2】[多模态为什么比单模态好？第一份严谨证明来了！](https://mp.weixin.qq.com/s/s_VE63vy8tkXCMfC_Ab2ig)：一篇多模态学习理论分析的文章，从数学角度证明了潜表征空间质量直接决定了多模态学习模型的效果。而在充足的训练数据下，模态的种类越丰富，表征空间的估计越精确
- [What Makes Multimodal Learning Better than Single (Provably)](https://arxiv.org/pdf/2106.04538.pdf),基于一种经典的多模态学习框架，即无缝进行潜空间学习（Latent Space Learning）与任务层学习（Task-specific Learning）。
- 两个角度回答了这个问题：
  - （When）在何种条件下，多模态学习比单模态学习好
  - （Why）是什么造成了其效果的提升
- 从真实世界收集的多模态情绪分析的数据集IEMOCAP(Interactive Emotional Dyadic Motion Capture)，它包括三种模态：文字（Text）、视频（Video）和音频（Audio）。首先使用离线的特征抽取工具对三种模态信息提取好特征：Audio 100维，Text 100维以及Video 500维。这个数据集的分类有六种，分别是快乐、悲伤、中立、愤怒、兴奋和沮丧。使用了13200条数据做训练，3410条做测试。实验模型上，潜空间的映射使用了一层线性层+Relu，任务层使用了一层Softmax。在对比实验中，如果是单模态模型，则直接进行对应特征映射；如果是多模态模型，则首先进行多模态特征拼接，然后再进行映射。
- 论文使用机器自动生成的方式，构造了不同的模态关联数据用于验证。这里考虑三种情况：（1）模态之间完全不共享信息，即每个模态只包含模态特定的信息。（2）所有模态之间共享所有信息，没有区分。（3）介于两者之间，既共享一部分信息，也保有模态特定信息。

## 纠错

### 错误类型

在中文中，常见的错误类型大概有如下几类：

由于字音字形相似导致的错字形式：体脂称—>体脂秤
- **多**字错误：iphon**e**e —> iphone
- **少**字错误：爱有天意 --> **假如**爱有天意
- **顺序**错误: 表达难以 --> 难以表达

### 纠错模块

纠错一般分两大模块：
- 错误**检测**：识别错误发生的位置
- 错误**纠正**：对疑似的错误词，根据字音字形等对错词进行候选词召回，并且根据语言模型等对纠错后的结果进行排序，选择最优结果。
如下图所示：
- ![](https://img-blog.csdnimg.cn/20210317195613919.png)

### 拼音纠错

【2021-9-28】语音识别有误，NLU环节如何补救？基于拼音的纠错

```python
from pypinyin import lazy_pinyin,pinyin
print(lazy_pinyin('江西赣州')) #返回拼音
# ['jiang', 'xi', 'gan', 'zhou']
print(lazy_pinyin('江西赣州',1))    #带声调的拼音
# ['jiāng', 'xī', 'gàn', 'zhōu']
print(lazy_pinyin('江西赣州',2))    #另一种拼音风格
# ['jia1ng', 'xi1', 'ga4n', 'zho1u']
print(lazy_pinyin('江西赣州',3))    #只返回拼音首字母
# ['j', 'x', 'g', 'zh']
print(lazy_pinyin('重要',1))    #能够根据词组智能识别多音字
# ['zhòng', 'yào']
print(lazy_pinyin('重阳',1))
# ['chóng', 'yáng']
print(pinyin('江西')) #返回拼音
# [['jiāng'], ['xī']]
print(pinyin('重阳节',heteronym=True)) #返回多音字的所有读音
# [['zhòng', 'chóng', 'tóng'], ['yáng'], ['jié', 'jiē']]
import jieba
x='中英文混合test123456'
print(lazy_pinyin(x))  #自动调用已安装的pypinyin扩展分词功能
# ['zhong', 'ying', 'wen', 'hun', 'he', 'test123456']
print(list(jieba.cut(x))) #自动调用jieba扩展分词功能
# ['中英文', '混合', 'test123456']
x='江西的桃子真好吃'
print(sorted(x,key=lambda ch:lazy_pinyin(ch))) #按拼音对汉字进行排序
# ['吃', '的', '好', '江', '桃', '西', '真', '子']
```

### 主流方案

业界纠错主流方案一般包括错误检测→候选召回→候选排序三部分，根据作者调研工作其技术方案主要可分为以下三种：
- ① 基于**规则**式的通用纠错库pycorrector；
- ② 基于**大样本**训练深度学习模型的检错算法，以百度纠错系统为代表；
- ③ 基于垂直领域的**DCQC**纠错框架。

总结
- ① 从百度纠错系统和IJCNLP第一名使用方法来看，当具有充足的标注语料时，将LSTM+CRF这种序列标注方式用来进行错误位置检测是一个比较好的方法，但当标注语料有限时，该种方法难以应用落地；
- ② 基于机器翻译的纠错方法是目前业界及学术界关注的焦点，寿险场景也可以尝试使用NMT的方法来测试效果，然而该技术落地的条件是仍然需要大量的标注数据。
那么在标注资源受限的条件下，如何部署高性能、高效率的纠错系统将成为寿险场景中纠错问题所面临也是急需解决的关键挑战。

参考：[NLP上层应用的关键一环——中文纠错技术简述](https://zhuanlan.zhihu.com/p/82807092)


#### pycorrector

pycorrector通用纠错模块是github上的开源项目，它提供了一种规则式检错、纠错方案，该方法因逻辑清晰、不依赖大量标注样本从而较容易实现落地，因此为广大研究者提供了良好的借鉴意义。

该模块在检错部分利用常用字典、混淆字典与传统语言模型共同判断当前位置是否有错。制订了如下规则：
- ①当切词后的片段不在常用字典中或存在于混淆字典的映射对中时判定有错；
- ②计算传统语言模型概率是否低于门限并判错。
候选召回部分利用同音、同型召回候选字词，打分排序利用句子困惑度来计算候选词权重，从而对候选进行排序。

该方案是面向通用领域开发的，思路简单、易实现，但在保险垂直领域中容易得到较差的表现。

#### 百度纠错系统

百度纠错系统凭借其海量用户点击语料训练了基于深度学习的序列标注模型，方法如下：
- 错误检测：在错误检测部分，它利用10TB的无监督语料预训练Transformer/Lstm + CRF模型，再利用对齐语料（错误句子→正确句子）进行有监督学习该序列标注模型；
- 候选召回：在候选召回部分，利用了对齐语料和对齐模型构建字级别、词级别、音级别的混淆字典，先利用字、音混淆字典初步召回候选，然后再利用词级别混淆字典和语言模型二次筛选候选，从而形成最终候选；
- 候选排序：利用上下文DNN特征和人工提取的形音、词法、语义等特征一起训练GBDT&LR的排序模型。

#### 腾讯基于垂域的DCQC纠错框架

腾讯提出一套针对垂直业务的通用纠错框架——DCQC（Domain Common Query Correction），由召回层和决策层两层组成。该方法可以方便扩展到其他领域[2] 。
- 召回层：每个领域建立属于自己的一个数据库，建立字和拼音两个维度的倒排索引，通过检索排序的方式召回一定量的候选；
- 决策层：人工提取用户特征、拼音特征等5种类型特征训练svm二分类模型用于候选排序。

### 学术界的进展

学术界近期发表的中文纠错论文主要集中在中文纠错比赛项目上，如：SIGHAN举办的CSC（中文拼写纠错）比赛、IJCNLP举办的CGED（中文语法错误诊断）比赛及NLPCC举办的GGED（中文语法错误诊断）比赛等。

#### LSTM+CRF序列标注用于错误检测

IJCNLP2017 [3]和2018 [4]的CGED比赛任务中，第一名的方法都用了LSTM+CRF用于错误位置的检测。然而文中，该方法在错误位置检测方面的F1值最高也只有0.39，目前还没有达到工业化使用的要求。

####  NMT基于神经机器翻译模型纠错方法

基于机器翻译模型的纠错方法的思想是，将纠错任务类比于机器翻译任务，预想利用模型将错误语句翻译为正确语句，利用Seq2seq模型完成端到端的纠正过程。

YUAN等学者在2016首次利用双向RNN编码器+单向RNN解码器+Attention的经典结构对CoNll-2014(英文语料)进行语法纠错，达到当时最好效果，其统计指标F0.5为39.9%；其后，有道团队在NLPCC-2018举办的CGEC比赛中利用Transformer的翻译模型达到比赛最好的结果，其F0.5值 为29.9% [5]。


### 深度学习纠错

【2021-3-20】[中文纠错（Chinese Spelling Correct）最新技术方案总结](https://blog.csdn.net/BGoodHabit/article/details/114589007), 总结：
- ![](https://img-blog.csdnimg.cn/20210319101130373.png)

#### FASPell（爱奇艺)

- 论文：[FASPell: A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based On DAE-Decoder Paradigm](https://www.aclweb.org/anthology/D19-5522.pdf)
- [code](https://github.com/iqiyi/FASPell)

爱奇艺团队2019年发表在EMNLP会议上的论文，通过训练一个以BERT为基础的**深度降噪编码器**(DAE)和以置信度-字音字行相似度为基础的**解码器**（CSD)进行中文拼写纠错。在DAE阶段，BERT可以动态生成候选集去取代传统的混淆集，而CSD通过计算置信度和字音字形相似度两个维度去取代传统的单一的阈值进行候选集的选择，提高纠错效果，取得了SOTA( state-of-the-art)的一个效果。

模型主要分两大块组成：
- 第一：Masked Language Model (bert)
  - 是一个自动编码器（DAE), 基于bert模型，每次获取预测词的top k个候选字。
- 第二：Cofidence-Similarity Decoder
  - 该部分是一个解码器，通过编码器输出的置信度confidence分值和中文字音字形的相似度similarity分值两个维度进行候选集的过滤和刷选，选择最佳候选的路径作为输出。

![模型结构](https://img-blog.csdnimg.cn/20210310114404890.png)

Similarity：包含形体相似（visual similarity) 和字音相似（phonological similarity) ，本论文通过这两种情况进行汉字的相似度计算。
- （1）Visual similarity
  - 在字形上采用Unicode标准的Ideographic Description Sequence (IDS)表征。描述汉字更细粒度字体的笔画结构和布局形式（相同的笔画和笔画顺序，例如：“牛”和“午”相似度不为1），比起纯笔画或者五笔编码等计算方式要精细。如下图，展示的是“贫”字的IDS计算,每个字符编码是每个叶子节点从根节点到的搜索路径
  - ![](https://img-blog.csdnimg.cn/20210310171915671.png)
- （2）Phonological similarity
  - 在字音上，使用了所有的CJK语言中的汉字发音，本文用了普通话（MC)，粤语 (CC)，日语(JO)，韩语(K)和越南语 (V) 中的发音，计算拼音的编辑距离作为相似度分值，最终做一个归一化操作。如下图展示的是字形和字音相似度结果
  - ![](https://img-blog.csdnimg.cn/20210310105721409.png)


训练过程
- 预训练MLM部分
  - 按照bert原始的mlm方式：对句子15%部分进行置换操作，其中80%替换成[MASK]，10%替换成随机的一个词， 10%替换成原始的词
- fine-tune MLM
  - 第一部分： 对于没有错词的文本，按照bert原始的mlm方式
  - 第二部分： 对于有错词的文本， 两种样本构造：1）用原始错误的词作为mask，正确的词作为target目标词； 2）同时对正确的词也做mask，用原始的词作为mask，目标词为原始词
- 训练CSD
  - 训练完encoder，然后根据encoder预测出的confidence分值和字音字形相似度分值，绘制散点图，通过人工观察用直线拟合，确定能够分开正确和错误的点的分界曲线，论文最终给出的曲线为： 0.8 × \times× confidence + 0.2 × \times× similarity >0.8


优点和缺点
- 优点：
  - 用MLM预测动态生成候选集，取代了传统的混淆表，整个流程相对简单
  - 通过字音字形特征，用多种语言表达的字音特征进行曲线拟合，可解释好
- 缺点
  - 不包括少词和多词错误形式纠错
  - 训练不是end-to-end的过程，CSD分界曲线靠观察拟合生成

#### SpellGCN （阿里）

技术方案
- [SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://arxiv.org/pdf/2004.14166.pdf)
- [code](https://github.com/ACL2020SpellGCN/SpellGCN)

阿里团队于2020年在ACL会议上发表，主要通过graph convolutional network (GCN）对字音和字形结构关系进行学习，并且将这种字音字形的向量融入到字的embedding中，在纠错分类的时候，纠错更倾向于预测为混淆集里的字。模型训练是一个end-to-end的过程，试验显示，在公开的中文纠错数据集上有一个较大的提升

模型也主要分两部分组成：
- 第一部分：**特征提取**器, 特征提取器基于12层的bert最后一层的输出
- 第二部分：**纠错分类**模型, 通过GCN学习字音字形相似结构信息，融合字的语义信息和字的结构信息，在分类层提高纠错准确率。

![](https://img-blog.csdnimg.cn/2021031318043625.png)

- 优点
  - 将字音字形特征通过GCN学习嵌入到语义字向量表征中去，使得错词在纠错的时候能够更倾向于纠正为混淆集中的词，提高纠错准确率
- 缺点
  - 不包括少词和多词错误形式纠错
  - 混淆集在测试集的覆盖率影响效果评估

#### Soft-Mask BERT （字节）

技术方案
- [Spelling Error Correction with Soft-Masked BERT](https://arxiv.org/pdf/2005.07421.pdf)

字节团队于2020年发表在ACL会议，将纠错任务分成两部分：detection network（错误检测）和correction network（错误纠正）。在错误检测部分，通过BiGRU模型对每个输入字符进行错误检测，得到每个输入字符的错误概率值参与计算soft-masked embedding作为纠错部分的输入向量，一定程度减少了bert模型的过纠问题，提高纠错准确率。

![](https://img-blog.csdnimg.cn/20210314154955483.png)

- 优点
  - 对中文纠错分成错误检测和错误纠正，并给通过soft-masking技术将两者进行融合，减少bert的过纠问题，提高准确率
- 缺点
  - 不包括少词和多词错误形式纠错
  - 模型没有引入字音字形相似性约束，虽然引入了错误检测模块，通过soft mask技术减少过纠问题，但是只是依赖bert的语义识别进行纠错，不够鲁棒

#### Spelling Correction as a Foreign Language （ebay)

技术方案
- [Spelling Correction as a Foreign Language](http://ceur-ws.org/Vol-2410/paper28.pdf)

ebay团队于2019年发表在SIGIR 2019 eCom会议上，将纠错任务当做机器翻译任务，基于encoder-decoder框架。

![](https://img-blog.csdnimg.cn/20210314173324120.png)

训练过程
- 训练数据： 主要基于电商平台用户搜索session，通过跟踪用户行为构造样本数据，一个前提假设是，在一个session内，在没有找到满足用户意图商品情况下，用户会主动修改搜索query直到搜索结果符合用户的意图，而基于这样的一个搜索组成的query序列，从中可以挖掘出潜在的错误和正确的query pair对构造训练样本。
- 训练过程： end-to-end训练seq2seq模型。

优点和缺点
- 优点
  - 将纠错当做翻译任务去做，可以对不同类型的错误形式：错词，少词，多词等进行纠错
- 缺点
  - 模型没有对字音字形相似关系的学习，纠错后的结果不受约束，很容易出现过纠错和误纠问题

#### HeadFit (加利福尼亚大学)

技术方案
- [Domain-shift Conditioning using Adaptable Filtering via Hierarchical Embeddings for Robust Chinese Spell Check](https://arxiv.org/pdf/2008.12281.pdf)

在中文公开纠错数据集上取得了不错的效果。纠错模块分两部分：第一部分是基于bert的base模型，输出可能纠错后的结果，第二部分是一个filter模块，主要基于treeLSTM模型学习出字的hierarchical embedding，通过向量相似度衡量两个字的相似度，取代了预先设定好的混淆集并且可以通过模型的自适应学习，发现字与字之间新的混淆关系能力，通过filter模块，进一步过滤bert模型的过纠等问题，提高准确率。

![](https://img-blog.csdnimg.cn/20210318173217356.png)

- 优点
  - 通过treeLSTM模型学习字的hierarchical向量，不依赖于固定的混淆集，让模型自适应学习字形结构向量特征，通过模型的学习能够扩充混淆关系集，并且对新词和新的领域有较好的适应能力
- 缺点
  - 模型学习中没有利用到字与字之间的拼音相似关系
  - 模型训练不是一个end-to-end的过程


# nlp工具集合

## Demo集合

- 【2020-5-19】NLP各类应用Demo汇总（Colab实现），[The Super Duper NLP Repo](https://notebooks.quantumstat.com/)，如文本生成、对话、问答、翻译等
![](https://image.jiqizhixin.com/uploads/editor/de27f3bd-2740-4097-b6d6-7340aeeb32cd/302.png)

<iframe src='https://notebooks.quantumstat.com/' frameborder='0' scrolling='no' allowfullscreen="true"></iframe>


## 数据集

- 中文汉字集合、停用词、生僻字[数据集github](https://github.com/elephantnose/characters)

## web平台

- 【2021-7-15】[nlp-web-demo](https://github.com/lonly197/nlp-web-demo)页面集成了Stanford, Hanlp, FNLP, Thulc, FudanDNN, Boson NLP and Jieba等方法，对比分词、词性标注、句法、摘要等效果
- watson的[Natural Language Understanding](https://natural-language-understanding-demo.ng.bluemix.net/)工具包 demo，支持中文，可以实测。

## 开源nlp工具包

[nlp工具包汇总推荐](https://www.biaodianfu.com/nlp-tools.html)

|工具包|作者|基本功能|特殊功能|擅长语种|备注|
|---|---|---|---|---|---|
|[jieba](https://github.com/fxsjy/jieba/)|个人|分词/关键词/词性|自定义字典/三种模式分词|中文|-|
|pkuseg|北大|分词/词性|自定义字典/多领域分词/高准确/用户自训练/文件分词|中文|优于jieba+THULAC|
|[LTP](https://github.com/HIT-SCIR/ltp)|哈工大|分词/词性/句法分析|-|中文|pyltp, [demo](http://ltp.ai/demo.html)|
|THULAC|清华|分词/词性/句法分析|-|中文|中文词法分析工具包，准确率高，速度快; [demo](http://thulac.thunlp.org/demo)|
|stanfordNLP|斯坦福|分词/词性/句法分析/实体识别/语法树|词干还原/wordnet(同义词)/语料库|英文|Java+python，依赖pytorch,[demo](https://corenlp.run/)|
|[allennlp](https://allennlp.org/)|组织|-|多种高级NLP能力（阅读理解/指代消解等）|英文|紧跟sota，[demo](https://demo.allennlp.org/reading-comprehension/bidaf-elmo)|
|wastonnlp|IBM|-|-|英文|[demo](https://natural-language-understanding-demo.ng.bluemix.net/)|
|NLTK|宾大|分词/词性/句法分析/实体识别|句法可视化|英文|学术界|
|TextBlob|个人|分词/词性/实体识别/摘要/关键词|文本分类/情感分析/词根化/纠错/机器翻译/wordnet|英文|Nltk抽象而来|
|SnowNLP|个人|分词/词性/分句/摘要/关键词|文本分类/情感分析/繁简/拼音/相似度|中文|TextBolb非官方中文版，不依赖nltk|
|DeepNLP|-|分词/词性/句法/摘要/关键词/实体识别|文本分类/pipeline/web|api/用户自训练|中文||
|小明NLP|个人|分词/词性/摘要/关键词|自定义字典/文本分类/情感分析/繁简/拼音/纠错/偏旁部首|中文||
|spaCy|-|分词/词性/摘要/关键词/分句/句法/语法树|文本相似度/统计学习/深度学习/可视化|53种语言|工业级，cython，高性能，TensorFlow/pytorch支持，标注工具 https://prodi.gy/|
|FoolNLP|复旦|分词/关键词/词性|自定义字典|中文|非最快但最准，bi-LSTM|
|HanNLP|-|分词/词性/实体识别/摘要/关键词/句法分析|繁简/拼音/纠错/聚类/w2v|中文|Java→python，支持TensorFlow|
|bosonnlp|公司|-|-|中文|[Demo](http://static.bosonnlp.com/demo)，[云孚科技](https://www.yunfutech.com/products/yfnlp/demo?tab=0&module=pos)|

杭州[实在智能](https://www.ai-indeed.com/)（阿里P8创立的公司）的[nlp-base基础能力demo](https://nlp-base.external.ai-indeed.com/)


### 词性解释

- n/名词 np/人名 ns/地名 ni/机构名 nz/其它专名
- m/数词 q/量词 mq/数量词 t/时间词 f/方位词 s/处所词
- v/动词 a/形容词 d/副词 h/前接成分 k/后接成分
- i/习语 j/简称 r/代词 c/连词 p/介词 u/助词 y/语气助词
- e/叹词 o/拟声词 g/语素 w/标点 x/其它

### Jieba结巴

[github地址](https://github.com/fxsjy/jieba/blob/67fa2e36e72f69d9134b8a1037b83fbb070b9775/jieba/__init__.py#L380)

#### 分词
- jieba.**cut** 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型
- jieba.**cut_for_search** 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
- 待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8
- jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用jieba.lcut 以及 jieba.lcut_for_search 直接返回 list
- jieba.**Tokenizer**(dictionary=DEFAULT_DICT) 新建**自定义**分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。

结巴中文分词采用的算法
- 基于**Trie树**（前缀树）结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的**有向无环图**（DAG)
- 采用了**动态规划**查找最大概率路径, 找出基于词频的**最大**切分组合
- 对于未登录词，采用了基于汉字成词能力的**HMM模型**，使用了**Viterbi算法**

三种分词模式：
- **精确**模式，试图将句子最精确地切开，适合文本分析；
- **全**模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
- **搜索引擎**模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

自定义字典
- 开发者可以指定自己的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
- 用法： jieba.**load_userdict**(file_name) # file_name 为文件类对象或自定义词典的路径
- 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：**词语**、**词频**（可省略）、**词性**（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。
- 词频省略时使用自动计算的能保证分出该词的词频。
- 注：词性可以自定义，如norm、project都可以

```
创新办 3 i
云计算 5
凱特琳 nz
台中
```

调整字典
- 使用 **add_word**(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。
- 使用 **suggest_freq**(segment, tune=True) 可调节**单个词语**的词频，使其能（或不能）被分出来。

注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。

延迟加载机制
- jieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。
- 手工初始化：jieba.initialize()  # 手动初始化（可选）
- 在 0.28 之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径: [示例](https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py)
  - jieba.set_dictionary('data/dict.txt.big')


#### 并行分词

- 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升
- 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows
- 用法：
  - jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数
  - jieba.disable_parallel() # 关闭并行分词模式
- [例子](https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py)

实验结果：
> 在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。

注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。

#### Tokenize 分词位置

返回词语在原文的起止位置

注意，输入参数只接受 unicode

```python
result = jieba.tokenize(u'永和服装饰品有限公司')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))
```

#### 命令行分词

使用示例：python -m jieba news.txt > cut_result.txt

```
使用: python -m jieba [options] filename

结巴命令行界面。

固定参数:
  filename              输入文件

可选参数:
  -h, --help            显示此帮助信息并退出
  -d [DELIM], --delimiter [DELIM]
                        使用 DELIM 分隔词语，而不是用默认的' / '。
                        若不指定 DELIM，则使用一个空格分隔。
  -p [DELIM], --pos [DELIM]
                        启用词性标注；如果指定 DELIM，词语和词性之间
                        用它分隔，否则用 _ 分隔
  -D DICT, --dict DICT  使用 DICT 代替默认词典
  -u USER_DICT, --user-dict USER_DICT
                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用
  -a, --cut-all         全模式分词（不支持词性标注）
  -n, --no-hmm          不使用隐含马尔可夫模型
  -q, --quiet           不输出载入信息到 STDERR
  -V, --version         显示版本信息并退出

如果没有指定文件名，则使用标准输入。
```

#### 关键词

（1）基于 TF-IDF 算法的关键词抽取 import jieba.analyse
- jieba.analyse.**extract_tags**(sentence, topK=20, withWeight=False, allowPOS=())
   - sentence 为待提取的文本
   - topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
   - withWeight 为是否一并返回关键词权重值，默认值为 False
   - allowPOS 仅包括指定词性的词，默认值为空，即不筛选
- jieba.analyse.**TFIDF**(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件
[代码示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py) （关键词提取）

- 关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径
   - 用法： jieba.analyse.**set_idf_path**(file_name) # file_name为自定义语料库的路径
   - 自定义语料库[示例](https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big), [用法示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py)
- 关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径
   - 用法： jieba.analyse.**set_stop_words**(file_name) # file_name为自定义语料库的路径
   - 自定义语料库[示例](https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt), 用法[示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py)
- 关键词一并返回关键词权重值示例
  - 用法[示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py)

(2) 基于 TextRank 算法的关键词抽取

算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)

基本思想:
- 将待抽取关键词的文本进行分词
- 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
- 计算图中节点的PageRank，注意是无向带权图
使用示例: [test/demo.py](https://github.com/fxsjy/jieba/blob/master/test/demo.py)

- jieba.analyse.**textrank**(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。
- jieba.analyse.TextRank() 新建自定义 TextRank 实例

#### 词性标注

- jieba.posseg.**POSTokenizer**(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。
- 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法

更多内容：[python使用结巴中文分词以及训练自己的分词词典](https://ptorch.com/news/182.html), [github代码示例](https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py)




```python
# -*- coding:utf-8 -*-
import jieba

text = '我来到北京清华大学'
# （1）添加或管理自定义词典，dict.txt，一个词占一行；
# 每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。
# file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。
# 实例：
# 创新办 3 i
# 不处理 nr
# 不还款
jieba.load_userdict("./dict.txt")
# （2）修改字典
# 使用add_word(word, freq=None, tag=None)和del_word(word)可在程序中动态修改词典。
# 使用suggest_freq(segment, tune=True)可调节单个词语的词频，使其能（或不能）被分出来。
jieba.suggest_freq('不处理',True)
jieba.add_word('不处理',tag='d')
jieba.add_word('中国银行APP',tag='d')
jieba.del_word('不处理')
word_list = jieba.cut("我今天不处理逾期信用贷款，因为你们中国银行APP根本打不开")
print("|".join(word_list))
# （4）分词
default_mode = jieba.cut(text) # 精确
full_mode = jieba.cut(text,cut_all=True) # 全模式
search_mode = jieba.cut_for_search(text) # 搜索引擎

print("精确模式:","/".join(default_mode))
print("全模式:","/".join(full_mode))
print("搜索引擎模式:","/".join(search_mode))
# （5）关键词提取
tags = jieba.analyse.extract_tags(text,2)
print("关键词抽取:","/".join(tags))

```


### 北大pkuseg —— 超过jieba+THULAC

北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。pkuseg具有如下几个特点：
- **多领域**分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了**新闻**领域，**网络**领域，**医药**领域，**旅游**领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考txt。
- 更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。
- 支持用户**自训练**模型。支持用户使用全新的标注数据进行训练。
- 支持词性标注。

```python
import pkuseg

seg = pkuseg.pkuseg()   # 以默认配置加载模型
seg = pkuseg.pkuseg(model_name='medicine')  # 程序会自动下载所对应的细领域模型, news/web/tourism/medicine
seg = pkuseg.pkuseg(postag=True)  # 开启词性标注功能
seg = pkuseg.pkuseg(user_dict='my_dict.txt')  # 给定用户词典为当前目录下的"my_dict.txt"

text = seg.cut('我爱北京天安门')  # 进行分词
print(text)

# 对input.txt的文件分词输出到output.txt中, 开8个进程
pkuseg.test('input.txt', 'output.txt', nthread=8)
# 模型训练
pkuseg.train(trainFile, testFile, savedir, train_iter = 20, init_model)
```

[pkuseg](https://github.com/lancopku/pkuseg-python)分词示例，及词云可视化

```python
import pkuseg
from collections import Counter
import pprint
from wordcloud import WordCloud
import matplotlib.pyplot as plt

with open("data/santisanbuqu_liucixin.txt", encoding="utf-8") as f:
    content = f.read()

with open("data/CNENstopwords.txt", encoding="utf-8") as f:
    stopwords = f.read()

lexicon = ['章北海', '汪淼', '叶文洁']
seg = pkuseg.pkuseg(user_dict=lexicon)
text = seg.cut(content)

new_text = []
for w in text:
    if w not in stopwords:
        new_text.append(w)
counter = Counter(new_text)
pprint.pprint(counter.most_common(50))

cut_text = " ".join(new_text)

wordcloud = WordCloud(font_path="font/FZYingXueJW.TTF", background_color="white", width=800, height=600).generate(
    cut_text)

plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

[img](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png) ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png)

### 哈工大LTP

- [LTP](https://github.com/HIT-SCIR/ltp), pyltp是python下对ltp(c++)的封装。
- 安装
  - pip install pyltp
  - 安装完毕后，还需要下载模型文件，文件[下载地址](http://ltp.ai/download.html)

代码

```python
from ltp import LTP

ltp = LTP()  # 默认加载 Small 模型
seg, hidden = ltp.seg(["他叫汤姆去拿外衣。"])
pos = ltp.pos(hidden)
ner = ltp.ner(hidden)
srl = ltp.srl(hidden)
dep = ltp.dep(hidden)
sdp = ltp.sdp(hidden)
```
pyltp示例

```python
import os
from pyltp import SentenceSplitter
from pyltp import Segmentor
from pyltp import Postagger
from pyltp import NamedEntityRecognizer
from pyltp import Parser
from pyltp import SementicRoleLabeller

LTP_DATA_DIR = 'D:\CodeHub\CutSeg\ltp_data_v3.4.0'  # ltp模型目录的路径

# 分句
sents = SentenceSplitter.split('元芳你怎么看？我就趴窗口上看呗！')  # 分句
print('\n'.join(sents))

# 分词
cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`
segmentor = Segmentor()  # 初始化实例
segmentor.load(cws_model_path)  # 加载模型
words = segmentor.segment('元芳你怎么看')  # 分词
print('/'.join(words))
segmentor.release()  # 释放模型

# 词性标注
pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`
postagger = Postagger()  # 初始化实例
postagger.load(pos_model_path)  # 加载模型
postags = postagger.postag(words)  # 词性标注
print('/'.join(postags))
postagger.release()  # 释放模型

# 命名实体识别
ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`
recognizer = NamedEntityRecognizer()  # 初始化实例
recognizer.load(ner_model_path)  # 加载模型
netags = recognizer.recognize(words, postags)  # 命名实体识别
print('/'.join(netags))
recognizer.release()  # 释放模型

# 依存句法分析
par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`
parser = Parser()  # 初始化实例
parser.load(par_model_path)  # 加载模型
arcs = parser.parse(words, postags)  # 句法分析
print("/".join("%d:%s" % (arc.head, arc.relation) for arc in arcs))
parser.release()  # 释放模型

# 语义角色标注
# srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl.model')  # 语义角色标注模型目录路径，模型目录为`pisrl.model`
srl_model_path = os.path.join(LTP_DATA_DIR,
                              'pisrl_win.model')  # windows系统要用不同的SRL模型文件，http://ospm9rsnd.bkt.clouddn.com/server/3.4.0/pisrl_win.model或http://model.scir.yunfutech.com/server/3.4.0/pisrl_win.model
labeller = SementicRoleLabeller()  # 初始化实例
labeller.load(srl_model_path)  # 加载模型
roles = labeller.label(words, postags, arcs)  # 语义角色标注
for role in roles:
    print(role.index, "".join(
        ["%s:(%d,%d)" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))
labeller.release()  # 释放模型
```



### 清华THULAC

[THULAC](https://github.com/thunlp/THULAC-Python)（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：
- 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
- 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达3％，词性标注的F1值可达到9％，与该数据集上最好方法效果相当。
- 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到3MB/s。

安装
- pip install thulac
- 安装时会默认下载模型文件。

示例：

```shell
#===========命令行============
#从input.txt读入，并将分词和词性标注结果输出到ouptut.txt中
python -m thulac input.txt output.txt
#如果只需要分词功能，可在增加参数"seg_only" 
python -m thulac input.txt output.txt seg_only
```
Python接口

```python
#=======================
import thulac

# 示例1
thu1 = thulac.thulac()  # 默认模式
text = thu1.cut("我爱北京天安门", text=True)  # 进行一句话分词
print(text)

# 示例2
thu2 = thulac.thulac(seg_only=True)  # 只进行分词，不进行词性标注
thu2.cut_f("input.txt", "output.txt")  # 对input.txt文件内容进行分词，输出到output.txt
# 完整格式
thulac(user_dict = None, model_path = None, T2S = False, seg_only = False, filt = False, max_length = 50000, deli='_', rm_space=False)

```
参数：
- user_dict：设置用户词典，用户词典中的词会被打上uw标签。词典中每一个词一行，UTF8编码
- model_path：设置模型文件所在文件夹，默认为models/
- T2S：默认False, 是否将句子从繁体转化为简体
- seg_only：默认False, 时候只进行分词，不进行词性标注
- filt：默认False, 是否使用过滤器去除一些没有意义的词语，例如“可以”。
- max_length：最大长度
- deli：默认为‘_’, 设置词与词性之间的分隔符
- rm_space：默认为False, 是否去掉原文本中的空格后再进行分词
- text：默认为False, 是否返回文本，不返回文本则返回一个二维数组([[word, tag]..]),seg_only模式下tag为空字符。


### 斯坦福stanfordNLP

多个版本：
- Stanford CoreNLP的源代码[stanfordnlp](https://stanfordnlp.github.io/stanfordnlp/)是使用Java写的，提供了Server方式进行交互。
  - pip install stanfordnlp
- [stanfordcorenlp](https://stanfordnlp.github.io/CoreNLP/)（推荐使用）：一个对Stanford CoreNLP进行了封装的Python工具包。

安装流程：
- 安装Python包：pip install stanfordcorenlp
- 下载安装JDK（版本要求JDK 1.8以上）
- 下载安装[Stanford CoreNLP文件即语言包](https://github.com/stanfordnlp/CoreNLP)。解压CoreNLP文件，并将中文包放在解压文件的根目录。

python调用

```python
from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('D:\\stanford-corenlp-full-2018-02-27', lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词, ['斯坦福', '大学', '自然', '语言', '处理', '包', 'StanfordNLP']
print(nlp.pos_tag(sentence))  # 词性标注, [('斯坦福', 'NR'), ('大学', 'NN'), ('自然', 'AD'), ('语言', 'NN'), ('处理', 'VV'), ('包', 'NN'), ('StanfordNLP', 'NN')]
print(nlp.ner(sentence))  # 实体识别, [('斯坦福', 'ORGANIZATION'), ('大学', 'ORGANIZATION'), ('自然', 'O'), ('语言', 'O'), ('处理', 'O'), ('包', 'O'), ('StanfordNLP', 'O')]
print(nlp.parse(sentence))  # 语法树, 略
print(nlp.dependency_parse(sentence))  # 依存句法, [('ROOT', 0, 5), ('compound:nn', 2, 1), ('nsubj', 5, 2), ('advmod', 5, 3), ('nsubj', 5, 4), ('compound:nn', 7, 6), ('dobj', 5, 7)]

#========服务方式调用=========
# 启动服务：java -mx6g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 5000
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost', port=9000, lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词
print(nlp.pos_tag(sentence))  # 词性标注
print(nlp.ner(sentence))  # 实体识别
print(nlp.parse(sentence))  # 语法树
print(nlp.dependency_parse(sentence))  # 依存句法
```

### TextBlob——nltk加工

[TextBlob](https://github.com/sloria/TextBlob)是一个用Python编写的开源的文本处理库。是自然语言工具包[NLTK](https://www.biaodianfu.com/nltk.html)的一个包装器，目的是抽象其复杂性。它可以用来执行很多自然语言处理的任务，比如，词性标注，名词性成分提取，情感分析，文本翻译，等等。

主要特性：
- 名词短语提取
- 词性标记
- 情绪分析
- 分类
- 由 Google 翻译提供的翻译和检测
- 标记（将文本分割成单词和句子）
- 词句、短语频率
- 解析
- n-gram
- 词变化（复数和单数化）和词形化
- 拼写校正
- 通过扩展添加新模型或语言
- WordNet 集成

安装
- pip install textblob

```python
from textblob import TextBlob
text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text) # 可能报错，此时需要安装nltk
```
完整示例

```python
from textblob import TextBlob
from textblob import Word

text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text)

print(blob.words)  # 分词
print(blob.tags)  # 词性标注
print(blob.noun_phrases)  # 短语抽取

# 分句+计算句子情感值
# 使用TextBlob情感分析的结果，以元组的方式进行返回，形式如(polarity, subjectivity).
# polarity 是一个范围为 [-1.0 , 1.0 ] 的浮点数, 正数表示积极，负数表示消极。
# subjectivity 是一个 范围为 [0.0 , 1.0 ] 的浮点数，其中 0.0 表示 客观，1.0表示主观的。
for sentence in blob.sentences:
    print(sentence + '------>' + str(sentence.sentiment.polarity))

# 词语变形(Words Inflection)
w = Word("apple")
print(w.pluralize())  # 变复数
print(w.pluralize().singularize())  # 变单数

# 词干化(Words Lemmatization)
w = Word('went')
print(w.lemmatize('v'))
w = Word('octopi')
print(w.lemmatize())

# 拼写纠正(Spelling Correction)
sen = 'I lvoe naturl language processing!'
sen = TextBlob(sen)
print(sen.correct())

# 句法分析(Parsing)
text = TextBlob('I lvoe naturl language processing!')
print(text.parse())

# N-Grams
text = TextBlob('I lvoe naturl language processing!')
print(text.ngrams(n=2))
```


### SnowNLP——繁体转换、idf，相似度

[SnowNLP](https://github.com/isnowfy/snownlp)是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。

主要特性：
- 中文分词（Character-Based Generative Model）
- 词性标注（TnT3-gram 隐马）
- 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）
- 文本分类（Naive Bayes）
- **转换成拼音**（Trie树实现的最大匹配）
- **繁体转简体**（Trie树实现的最大匹配）
- 提取文本关键词（TextRank算法）
- 提取**文本摘要**（TextRank算法）
- tf，idf
- Tokenization（分割成句子）
- **文本相似**（BM25）

安装：
- pip install snownlp

代码
```python
from snownlp import SnowNLP

s1 = SnowNLP('这个东西真心很赞')
print(s1.words) # ['这个', '东西', '真心', '很', '赞']
print(list(s1.tags)) # [('这个', 'r'), ('东西', 'n'), ('真心', 'd'), ('很', 'd'), ('赞', 'Vg')]
print(s1.sentiments) # 0.9769551298267365
print(s1.pinyin) # ['zhe', 'ge', 'dong', 'xi', 'zhen', 'xin', 'hen', 'zan']

s2 = SnowNLP('「繁體字」「繁體中文」的叫法在臺灣亦很常見。')
print(s2.han) # 「繁体字」「繁体中文」的叫法在台湾亦很常见。

text = '''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，
而在于研制能有效地实现自然语言通信的计算机系统，
特别是其中的软件系统。因而它是计算机科学的一部分。
'''
s3 = SnowNLP(text)
print(s3.keywords(3)) # ['语言', '自然', '计算机']
print(s3.summary(3)) # ['因而它是计算机科学的一部分', '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学']
print(s3.sentences) # ['自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学', '因此', '这一领域的研究将涉及自然语言', '即人们日常使用的语言', '所以它与语言学的研究有着密切的联系', '但又有重要的区别', '自然语言处理并不是一般地研究自然语言', '而在于研制能有效地实现自然语言通信的计算机系统', '特别是其中的软件系统', '因而它是计算机科学的一部分']

s4 = SnowNLP([['这篇', '文章'], ['那篇', '论文'], ['这个']])
print(s4.tf) # [{'这篇': 1, '文章': 1}, {'那篇': 1, '论文': 1}, {'这个': 1}]
print(s4.idf) # {'这篇': 0.5108256237659907, '文章': 0.5108256237659907, '那篇': 0.5108256237659907, '论文': 0.5108256237659907, '这个': 0.5108256237659907}
print(s4.sim([u'文章'])) # [0.4686473612532025, 0, 0]
```


### DeepNLP——神经网络版

[DeepNLP](https://github.com/rockingdingo/deepnlp)项目是基于Tensorflow平台的一个python版本的NLP套装, 目的在于将Tensorflow深度学习平台上的模块，结合 最新的一些算法，提供NLP基础模块的支持，并支持其他更加复杂的任务的拓展，如生成式文摘等等。

NLP 套装模块
- 分词 Word Segmentation/Tokenization：线性链条件随机场 Linear Chain CRF, 基于CRF++包来实现
- 词性标注 Part-of-speech (POS)：单向LSTM/ 双向BI-LSTM, 基于Tensorflow实现
- 命名实体识别 Named-entity-recognition(NER)：单向LSTM/ 双向BI-LSTM/ LSTM-CRF 结合网络, 基于Tensorflow实现
- 依存句法分析 Dependency Parsing (Parse)：基于arc-standard system的神经网络的parser
- 自动生成式文摘 Textsum (Seq2Seq-Attention)
- 关键句子抽取 Textrank
- 文本分类 Textcnn (WIP)
- 预训练模型：
  - 中文: 基于人民日报语料和微博混合语料: 分词, 词性标注, 实体识别
- 可调用 Web Restful API
- 计划中: 句法分析 Parsing

```python
# pip install deepnlp
import deepnlp
# Download all the modules
deepnlp.download()
# Download specific module
deepnlp.download('segment')
deepnlp.download('pos')
deepnlp.download('ner')
deepnlp.download('parse')
# Download module and domain-specific model
deepnlp.download(module = 'pos', name = 'en') 
deepnlp.download(module = 'ner', name = 'zh_entertainment')
```

示例

```python
from deepnlp import segmenter, pos_tagger, ner_tagger, nn_parser
from deepnlp import pipeline

# 分词模块
tokenizer = segmenter.load_model(name='zh')
text = "我爱吃北京烤鸭"
seg_list = tokenizer.seg(text)
text_seg = " ".join(seg_list)
print(text_seg)

# 词性标注
p_tagger = pos_tagger.load_model(name='zh')
tagging = p_tagger.predict(seg_list)
for (w, t) in tagging:
    pair = w + "/" + t
print(pair)

# 命名实体识别
n_tagger = ner_tagger.load_model(name='zh')  # Base LSTM Based Model
tagset_entertainment = ['city', 'district', 'area']
tagging = n_tagger.predict(seg_list, tagset=tagset_entertainment)
for (w, t) in tagging:
    pair = w + "/" + t
    print(pair)

# 依存句法分析
parser = nn_parser.load_model(name='zh')
words = ['它', '熟悉', '一个', '民族', '的', '历史']
tags = ['r', 'v', 'm', 'n', 'u', 'n']
dep_tree = parser.predict(words, tags)
num_token = dep_tree.count()
print("id\tword\tpos\thead\tlabel")
for i in range(num_token):
    cur_id = int(dep_tree.tree[i + 1].id)
    cur_form = str(dep_tree.tree[i + 1].form)
    cur_pos = str(dep_tree.tree[i + 1].pos)
    cur_head = str(dep_tree.tree[i + 1].head)
    cur_label = str(dep_tree.tree[i + 1].deprel)
    print("%d\t%s\t%s\t%s\t%s" % (cur_id, cur_form, cur_pos, cur_head, cur_label))

# Pipeline
p = pipeline.load_model('zh')
text = "我爱吃北京烤鸭"
res = p.analyze(text)
print(res[0])
print(res[1])
print(res[2])
words = p.segment(text)
pos_tagging = p.tag_pos(words)
ner_tagging = p.tag_ner(words)
print(list(pos_tagging))
print(ner_tagging)
```

### 小明NLP——拼写检查、偏旁部首

小明NLP [xmnlp](https://github.com/SeanLee97/xmnlp)的主要功能：
- 中文分词 & 词性标注
- 支持繁體
- 支持自定义词典
- 中文拼写检查
- 文本摘要 & 关键词提取
- 情感分析
- 文本转拼音
- 获取汉字偏旁部首

```python
mport xmnlp
print(xmnlp.radical('自然语言处理'))
print(xmnlp.radical('自然語言處理'))
```

### spaCy——（工业界）

[spaCy](https://spacy.io/) 是一个Python自然语言处理工具包，诞生于2014年年中，号称“Industrial-Strength Natural Language Processing in Python”，是具有工业级强度的Python NLP工具包。spaCy里大量使用了 Cython 来提高相关模块的性能，这个区别于学术性质更浓的NLTK，因此具有了业界应用的实际价值。

主要特性：
- 分词
- 命名实体识别
- 多语言支持（号称支持53种语言）
- 针对11种语言的23种统计模型
- 预训练词向量
- 高性能
- 轻松的整合深度学习
- 词性标注
- 依存句法分析
- 句法驱动的句子切分
- 用于语法和命名实体识别的内置可视化工具
- 方便的字符串到哈希映射
- 导出到numpy数据数组
- 高效的二进制序列化
- 易于模型打包和部署
- 稳健，精确评估

先执行包的安装：pip install spacy，再执行数据集和[模型](https://spacy.io/models)的下载。
- 英文：python -m spacy download en_core_web_sm
- 无官方中文，小道中文版
  - 模型：https://github.com/howl-anderson/Chinese_models_for_SpaCy
  - 执行：pip install ./zh_core_web_sm-2.0.5.tar.gz

```python
import spacy

nlp = spacy.load("en_core_web_sm") # 英文
nlp = spacy.load("zh_core_web_sm") # 中文

doc = nlp("王小明在北京的清华大学读书")
# 分词 词性标注
for token in doc:
    print(token, token.pos_, token.pos)
# 命名实体识别（NER）
for ent in doc.ents:
    print(ent, ent.label_, ent.label)
# 名词短语提取
for np in doc.noun_chunks:
    print(np)
# 依存关系
for token in doc:
    print(token.text, token.dep_, token.head)
# 文本相似度
doc1 = nlp(u"my fries were super gross")
doc2 = nlp(u"such disgusting fries")
similarity = doc1.similarity(doc2)
print(similarity)
# 句法树展示
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
          token.shape_, token.is_alpha, token.is_stop, token.has_vector,
          token.ent_iob_, token.ent_type_,
          token.vector_norm, token.is_oov)
spacy.displacy.serve(doc)
```

同时启动web服务

![](https://www.biaodianfu.com/wp-content/uploads/2020/10/displatcy-768x461.png)


### 复旦FoolNLTK——最准（非最快）的开源中文工具包

[FoolNLTK](https://github.com/rockyzhengwu/FoolNLTK) [文章](FoolNLTK：一个便捷的中文处理工具包)介绍, 复旦大学，一个使用双向 LSTM （BiLSTM 模型）构建的便捷的中文处理工具包，该工具不仅可以实现分词、词性标注和命名实体识别，同时还能使用用户自定义字典加强分词的效果。根据该项目所述，这个中文工具包可能不是最快的开源中文分词，但很可能是最准的开源中文分词。
- 基于神经网络的方法，往往使用「字向量 + 双向 LSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量将到最低。
- ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/BiLSTM.png)

使用示例：
```python
# pip install tensorflow==1.15 # tf版本限制
import fool
sentence = "中文分词测试：我爱北京天安门"
print(fool.cut(sentence))  # 分词, [['中文', '分词', '测试', '：', '我', '爱', '北京', '天安', '门']]
print(fool.pos_cut(sentence))  # 词性标注, [[('中文', 'nz'), ('分词', 'n'), ('测试', 'n'), ('：', 'wm'), ('我', 'r'), ('爱', 'v'), ('北京', 'ns'), ('天安', 'nz'), ('门', 'n')]]
print(fool.analysis(sentence)[1]) # 明名实体识别, [[(9, 15, 'company', '北京天安门')]]
```

### HanLP——支持TensorFlow 2


- 借助世界上最大的多语种语料库，HanLP支持包括简繁中英日俄法德在内的104种语言上的10种联合任务：分词（粗分、细分2个标准，强制、合并、校正3种词典模式）、词性标注（PKU、863、CTB、UD四套词性规范）、命名实体识别（PKU、MSRA、OntoNotes三套规范）、依存句法分析（SD、UD规范）、成分句法分析、语义依存分析（SemEval16、DM、PAS、PSD四套规范）、语义角色标注、词干提取、词法语法特征提取、抽象意义表示（AMR）。
- HanLP已经被广泛用于Lucene、Solr、ElasticSearch、Hadoop、Android、Resin等平台，有大量开源作者开发各种插件与拓展，并且被包装或移植到Python、C#、R、JavaScript等语言上去。 基于深度学习的HanLP2.x已于2020年初发布，面向下一个十年的前沿NLP技术，与1.x相辅相成，平行发展。

[HanLP](https://www.hanlp.com/)原先是一个JAVA版本的自然语言处理工具包，在目前的升级中已经支持了Python。当前已经支持基于 TensorFlow 2.x。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。特殊功能：
- 短语提取
- 拼音转换
- 繁简转换
- 文本推荐

- [Demo体验](https://hanlp.hankcs.com/?sentence=%E8%B4%9D%E5%A3%B3%E5%92%8C%E9%93%BE%E5%AE%B6%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E9%83%BD%E6%98%AF%E4%B8%AD%E4%BB%8B%E5%85%AC%E5%8F%B8%E5%90%97&mul=off)

```python
# https://github.com/hankcs/pyhanlp
from pyhanlp import *

sen = '贝壳和链家什么关系,左晖和stanly又是谁'
CustomDictionary.add("链家", "n") # 自定义字典
res = HanLP.segment(sen) # pos
for term in res:
    print(term, term.word, str(term.nature))
res = HanLP.extractKeyword(sen, 10)
print(res)
summary  = HanLP.extractSummary(sen, 5)
print(summary)
print('分词：', HanLP.segment(sen))
#TextRankKeyword = JClass("com.hankcs.hanlp.summary.TextRankKeyword")
print('关键词：', HanLP.extractKeyword(sen, 2))
print('自动摘要：', HanLP.extractSummary(sen, 3))
sentence_list = HanLP.getSummary(sen, 50)  #提取短语,同时指定摘要的最大长度 
print('自动摘要：', sentence_list)
NER=HanLP.newSegment().enableNameRecognize(True)
print('中文名识别：', NER.seg(sen))
person_ner = HanLP.newSegment().enableTranslatedNameRecognize(True)
print('英文名识别：', person_ner.seg(sen))
print('句法分析：', HanLP.parseDependency(sen))
PerceptronLexicalAnalyzer=JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer=PerceptronLexicalAnalyzer()
print('感知机句法分析：', analyzer.analyze(sen))
print('短语提取：', HanLP.extractPhrase(sen, 5))
print('简体：', HanLP.convertToSimplifiedChinese("我愛自然語言處理技術！"))
print('繁体：', HanLP.convertToTraditionalChinese("我爱自然语言处理技术！"))
pinyinList = HanLP.convertToPinyinList(sen)
for pinyin in pinyinList:
    print(pinyin.getPinyinWithoutTone(),pinyin.getTone(), pinyin, pinyin.getPinyinWithToneMark())
# 可视化服务：shell命令
hanlp serve  # http://localhost:8765
# 命令行方式传入
hanlp segment <<< '欢迎新老师生前来就餐' 
```

## CRF工具包

CRF++工具包，[CRF++: Yet Another CRF toolkit](https://taku910.github.io/crfpp/)

### （1）编译安装

步骤：[python调用CRF++工具包](https://www.jianshu.com/p/9a8425110f43)

```shell
# 拉取github上的源文件
git clone https://github.com/taku910/crfpp.git
cd crfpp

# 去除找不到winmain.h的错误 [2021-7-15] sed命令有误，手工删除winmain引用即可
sed -i '/#include "winmain.h"/d' crf_test.cpp
sed -i '/#include "winmain.h"/d' crf_learn.cpp

# 编译安装
./configure
make && make install

# 配置文件并导入，消除【错误2】 【2021-7-15】mac下没有ld.so.conf文件
echo "include /usr/local/lib" >> /etc/ld.so.conf
/sbin/ldconfig -v
# 导入python
cd python
python3 setup.py install

```

###  （2）直接安装

命令：
- pip install crfpy

### 使用

- 训练模型
  - crf_learn template train.data model 
- 测试使用

```python
import CRFPP
import codecs

# 加载模型
tagger=CRFPP.Tagger(r'-m C:\Users\secoo\Desktop\CRF++\model')

# 加载测试文件
input_data = codecs.open(r'C:\Users\secoo\Desktop\CRF++\shangde.txt', 'r', 'utf-8')
word_str=input_data.readlines()
word_str=''.join(word_str)
# 测试
print(tagger.parse(word_str))

```


## python工具包

- 【2021-1-19】[python高效使用统计语言模型kenlm：新词发现、分词、智能纠错等](https://linux.ctolib.com/mattzheng-py-kenlm-model.html)
- kenlm的优点（关于kenlm工具训练统计语言模型）： 训练语言模型用的是传统的“统计+平滑”的方法，使用kenlm这个工具来训练。它快速，节省内存，最重要的是，允许在开源许可下使用多核处理器。 kenlm是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。
- 额外需要加载的库：
   - kenlm
   - pypinyin
   - pycorrector
- 粗略整理代码: 
   - kenlm：[mattzheng/py-kenlm-model](https://github.com/mattzheng/py-kenlm-model)
      - 安装：pip install https://github.com/kpu/kenlm/archive/master.zip
   - 新词发现：[mattzheng/word-discovery](https://github.com/mattzheng/word-discovery)
- kenlm模型训练，详见[地址](https://www.cnblogs.com/zidiancao/p/6067147.html)
  - 文件必须是分词以后的文件。
  - -o后面的5表示的是5-gram,一般取到3即可，但可以结合自己实际情况判断

```shell
# 训练命令
bin/lmplz -o 5 --verbose_header --text data/chat_log.txt --arpa result/log.arpa --vocab_file result/log.vocab
# 结果后得到arpa文件
```

### kenlm模式

- 【2021-4-20】[Kenlm文本流畅度检测](https://zhuanlan.zhihu.com/p/265677864)

```python
import kenlm
model = kenlm.Model('lm/test.arpa')
print(model.score('this is a sentence .', bos = True, eos = True))
# 豆瓣语料模型
model = kenlm.Model('build/my_model/douban.arpa')
test_list = ["我是一名程序员",    "是一名程序员我",    "我 是 一名 程序员",    "是 一名 程序员 我",    "一名 是 程序员 我",    "一名 程序员 是 我",    "我 我 我 啊 嗯 你 一名 是 吗 噢噢 程序员 是 吗 我",    "我 等一下 去 交 进去 的 觉得 我 不卡 我 要 能 给我 媳妇 上 了 取得 这样的 他 方便 一点 他 就 觉得 9个人 要 办个 手续 这样 的"]
for i in test_list:
   time_start = time.time()
   print(i, ': ', model.score(i, bos=True, eos=True), 'time: ', time.time() - time_start)
# 结果见下图

#Stateful query 状态转移概率
state = kenlm.State()
state2 = kenlm.State()
#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).
model.BeginSentenceWrite(state)
```

示例：
![](https://pic1.zhimg.com/80/v2-f9277d33e552b3ab71ffc950ad3aa2ac_720w.png)

### pypinyin拼音模块

- 拼音模块涉及到了pypinyin，用来识别汉字的拼音，还有非常多种的模式：

```python
from pypinyin import lazy_pinyin, Style
	# Python 中拼音库 PyPinyin 的用法
	# https://blog.csdn.net/devcloud/article/details/95066038

tts = ['BOPOMOFO', 'BOPOMOFO_FIRST', 'CYRILLIC', 'CYRILLIC_FIRST', 'FINALS', 'FINALS_TONE',
 'FINALS_TONE2', 'FINALS_TONE3', 'FIRST_LETTER', 'INITIALS', 'NORMAL', 'TONE', 'TONE2', 'TONE3']
for tt in tts:
    print(tt,lazy_pinyin('聪明的小兔子吃', style=eval('Style.{}'.format(tt))   ))
```

其中结果为：
```shell
BOPOMOFO ['ㄘㄨㄥ', 'ㄇㄧㄥˊ', 'ㄉㄜ˙', 'ㄒㄧㄠˇ', 'ㄊㄨˋ', 'ㄗ˙', 'ㄔ']
BOPOMOFO_FIRST ['ㄘ', 'ㄇ', 'ㄉ', 'ㄒ', 'ㄊ', 'ㄗ', 'ㄔ']
CYRILLIC ['цун1', 'мин2', 'дэ', 'сяо3', 'ту4', 'цзы', 'чи1']
CYRILLIC_FIRST ['ц', 'м', 'д', 'с', 'т', 'ц', 'ч']
FINALS ['ong', 'ing', 'e', 'iao', 'u', 'i', 'i']
FINALS_TONE ['ōng', 'íng', 'e', 'iǎo', 'ù', 'i', 'ī']
FINALS_TONE2 ['o1ng', 'i2ng', 'e', 'ia3o', 'u4', 'i', 'i1']
FINALS_TONE3 ['ong1', 'ing2', 'e', 'iao3', 'u4', 'i', 'i1']
FIRST_LETTER ['c', 'm', 'd', 'x', 't', 'z', 'c']
INITIALS ['c', 'm', 'd', 'x', 't', 'z', 'ch']
NORMAL ['cong', 'ming', 'de', 'xiao', 'tu', 'zi', 'chi']
TONE ['cōng', 'míng', 'de', 'xiǎo', 'tù', 'zi', 'chī']
TONE2 ['co1ng', 'mi2ng', 'de', 'xia3o', 'tu4', 'zi', 'chi1']
TONE3 ['cong1', 'ming2', 'de', 'xiao3', 'tu4', 'zi', 'chi1']
```

可以看出不同的style可以得到不同拼音形式。

### pycorrector纠错模块

- pycorrector的detect，可以返回，错误字的信息

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)
# [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]

#correct是专门用来纠正：
pycorrector.correct(sentence)
```

-  pycorrector与kenlm纠错对比
- 来对比一下pycorrector自带的纠错和本次实验的纠错：

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)

correct = []
for ide in idx_errors:
    right_word = km.find_best_word(ide[0],ngrams_,freqs = 0)
    if right_word != ide[0]:
        correct.append([right_word] + ide)

print('错误：',idx_errors)
print('pycorrector的结果：',pycorrector.correct(sentence))
print('kenlm的结果：',correct)
```

> 错误： [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]
> pycorrector的结果： ('这瓶洗面奶用着狠不错', [['棉奶', '面奶', 3, 5]])
> kenlm的结果： [['面奶', '棉奶', 3, 5, 'word']]

其他类似的案例：

sentence =  '少先队员因该给老人让坐'

> 错误： [['因该', 4, 6, 'word'], ['坐', 10, 11, 'char']]
> pycorrector的结果： ('少先队员应该给老人让座', [['因该', '应该', 4, 6], ['坐', '座', 10, 11]])
> kenlm的结果： [['应该', '因该', 4, 6, 'word']]
这里笔者的简陋规则暴露问题了，只能对2个字以上的进行判定。

sentence = '绿茶净华可以舒缓痘痘机肤'

> 错误： [['净华', 2, 4, 'word'], ['机肤', 10, 12, 'word']]
> pycorrector的结果： ('绿茶净化可以舒缓痘痘肌肤', [['净华', '净化', 2, 4], ['机肤', '肌肤', 10, 12]])
> kenlm的结果： [['精华', '净华', 2, 4, 'word'], ['肌肤', '机肤', 10, 12, 'word']]


## [nlp-tutorial](https://github.com/graykode/nlp-tutorial)

<p align="center"><img width="100" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png" />  <img width="100" src="https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png" /></p>

`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **TensorFlow** and **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)



## Curriculum - (Example Purpose)

#### 1. Basic Embedding Model

- 1-1. [NNLM(Neural Network Language Model)](https://github.com/graykode/nlp-tutorial/tree/master/1-1.NNLM) - **Predict Next Word**
  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  - Colab - [NNLM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Tensor.ipynb), [NNLM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM_Torch.ipynb)
- 1-2. [Word2Vec(Skip-gram)](https://github.com/graykode/nlp-tutorial/tree/master/1-2.Word2Vec) - **Embedding Words and Show Graph**
  - Paper - [Distributed Representations of Words and Phrases
    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  - Colab - [Word2Vec_Tensor(NCE_loss).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(NCE_loss).ipynb), [Word2Vec_Tensor(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Tensor(Softmax).ipynb), [Word2Vec_Torch(Softmax).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram_Torch(Softmax).ipynb)
- 1-3. [FastText(Application Level)](https://github.com/graykode/nlp-tutorial/tree/master/1-3.FastText) - **Sentence Classification**
  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)
  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)

#### 2. CNN(Convolutional Neural Network)

- 2-1. [TextCNN](https://github.com/graykode/nlp-tutorial/tree/master/2-1.TextCNN) - **Binary Sentiment Classification**
  - Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)
  - Colab - [TextCNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Tensor.ipynb), [TextCNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN_Torch.ipynb)
- 2-2. DCNN(Dynamic Convolutional Neural Network)

#### 3. RNN(Recurrent Neural Network)

- 3-1. [TextRNN](https://github.com/graykode/nlp-tutorial/tree/master/3-1.TextRNN) - **Predict Next Step**
  - Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)
  - Colab - [TextRNN_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Tensor.ipynb), [TextRNN_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN_Torch.ipynb)
- 3-2. [TextLSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM) - **Autocomplete**
  - Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
  - Colab - [TextLSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Tensor.ipynb), [TextLSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM_Torch.ipynb)
- 3-3. [Bi-LSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-3.Bi-LSTM) - **Predict Next Word in Long Sentence**
  - Colab - [Bi_LSTM_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Tensor.ipynb), [Bi_LSTM_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM_Torch.ipynb)

#### 4. Attention Mechanism

- 4-1. [Seq2Seq](https://github.com/graykode/nlp-tutorial/tree/master/4-1.Seq2Seq) - **Change Word**
  - Paper - [Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)
  - Colab - [Seq2Seq_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Tensor.ipynb), [Seq2Seq_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq_Torch.ipynb)
- 4-2. [Seq2Seq with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-2.Seq2Seq(Attention)) - **Translate**
  - Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)
  - Colab - [Seq2Seq(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Tensor.ipynb), [Seq2Seq(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention)_Torch.ipynb)
- 4-3. [Bi-LSTM with Attention](https://github.com/graykode/nlp-tutorial/tree/master/4-3.Bi-LSTM(Attention)) - **Binary Sentiment Classification**
  - Colab - [Bi_LSTM(Attention)_Tensor.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Tensor.ipynb), [Bi_LSTM(Attention)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention)_Torch.ipynb)


#### 5. Model based on Transformer

- 5-1.  [The Transformer](https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer) - **Translate**
  - Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
  - Colab - [Transformer_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb), [Transformer(Greedy_decoder)_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder)_Torch.ipynb)
- 5-2. [BERT](https://github.com/graykode/nlp-tutorial/tree/master/5-2.BERT) - **Classification Next Sentence & Predict Masked Tokens**
  - Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
  - Colab - [BERT_Torch.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT_Torch.ipynb)

|           Model            |              Example               |   Framework   | Lines(torch/tensor) |
| :------------------------: | :--------------------------------: | :-----------: | :-----------------: |
|            NNLM            |         Predict Next Word          | Torch, Tensor |        67/83        |
|     Word2Vec(Softmax)      |   Embedding Words and Show Graph   | Torch, Tensor |        77/94        |
|          TextCNN           |      Sentence Classification       | Torch, Tensor |        94/99        |
|          TextRNN           |         Predict Next Step          | Torch, Tensor |        70/88        |
|          TextLSTM          |            Autocomplete            | Torch, Tensor |        73/78        |
|          Bi-LSTM           | Predict Next Word in Long Sentence | Torch, Tensor |        73/78        |
|          Seq2Seq           |            Change Word             | Torch, Tensor |       93/111        |
|   Seq2Seq with Attention   |             Translate              | Torch, Tensor |       108/118       |
|   Bi-LSTM with Attention   |  Binary Sentiment Classification   | Torch, Tensor |       92/104        |
|        Transformer         |             Translate              |     Torch     |        222/0        |
| Greedy Decoder Transformer |             Translate              |     Torch     |        246/0        |
|            BERT            |            how to train            |     Torch     |        242/0        |


## Dependencies

- Python 3.5+
- Tensorflow 1.12.0+
- Pytorch 0.4.1+
- Plan to add Keras Version



# 应用

## 项目实战

- 【2020-6-2】[京东AI项目实战课](http://finance.sina.com.cn/wm/2020-06-02/doc-iircuyvi6299111.shtml)：五个阶段从理论到实践、从项目实战到面试准备的一站式教学，涵盖NLP领域核心技能（特征工程、分类模型、语法树等），前沿技术（BERT、XLNet、Seq2Seq、Transformer、ALBERT、模型蒸馏、模型压缩等）；
- ![](http://n.sinaimg.cn/sinakd202062s/263/w1000h4063/20200602/f2ae-iumkapw2054484.png)
- ![](http://n.sinaimg.cn/sinakd202062s/612/w1000h5212/20200602/0036-iumkapw2054582.jpg)

## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 结束


