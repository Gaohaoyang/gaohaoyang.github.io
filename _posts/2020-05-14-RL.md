---
layout: post
title:  "强化学习-Reinforcement Learning"
date:   2020-05-14 17:34:00
categories: 强化学习
tags: 深度学习 强化学习 增强学习 动态规划 贝尔曼方程
excerpt: AI=DL+RL，那么RL强化学习是什么，包含哪些内容，有哪些典型应用？
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- 【2021-3-18】[大年初一，一起来深度强化学习](https://zhuanlan.zhihu.com/p/350506752)，Thomas Simonini 的 Deep Reinforcement Learning Course的课程笔记
- 【2020-7-4】【(EEML2020)强化学习教程(Colab)】“[EEML2020 RL Tutorial](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb)” 
- 【2020-7-5】XRL：可解释强化学习《[XRL: eXplainable Reinforcement Learning](https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a)》by Meet Gandhi
- 【2019】强化学习课程：[2019斯坦福大学最新强化学习课程：CS234](https://www.bilibili.com/video/BV1Nb411s7pP)，系列，B站视频，CSDN笔记[总结](https://blog.csdn.net/solo95/category_9298323.html)
  - <iframe src="//player.bilibili.com/player.html?aid=47812079&bvid=BV1Nb411s7pP&cid=83748673&page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="600"> </iframe>
- 【2021-2-27】【[杜伦大学10小时强化学习课程](https://www.bilibili.com/video/BV1vN411X7qB/)】“Reinforcement Learning Lectures  Durham University” by Chris G. Willcocks
  - <iframe src="//player.bilibili.com/player.html?aid=501767046&bvid=BV1vN411X7qB&cid=302763703&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>
- 【2021-3-17】[强化学习的“神话”和“鬼话”](https://zhuanlan.zhihu.com/p/196421049)，Csaba Szepesvári在2020年数据挖掘顶会KDD的Deep Learning Day做了题为Myths and Misconceptions in Reinforcement Learning的讲座。Csaba Szepesvári是阿尔伯塔大学计算机系教授，在Deepmind领导Foundations团队。他于2010年出版Algorithms for Reinforcement Learning. 最近出版Bandit Algorithms. 他于2006年发表题为Bandit based Monte-Carlo Planning的论文，提出UCT算法，对AlphaGo的研发起到了关键作用。
  - 1）要不要学习RL
    - 误解：有人说，RL就是指一些特定的算法，比如TD、DQN、PPO，等等；而像ES(evolutionary search, 进化搜索)、随机搜索、SSL(self-supervised learning, 自我监督学习)等等这些就不是RL.
    - 三类基本的RL问题：Online RL（智能体直接与环境交互学习）, Batch RL（历史数据中学习）, Planning/simulation optimization（规划/仿真优化）. 当然有很多变种。
  - 2）RL是不是有很多问题
  - 3）RL与相邻学科的关系如何
  - 然后讨论一些“Meta consideration”
  - [原版ppt及youtube视频](https://sites.ualberta.ca/~szepesva/talks.html)，[model based RL ppt](https://www.dropbox.com/s/8b4eivix82tfp2z/ModelBasedRL2.pptx?dl=0)
- 【2021-3-19】人工智能拒绝内卷，AI研习社文章：[开局一头狼六只羊，这个狼吃羊的AI火了！傻狼拒绝内卷：抓羊可太累了，我只想自杀......](https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247585920&idx=1&sn=ad78a94ded54c54f092d4694e11ece25&chksm=90959913a7e210053febe37a27bff456a136aec2c8e3dd4fc15d71cdcea648a5663105f553ca&mpshare=1&scene=1&srcid=0319JgBKEY71NSi5iVRaOgZS&sharer_sharetime=1616158813661&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
  - ![](https://wx3.sinaimg.cn/mw690/007dt7foly1gog7nqut5hj30v81glqb8.jpg)
  - ![](https://mmbiz.qpic.cn/mmbiz_png/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZfbvfibRcRUqz4PDXWSFTBWn2JApWS5uPsnFqxtRPDEaKsZic0QBGls3ww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
  - ![](https://pic2.zhimg.com/80/v2-abc6b3d9a109ae9c1a182f322f069363_1440w.jpg?source=1940ef5c)
    - 知乎曾伊言的回答[这样的AI是真的吗？](https://www.zhihu.com/question/448931860/answer/1779097110) ：这远不是通用人工智能，回报函数(reward function)的设计者若理解期望折扣回报(expected discounted return)，就不会犯这种错，也不需要研究三天。
  - ①狼就是打工人…每秒扣的是青春和时间，羊是永远达不到的“升职、加薪、迎娶白富美、走上人生巅峰”，撞石头就是躺平摸鱼
  - ②阿西莫夫三定律是多么重要啊，不然以效率和结果为一切指标的ai，非常可能在很多场景中计算后发现，人类+自己毁灭后效益最大，然后采取灭世措施。
  - [微博原贴](https://weibo.com/6611961566/K5J8t8XfM?type=comment#_rnd1616126336679)
  - 星尘研表示狼自杀的错误是很多东西共同影响产生的，最主要的一个错误是迭代次数太少，20W次完全不够学，后面提高到100W次起步, 效果直线上升。
  - ![](https://mmbiz.qpic.cn/mmbiz_gif/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZf8IMo27fyqEvd9WG6ZTYc8l4OvicctJD2GWkKDc89VBxHmqBib5ZQKaOw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)
  - [今天微博上好像有一个内卷AI狼...](https://www.bilibili.com/video/BV16X4y1V7Yu?from=search&seid=10508566856251031934)
  - <iframe src="//player.bilibili.com/player.html?aid=714560902&bvid=BV16X4y1V7Yu&cid=309180940&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
- Richard S. Sutton and Andrew G. Barto的[Reinforcement Learning: An Introduction, Second edition](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)


# 什么是强化学习

## 定义

- 强化学习的思想为：**agent**通过与**环境**的交互（其实就是试错）得到**奖励**（可正可负，负的奖励就是惩罚）作为反馈，从而做出对应的**action**。这个思想很符合自然经验，小时候学走路，摔了会痛（奖励为负），走得稳了有糖吃（奖励为正），为了多吃点糖（取得更多的奖励），你最终学会了走路。
  - ![](https://pic1.zhimg.com/80/v2-74f512eaf6fc5f68d424ec37dc7bb144_720w.jpg)
- 接下来给出强化学习的正式定义：

> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.

## 基本组件

- 强化学习强化学习系统由**智能体**（Agent）、**状态**（state）、**奖赏**（reward）、**动作**（action）和**环境**（Environment）五部分组成，如下图所示。
  - ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavlKDwRa4UIy5IjcjDAm1CPvPMxyK50fy1B5VEd2pt4NeCRaeLlkXMibyiam9cRsmPo3VLpZfpkuNLw/640?wx_fmt=other)
  - `Agent`：智能体是整个强化学习系统核心。它能够感知环境的状态（State），并且根据环境提供的奖励信号（Reward），通过学习选择一个合适的动作（Action），来最大化长期的Reward值。简而言之，Agent就是根据环境提供的Reward作为反馈，学习一系列的环境状态（State）到**动作**（Action）的映射，动作选择的原则是最大化未来累积的Reward的概率。选择的动作不仅影响当前时刻的Reward，还会影响下一时刻甚至未来的Reward，因此，Agent在学习过程中的基本规则是：如果某个动作（Action）带来了环境的正回报（Reward），那么这一动作会被加强，反之则会逐渐削弱，类似于物理学中条件反射原理。
  - `Environment`：环境会接收Agent执行的一系列的动作（Action），并且对这一系列的动作的好坏进行评价，并转换成一种可量化的（标量信号）Reward反馈给Agent，而不会告诉Agent应该如何去学习动作。Agent只能靠自己的历史（History）经历去学习。同时，环境还像Agent提供它所处的状态（State）信息。
  - `Reward`：环境提供给Agent的一个可量化的标量反馈信号，用于评价Agent在某一个时间步所做action的好坏。强化学习就是基于一种最大化累计奖赏假设：强化学习中，Agent进行一系列的动作选择的目标是最大化未来的累计奖赏。
  - `State`：状态指Agent所处的环境信息，包含了智能体用于进行Action选择的所有信息，它是历史（History）的一个函数：St = f（Ht）。
- 强化学习的主体是Agent和环境Environment。Agent为了适应环境，做出的一系列的动作，使最终的奖励最高，同时在此过程中更新特定的参数。实际上可以把强化学习简单理解成是一种循环，具体的工作方式如下：
  - 智能体从环境中获取一个状态St；
  - 智能体根据状态St采取一个动作at；
  - 受到at的影响，环境发生变化，转换到新的状态St+1；
  - 环境反馈给智能体一个奖励（正向为奖励，负向则为惩罚）。
- 参考：[强化学习在智能对话上的应用](https://blog.csdn.net/Tencent_TEG/article/details/88859179)

- **动作空间**
  - 动作空间为在一个环境中所有可能动作的集合，可以是离散空间，比如有的游戏只能上下左右移动，也可以是连续空间，比如自动驾驶时的转角可能是无穷的。
- **奖励**与**折扣**
  - 奖励就是反馈，它能让agent知道动作是好是坏，每个时刻的累积奖励可以写为
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中τ代表状态和动作的序列。但是实际上我们并不直接这样相加，因为预测太远的事情总是不准的，我们一般选择更看重眼前的奖励，所以需要对未来的奖励进行衰减
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中γ是衰减因子来权衡我们对长远奖励的重视程度。![](https://www.zhihu.com/equation?tex=R_%7Bt%7D%3Dr_%7Bt%2B1%7D%2B%5Cgamma+r_%7Bt%2B2%7D%2B%5Cgamma%5E%7B2%7D+r_%7Bt%2B3%7D%2B%5Ccdots%3D%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D+%5Cgamma%5E%7Bk%7D+r_%7Bt%2Bk%2B1%7D+%5Ctag+%7B2%7D+)
- 强化学习假设
  - ① **奖励假说**：所有的目标可以被描述为**最大化期望回报**，为了做出最好的决策，需要去最大化期望回报。
  - ② **马尔可夫决策过程**：agent每次只需根据**当前**的状态就能做出决策，而不依赖于**过去**的状态和动作信息。
- 强化学习的任务有两类：
  - **周期**任务（比如你在玩超级玛丽，每次从一条新的生命开始，直到游戏人物死亡或者到达终点结束）
  - **连续**任务（比如做自动化股票交易，这里就没有起始和终点这一说了，agent在做出做好决策的同时不断和环境进行交互）
- Exploration/ Exploitation的权衡
  - **Exploration**就是去探索更多的环境信息，**Exploitation**就是利用已知的环境信息最大化奖励。在强化学习中，我们既要Exploration又要Exploitation，这样才能做出更好的决策。这里举了个饭店选择的例子：Exploration就是去尝试你没吃过的饭店，这样可能会踩雷，但是也可能挖掘到更好吃的饭店！Exploitation就是每天去你知道的好吃的饭店吃饭，但这样可能错过一些更好吃的饭店。

强化学习里面的“**Model**”指的是 S->S'的**概率分布**，即在当前状态下实施某个动作会抵达什么新状态。
Model-free的设定中，不知道S-S'的概率分布，那么怎么才能得到v或者q呢？很简单，做**取样**（sampling

# 算法

## 求解方法

- 既然需要最大化期望回报来做出更好的决策，那么这一过程如何进行的呢？
- 假设agent接收到一个状态然后给出相应的动作，那么这一过程就可以理解成是一个控制律（一个从状态到动作的函数）
  - ![](https://pic3.zhimg.com/80/v2-009413beffb5736398eacebd6811c8fe_720w.jpg)
  - 控制律π好比agent的大脑，也是我们希望去学习出来的，我们的目标就是找到最好的控制律使得期望回报最大化。
- （1）**直接法**
  - 给定状态并告诉agent应该采取什么动作，也就是直接学习这个控制律，称为**policy-based**方法。
  - 控制率是确定的（即状态和动作一一对应） ![](https://www.zhihu.com/equation?tex=a%3D%5Cpi%28s%29)，如动作的概率分布![](https://www.zhihu.com/equation?tex=%5Cpi%28a%7Cs%29%3DP%28A%7Cs%29)
  - ![](https://pic1.zhimg.com/80/v2-8350cacc652c29f252d0b27371b0cee8_720w.jpg)
- （2）**间接法**
  - 不告诉agent什么动作是最好的，而是告诉它什么状态是更有价值的，而能到达更好价值的状态需要采取什么动作也就知道了，称为**value-based**方法。
  - 需要训练一个函数能将当前状态映射成（某一控制律下）期望回报
    - ![](https://www.zhihu.com/equation?tex=%5C%5B%7Bv_%5Cpi+%7D%28s%29+%3D+%5Cmathbb%7BE%7D%7B_%5Cpi+%7D%5Cleft%5B+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7BR_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7BR_%7Bt+%2B+3%7D%7D+%2B+...%7C%7BS_t%7D+%3D+s%7D+%5Cright%5D%5C%5D)
  - ![](https://pic3.zhimg.com/80/v2-6e1a3720ceeedfdb1d518ddc0f0cc232_720w.jpg)
- 总结
  - 无论是采用policy还是value-based方法，都会有一个policy，在policy-based方法中这个policy是通过训练直接得到的，而在value-based方法中我们不需要去训练policy，这里的policy只是个简单的函数，比如贪婪策略根据最大值来选择动作。两者之间的对应关系可以如下所示
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

## 基本概念

### 值函数（value-function）的定义

- **Q-learning**是一种value-based的强化学习方法，首先我们需要定义两个value函数：
  - 一个是state-value函数，即
    - ![](https://pic1.zhimg.com/80/v2-678f6e45b921862ac2ed112a7e33433c_720w.jpg)
    - 如从期望价值为-7的状态出发，根据贪婪策略，动作为右、右、右、右、下、下、右、右。
    - ![](https://pic1.zhimg.com/80/v2-15d39e8e4870b99693bc56b07dae4bd0_720w.jpg)
  - 另一个是action-value函数，在动作-价值函数中，函数的输入为动作和状态的pair，输出仍然是期望回报
    - ![](https://pic3.zhimg.com/80/v2-f299bc1a427cc355e67893f4d6ac5992_720w.jpg)
- action-value函数与state-value函数的区别
  - 函数值由状态和动作同时决定，也就是说一个状态四个动作对应的value可以是不一样的。
  - ![](https://pic2.zhimg.com/80/v2-6a2a0b713bda8a6f59d87ccc4327133d_720w.jpg)

### 动态规划

- 什么是**动态规划**（Dynamic Programming，DP）？
  - Dynamic means the sequential or temporal component to the problem，“动态”指的是该问题的时间序贯部分； 
  - Programming means optimising a "program", i.e. a policy，“规划”指的是去优化一个策略。
- 是不是所有问题都能用动态规划求解呢？
  - 不是的，动态规划方法需要问题包含以下两个性质：
    - **最优子结构**（Optimal substructure）：保证问题能够使用最优性原理（多阶段决策过程的最优决策序列具有这样的性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序列必须构成最优策略），从而问题的最优解可以分解为子问题最优解； 
    - **重叠子问题**（Overlapping subproblems）：子问题重复出现多次，因而可以缓存并重用子问题的解。
- 恰巧，MDP满足上面两个性质
  - **贝尔曼方程**是递归的形式，把问题分解成子问题
  - 值函数保存和重用问题的解
- “规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用规划来进行预测和控制。
- 参考：[搬砖的旺财](https://zhuanlan.zhihu.com/p/51393982)

### Bellman贝尔曼方程

- 函数值如何计算？这里需要引入Bellman方程了，因为期望回报是从当前状态一直计算到终止的，对于每个状态都这样计算是繁琐的，而Bellman方程将相邻状态的函数值关联起来了，即**当前状态的函数值，等于从当前状态转移一步的直接奖励加上下一个状态的折扣函数值**，这也就是Bellman方程的**核心思想**，recursive递归
  - ![](https://pic1.zhimg.com/80/v2-30a49ed93dcfd3f80076da0c04f77ca8_720w.jpg)
  - 当前时刻Q的目标值其实是未来reward 按照$\gamma$衰减的和。如果$\gamma=0$，则说明当前状态的Q值更新，只和跳转的下一状态有关；如果$\gamma=1$，则说明未来决策的所有reward对当前状态的Q值更新有影响，且影响程度一样。
- 贝尔曼方程(Bellman Equation)，百度百科关于贝尔曼方程的介绍

> **贝尔曼方程**（Bellman Equation）也被称作**动态规划方程**（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是**动态规划**（Dynamic Programming）这些数学最佳化方法能够达到最佳化的**必要**条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。

- 总结：贝尔曼方程就是用来简化强化学习或者马尔可夫决策问题
- value function可以分为两部分：
  - 立即回报![](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D)
  - 后继状态的折扣价值函数![](https://www.zhihu.com/equation?tex=%5Cgamma+v%28S_%7Bt%2B1%7D%29)

![](https://pic2.zhimg.com/v2-7e5c05147555f1eaf91a257781f4cfd5_b.jpg)
![](https://pic4.zhimg.com/v2-ead0588f9d7e27a5bb0911a7f0fdc1b3_b.jpg)

- [马尔科夫决策过程之Bellman Equation（贝尔曼方程）](https://zhuanlan.zhihu.com/p/35261164)


以Q值函数为例：
- Bellman**期望**方程：![](https://pic3.zhimg.com/v2-eea4a54b293b6cbb17995413216a4a66_b.jpg)
- Bellman**最优**方程：![](https://pic1.zhimg.com/v2-4f02c51f82ed8ff9ee80e1c9592bb2d0_b.jpg)
 
其中：
- （1）Model-based的解决方案中，基于动态规划，有基于Bellman**期望**方程的Policy iteration算法；也有基于Bellman**最优**方程的Value Iteration算法；
- （2）但是在Model-free中，似乎只有基于Bellman**最优**方程的Q-learning、Sarsa等算法。

### MDP马尔科夫决策过程

Markov是一个俄国的数学家，为了纪念他在马尔可夫链所做的研究，所以以他命名了“Markov Decision Process”，以下用MDP代替。

![](https://pic3.zhimg.com/80/v2-c6cceda3bb97c4c8c572f31910dbcb26_1440w.jpg)

MDP核心思想就是下一步的State只和当前的状态State以及当前状态将要采取的Action有关，只回溯一步。比如上图State3只和State2以及Action2有关，和State1以及Action1无关。

演变关系：
- Markov Property 马尔科夫性质：![[公式]](https://www.zhihu.com/equation?tex=P%28S_%7Bt%2B1%7D%7CS_%7Bt%7D%29+%3D+P%28S_%7Bt%2B1%7D%7CS_%7B1%7D%2C....%2CS_%7Bt%7D%29)
  - 根据公式也就是说给定当前状态 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%7D) ,将来的状态与t时刻之前的状态已经没有关系
  - State Transition Matrix 状态转移矩阵
- 马尔科夫链：动作序列，所有序列组成马尔科夫过程
- Markov Process 马尔科夫过程：一个无记忆的随机过程，是 马尔科夫过程 一些具有马尔科夫性质的随机状态序列构成，可以用一个元组<S,P>表示，其中S是有限数量的状态集，P是状态转移概率矩阵。
- MDP (Markov Decision Processes)马尔科夫决策过程
  - 一个强化学习任务如果满足马尔可夫性则被称为马尔可夫决策过程。MDP是一个序贯决策过程，可以由一个5元组来表示： ![[公式]](https://www.zhihu.com/equation?tex=%3CS%2CA%2C%5Cmathcal%7BP%7D_%7Bs+s%5E%7B%5Cprime%7D%7D%5E%7Ba%7D%2C%5Cmathcal%7BR%7D_%7Bs+s%5E%7B%5Cprime%7D%7D%5E%7Ba%7D%2C%5Cgamma%3E)
  - 如果状态空间和行动空间都是有限的，则称为**有限马尔可夫过程**（finite MDP）
- MRP 马尔科夫奖励过程

![](https://pic4.zhimg.com/80/v2-752441d4371f8fd2435e54cd40c18fc7_1440w.jpg)

### Monte Carlo 和 Temporal Difference Learning

- 由于强化学习的agent通过与环境交互得到提升，我们需要考虑如何利用交互的经验信息来更新值函数，从而得到更好的控制律。这里主要考虑两种方法：**蒙特卡洛**（MC）法采用的是完整的一轮经验，而**时序差分**（TD）法采用的只是一步经验，下面以value-based方法举例说明两种方法的区别
- （1）**蒙特卡洛**（MC）法利用一条完整的采样来更新函数，比如下图所示，所有的状态函数值为0，采用学习率0.1，不对回报进行衰减，采用贪婪加随机的控制策略，当老鼠走超过十步时停止，最终的路径如箭头所示
  - ![](https://pic4.zhimg.com/80/v2-8f9a0c03bd8803ac3091f20ebc031383_720w.jpg)
- 得到了一系列的状态、动作、奖励，那么我们需要计算出它的回报，从而根据下面公式更新state-value函数
  - ![](https://pic3.zhimg.com/80/v2-0ab47f5e0eda7179929f955f57eb9496_720w.jpg)
- 假设吃到一个奶酪的奖励为1，则回报为 Gt = 1+0+0+0+0+0+1+1+0+0=3，对值函数进行更新为 V(S0)= V(S0)+lr*[Gt- V(S0)]=0.3。
- （2）**时序差分**（TD）法则不需要等一整个过程结束才更新函数值，而是每一步都能更新，但是由于只走了一步我们并不知道后面的奖励，因此TD采取了自助的方式，利用上一次的估计值V(St+1)来替代。
  - ![](https://pic3.zhimg.com/80/v2-638b08a88f3a37e7c7cb4e95bf03f682_720w.jpg)
  - 当老鼠走了一步后即可更新值函数：V(S0)=V(S0)+lr*[ R1+gamma*V(S1)-V(S0) ]=0.1

## Q-learning


### 什么是Q-learning？

> **Q-Learning** is an off-policy value-based method that uses a TD approach to train its action-value function

- 本质上，Q-learning就是去训练action-value函数，也就是Q函数（Q的含义是Quality，表示在某一状态下那个动作的质量有多高），从而得到对应的action。
  - ![](https://pic3.zhimg.com/80/v2-448b0e27e650190f8116ae6df18320c2_720w.jpg)
- 对于离散问题来说，Q函数其实就是一张Q表格，每一个cell代表对应的状态-动作的函数值，有限个状态，动作空间有限，对应的Q-table就是
  - ![](https://pic3.zhimg.com/80/v2-8e21b6ba5277fa94ce1409236da96406_720w.jpg)
- 只要给定输入（状态+动作），Q表就能输出对应的Q值。当训练完成得到一个最优的Q函数（表）后，也就有了最佳的控制律，因为已经知道了每一个状态下哪个动作的函数值比较大，关于动作空间最大化Q函数即可得到最优的控制律。
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

- Q-learning是强化学习中的一种，在Q-learning中，需要维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在态s下可以采用动作a可以获得的未来收益的折现和——Q值。不断迭代Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。参照《[极简Q-learning教程](https://zhuanlan.zhihu.com/p/29213893)》
  - 更新公式如下，R+γmax部分称为**Q-target**，即使用贝尔曼方程加贪心策略认为实际应该得到的奖励，目标是使Q值不断的接近Q-target值
  - ![](https://upload-images.jianshu.io/upload_images/4155986-23e17f9c5b81efce.png?imageMogr2/auto-orient/strip\|imageView2/2/w/992/format/webp)

### 算法流程

- 整体
  - ![](https://pic2.zhimg.com/80/v2-4e606c07b3b240419c8b896f58900f51_720w.jpg)
- 详情
  - 第一步，初始化Q表，比如全部为0
    - ![](https://pic4.zhimg.com/80/v2-09afc546990e75e6d309bf6e2547a56f_720w.jpg)
  - 第二步，就是根据Epsilon Greedy策略选择动作（既要利用已知的又要探索未知的）
    - ![](https://pic3.zhimg.com/80/v2-d4494bb7dc8eb4c09b0ac0f359378c96_720w.jpg)
    - 在训练的一开始，应该多探索，随着训练越久，Q表越好，应该不断减小探索的概率。
    - ![](https://pic3.zhimg.com/80/v2-cf4c7d7a5c7be299daa573f0adb73a92_720w.jpg)
  - 第三步是执行动作，得到新的奖励，进入下一状态
    - ![](https://pic4.zhimg.com/80/v2-ce2990d388cc1a991a5632a78ee24943_720w.jpg)
  - 第四步就是更新Q表
    - ![](https://pic3.zhimg.com/80/v2-4b0911f8cb15e29a8a3547f619d959ca_720w.jpg)
- 总结
  - 计算TD Target的时候采用的是贪婪策略（对动作空间求Q值得最大），由于动作的实施和更新策略不同，所以Q-learning称为off-policy！因此对应的也有on-policy，比如说Sarsa算法
    - ![](https://pic4.zhimg.com/80/v2-519e8fd6d095a3efcd6729fd7ff5a8cf_720w.jpg)
  - Q-learning的核心就是Q表的更新，但是当问题规模一大，这种简单粗暴的方法显然是不太现实的，因此就有了 Deep Q-learning 的出现了。


## DQN

- 经典强化学习的Q-learning需要创建一个**Q表**来寻找最佳动作，而深度Q-learning则是采用一个神经网络来近似q值，避免了大型表的记录和存储。
  - ![](https://pic4.zhimg.com/80/v2-c545c9ef6c679b899b81e881a6afe297_720w.jpg)
- DQN是深度学习与强化学习的结合，即使用神经网络代替Q-learning中Q表。在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，但是当状态和动作空间是**高维**或者**连续**时，使用Q-Table不现实，而神经网络恰好擅长于此。因此DQN将Q-Table的更新问题变成一个函数拟合问题，相近的状态得到相近的输出动作。如有一个Q值表，神经网络的作用就是给定一个状态s和动作a，预测对应的Q值，使得神经网络的结果与Q表中的值接近。不过DQN的方式肯定不能继续维护一个Q表，所以将上次反馈的奖励作为逼近的目标，如下式，通过更新参数 θ 使Q函数逼近最优Q值。因此，DQN就是要设计一个神经网络结构，通过函数来拟合Q值
- 问题：
  - 神经网络需要**大量带标签的样本**进行监督学习，但是强化学习只有reward返回值，如何构造有监督的数据成为第一个问题，而且伴随着噪声、延迟（过了几十毫秒才返回）、稀疏（很多State的reward是0）等问题；
  - 神经网络的前提是样本**独立同分布**，而强化学习前后state状态和反馈有**依赖**关系——**马尔科夫决策**；
  - 神经网络的目标**分布固定**，但是强化学习的分布一直变化，比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练；
  - 过往的研究表明，使用非线性网络表示值函数时出现不稳定等问题。
- 针对以上问题的具体解决方案如下：
  - **构造标签**：通过Q-Learning使用reward来构造标签（对应问题1），如上所述，用神经网络来预测reward，将问题转化为一个**回归**问题；
  - **经验回放**：通过experience replay（经验池）的方法来解决**相关性**及**非静态分布**问题（对应问题2、3）；具体做法是把每个时间步agent与环境交互得到的转移样本 (st,at,rt,st+1) 储存到回放记忆单元，要训练时就随机拿出一些（minibatch）来训练。（其实就是将游戏的过程打成碎片存储，训练时随机抽取就避免了相关性问题）
  - 双网络结构：使用一个神经网络产生当前Q值，使用另外一个神经网络产生Target Q值（对应问题4）。在Nature 2015版本的DQN中提出了这个改进，使用另一个网络（这里称为target_net）产生Target Q值。具体地，Q(s,a;θi) 表示当前网络eval_net的输出，用来评估当前状态动作对的值函数；Q(s,a;θ−i) 表示target_net的输出，代入上面求 TargetQ 值的公式中得到目标Q值。根据上面的Loss Function更新eval_net的参数，每经过N轮迭代，将MainNet的参数复制给target_net。引入target_net后，再一段时间里目标Q值使保持不变的，一定程度降低了当前Q值和目标Q值的相关性，提高了算法稳定性。
- 参考：[实战深度强化学习DQN-理论和实践](https://www.jianshu.com/p/10930c371cac?from=singlemessage)

### 代码实现

- 两种DQN的实现方式
  - 一种是将s和a输入到网络，得到q值
  - 另一种是只将s输入到网络，输出为s和每个a结合的q值。
- 莫烦的Demo采用第二种，[Github地址](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)
  - ![](https://upload-images.jianshu.io/upload_images/4155986-fd3c59c61907c4ad.png?imageMogr2/auto-orient/strip\|imageView2/2/w/542/format/webp)
- DQN的Tensorflow版本实现，Gym环境
    - [用gym库Classic control实现deep Q learning](https://blog.csdn.net/winycg/article/details/79468320)

- （1）CartPole实例
    - 运载体在一根杆子下无摩擦的跟踪。系统通过施加+1和-1推动运载体。杆子的摇摆在初始时垂直的，目标是阻止它掉落运载体。每一步杆子保持垂直可以获得+1的奖励。episode将会终结于杆子的摇摆幅度超过了离垂直方向的15°或者是运载体偏移初始中心超过2.4个单位。
    - ![](https://img-blog.csdn.net/20180307114338879)
    - 效果图：
        - ![](https://img-blog.csdn.net/20180308173917866)

```python
#https://blog.csdn.net/winycg/article/details/79468320
import numpy as np
import random
import tensorflow as tf
import gym
 
max_episode = 100
env = gym.make('CartPole-v0')
env = env.unwrapped
 
class DeepQNetwork(object):
    def __init__(self,
                 n_actions,
                 n_features,
                 learning_rate=0.01,
                 reward_decay=0.9,  # gamma
                 epsilon_greedy=0.9,  # epsilon
                 epsilon_increment = 0.001,
                 replace_target_iter=300,  # 更新target网络的间隔步数
                 buffer_size=500,  # 样本缓冲区
                 batch_size=32,
                 ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = epsilon_greedy
        self.replace_target_iter = replace_target_iter
        self.buffer_size = buffer_size
        self.buffer_counter = 0  # 统计目前进入过buffer的数量
        self.batch_size = batch_size
        self.epsilon = 0 if epsilon_increment is not None else epsilon_greedy
        self.epsilon_max = epsilon_greedy
        self.epsilon_increment = epsilon_increment
        self.learn_step_counter = 0  # 学习计步器
        self.buffer = np.zeros((self.buffer_size, n_features * 2 + 2))  # 初始化Experience buffer[s,a,r,s_]
        self.build_net()
        # 将eval网络中参数全部更新到target网络
        target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')
        eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')
        with tf.variable_scope('soft_replacement'):
            self.target_replace_op = [tf.assign(t, e) for t, e in zip(target_params, eval_params)]
        self.sess = tf.Session()
        tf.summary.FileWriter('logs/', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())
 
    def build_net(self):
        self.s = tf.placeholder(tf.float32, [None, self.n_features])
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features])
        self.r = tf.placeholder(tf.float32, [None, ])
        self.a = tf.placeholder(tf.int32, [None, ])
 
        w_initializer = tf.random_normal_initializer(0., 0.3)
        b_initializer = tf.constant_initializer(0.1)
        # q_eval网络架构，输入状态属性，输出4种动作
        with tf.variable_scope('eval_net'):
            eval_layer = tf.layers.dense(self.s, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                         bias_initializer=b_initializer, name='eval_layer')
            self.q_eval = tf.layers.dense(eval_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer1')
        with tf.variable_scope('target_net'):
            target_layer = tf.layers.dense(self.s_, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                           bias_initializer=b_initializer, name='target_layer')
            self.q_next = tf.layers.dense(target_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer2')
        with tf.variable_scope('q_target'):
            # 计算期望价值，并使用stop_gradient函数将其不计算梯度，也就是当做常数对待
            self.q_target = tf.stop_gradient(self.r + self.gamma * tf.reduce_max(self.q_next, axis=1))
        with tf.variable_scope('q_eval'):
            # 将a的值对应起来，
            a_indices = tf.stack([tf.range(tf.shape(self.a)[0]), self.a], axis=1)
            self.q_eval_a = tf.gather_nd(params=self.q_eval, indices=a_indices)
        with tf.variable_scope('loss'):
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_a))
        with tf.variable_scope('train'):
            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)
 
            # 存储训练数据
 
    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, r, s_))
        index = self.buffer_counter % self.buffer_size
        self.buffer[index, :] = transition
        self.buffer_counter += 1
 
    def choose_action_by_epsilon_greedy(self, status):
        status = status[np.newaxis, :]
        if random.random() < self.epsilon:
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: status})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)
        return action
 
    def learn(self):
        # 每学习self.replace_target_iter步，更新target网络的参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.target_replace_op)
            # 从Experience buffer中选择样本
        sample_index = np.random.choice(min(self.buffer_counter, self.buffer_size), size=self.batch_size)
        batch_buffer = self.buffer[sample_index, :]
        _, cost = self.sess.run([self.train_op, self.loss], feed_dict={
            self.s: batch_buffer[:, :self.n_features],
            self.a: batch_buffer[:, self.n_features],
            self.r: batch_buffer[:, self.n_features + 1],
            self.s_: batch_buffer[:, -self.n_features:]
        })
        self.epsilon = min(self.epsilon_max, self.epsilon + self.epsilon_increment)
        self.learn_step_counter += 1
        return cost
 

RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        # x是车的水平位移，theta是杆离垂直的角度
        x, x_dot, theta, theta_dot = observation_
        # reward1是车越偏离中心越少
        reward1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        # reward2为杆越垂直越高
        reward2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        reward = reward1 + reward2
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost = RL.learn()
            print('cost: %.3f' % cost)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1

# mountain car
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
plt.plot(np.arange(len(cost)), cost)
plt.show()
```

- （2） MountainCar实例
    - car的轨迹是一维的，定位在两山之间，目标是爬上右边的山顶。可是car的发动机不足以一次性攀登到山顶，唯一的方式是car来回摆动增加动量。
    - ![](https://img-blog.csdn.net/20180308211123582)
    - 输出图：
        - ![](https://img-blog.csdn.net/2018032423074127)
    - 代码如下：

```python
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
 
plt.plot(np.arange(len(cost)), cost)
plt.show()
```

## PG（Policy Gradients）策略梯度算法

- 基于Policy Gradients（策略梯度法，简称PG）的深度强化学习方法，思想上与基于Q-learning的系列算法有本质的不同
- 普通的PG算法，只能用于解决一些小的问题，比如经典的让杆子竖起来，让小车爬上山等。如果想应用到更复杂的问题上，比如玩星际争霸，就需要更复杂的一些方法，比如后期出现的Actor Critic，Asynchronous Advantage Actor-Critic (A3C)等等

### 解决什么问题

- （1）很多决策的行动空间是**高维甚至连续（无限）**的
  - 比如自动驾驶中，汽车下一个决策中方向盘的行动空间，就是一个从[-900°，900°]（假设方向盘是两圈半打满）的无限空间中选一个值，如果我们用Q系列算法来进行学习，则需要对每一个行动都计算一次reward，那么对无限行动空间而言，哪怕是把行动空间离散化，针对每个离散行动计算一次reward的计算成本也是当前算力所吃不消的。这是对Q系列算法提出的第一个挑战：无法遍历行动空间中所有行动的reward值。
- （2）决策往往是带有**多阶段**属性的，“不到最后时刻不知输赢”。
  - 以即时策略游戏（如：星际争霸，或者国内流行的王者荣耀）为例，玩家的输赢只有在最后游戏结束时才能知晓，谁也没法在游戏进行过程中笃定哪一方一定能够赢。甚至有可能发生：某个玩家的每一步行动看起来都很傻，但是最后却能够赢得比赛，比如，Dota游戏中，有的玩家虽然死了很多次，己方的塔被拆了也不管，但是却靠着偷塔取胜（虽然这种行为可能是不受欢迎的）。诸如此类的情形就对Q系列算法提出了第二个挑战，Agent每执行一个动作（action）之后的奖励（reward）难以确定，这就导致Q值无法更新。

- 由此衍生出了基于PG的系列深度强化学习算法

### 算法介绍

- 流程
  - ![](https://img-blog.csdnimg.cn/20200823095929950.png)
- 图解
  - ![](https://img-blog.csdnimg.cn/20200823150933700.png)
- 注意
  - 神经网络设计过程中，最后一层一般采用Softmax函数(离散动作) 或者 高斯函数激活（连续动作）。
- 奖励分配代码示例

```python
def _discount_and_norm_rewards(self):
##该函数将最后的奖励，依次分配给前面的回合，越往前，分配的越少。除此之外，还将分配后的奖励归一化为符合正太分布的形式。
    discounted_ep_rs = np.zeros_like(self.ep_rs) #self.ep_rs就是一局中每一回合的奖励，一般前面回合都是0，只有最后一个回合有奖励（一局结束）
    running_add = 0
    for t in reversed(range(0, len(self.ep_rs))):
        running_add = running_add * self.gamma + self.ep_rs[t] #self.gamma，是衰减系数，该系数越大，前面回合分配到的奖励越少（都衰减了嘛）
        discounted_ep_rs[t] = running_add

    discounted_ep_rs = discounted_ep_rs - np.mean(discounted_ep_rs)
    discounted_ep_rs = discounted_ep_rs / np.std(discounted_ep_rs)
    return discounted_ep_rs
```

### PG与Q系列算法

- PG深度强化学习算法与Q系列算法相比，优势主要有[2]：
  - 可以处理连续动作空间或者高维离散动作空间的问题。
  - 容易收敛，在学习过程中，策略梯度法每次更新策略函数时，参数只发生细微的变化，但参数的变化是朝着正确的方向进行迭代，使得算法有更好的收敛性。而价值函数在学习的后期，参数会围绕着最优值附近持续小幅度地波动，导致算法难以收敛。
- 缺点主要有：
  - 容易收敛到局部最优解，而非全局最优解；
  - 策略学习效率低；
  - 方差较高：这个可谓是普通PG深度强化学习算法最不可忍受的缺点了，由于PG算法参数更新幅度较小，导致神经网络有很大的随机性，在探索过程中会产生较多的无效尝试。另外，在处理回合结束才奖励的问题时，会出现不一致的问题：回合开始时，同样的状态下，采取同样的动作，但是由于后期采取动作不同，导致奖励值不同，从而导致神经网络参数来回变化，最终导致Loss函数的方差较大。

- 参考
  - [一图看懂Policy Gradients深度强化学习算法](https://blog.csdn.net/xz15873139854/article/details/108179193)


# 基于模型的RL

前面介绍的是基于**价值**的强化学习(Value Based RL)和基于**策略**的强化学习模型(Policy Based RL)都属于model free，直接从价值函数+策略函数中学习，不用学习环境的状态转化概率模型，下面介绍另一种强化学习流派，基于**模型**的强化学习(Model Based RL)，以及基于模型的强化学习算法框架**Dyna**。

参考：
- UCL强化学习课程的第8讲和Dyna-2的[论文](https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf)
- [model-based RL（一）——基本框架](https://zhuanlan.zhihu.com/p/266131804)
- [上交张伟楠副教授：基于模型的强化学习算法，基本原理以及前沿进展](https://zhuanlan.zhihu.com/p/162787188)

## model-based（MBRL）与model free (MFRL)

不建立环境模型，仅依靠实际环境的采样数据进行训练学习的强化学习算法称为**无模型强化学习**（Model-Free Reinforcement Learning，MFRL）算法，也即是不依赖于环境模型的强化学习算法。

MFRL发展中遇到的一个困境：**数据采集效率**（Sample Efficiency）太低。在有监督或无监督学习中，人们构建一个目标函数，通过梯度下降（或上升）的方式，不断趋近理想结果。与有监督/无监督学习不同的是，强化学习属于一种试错的学习范式，当前策略的采样结果如果无法有效帮助当前策略进行提升，则可以认为当前试错的采样结果是无效采样。在MFRL训练过程中，智能体有大量的交互采样属于无效采样，这些采样没有对行动策略的改进产生明显的影响。为了解决无模型强化学习中的这一数据效率低下的问题，人们开始转向基于模型强化学习（Model-Based Reinforcement Learning，MBRL）的方法。

MBRL的基本思想在于首先建立一个环境的动态模型，然后在建立的环境模型中训练智能体的行动策略，通过这种方式，实现数据效率的提升。
MFRL存在如下特点：
1. 相比于MBRL，MFRL拥有最好的渐进性能（Asymptotic Performance），当策略与环境交互达到收敛状态时，相比于MBRL，MFRL下训练所得策略所最终达到的性能会更好，能够避免出现复合误差的问题，因而在实际环境中表现会更为优异。
2. MFRL非常适合使用深度学习框架去采样超大规模的数据，并进行网络训练。
3. MFRL经常采用Off-Policy训练方法，这种情况下会有偏差（Bias）导致的训练效果不稳定（instability）的问题。
4. MFRL需要进行超大量的数据采样，因而需要超高的算力要求，这种算力要求是很多科研院所或者企业所无法负担的。
MBRL存在如下特点：
1. 环境模型一旦建立起来，便可以采用on-policy的训练方法，利用当前采样得到的数据训练当前的策略，在这种情形下，采样效率是最高的。
2. 建立环境模型后，便可以选择性地不再与实际场景交互，在模型中进行训练学习，完成训练后再在实际场景中投入使用（off-line RL，也称为batch RL）。
3. 相比于MFRL，MBRL数据采样效率会往往有较大的提升。
4. 存在模型与实际环境之间的复合误差问题（Compounding Error），模型向后推演的幅度越长，推演误差就会越大，直至模型完全失去作用。


### MBRL

MBRL的进一步分类，其主要包括**黑盒模型**与**白盒模型**两类：
- 黑盒模型中，环境模型的构造是未知的，仅作为数据的采样来源。由于采样数据来自于黑盒模型，而不是和真实环境交互得到，因此这些来自模型的采样数据不计入数据采样效率的计算中。虽然从计算结果来看MFBL的数据采样效率较高，但由于训练过程中使用了大量基于模型采样的数据，因此从采样数据总量上来看，实际采样了更多的数据。常用的基于黑盒模型的MBRL算法包括**Dyna-Q**、**MPC**、**MBPO**等。
![](https://pic4.zhimg.com/80/v2-1942d467f25fbf74f4f6ca566b8860eb_720w.jpg)
图1：基于黑盒模型的MBRL算法
- 白盒模型中，环境模型的构造是已知的，可以将模型中状态的价值函数直接对策略的参数进行求导，从而实现对策略的更新。常用的基于白盒模型的MBRL算法包括**MAAC**、**SVG**、**PILCO**等。
![](https://pic2.zhimg.com/80/v2-9f43425e5a85498daa68c291986fc865_720w.jpg)

对比：
![](https://pic1.zhimg.com/80/v2-b857c15be4302c5ad4dd820f8dcd41fc_720w.jpg)

基于黑盒模型的MBRL：
![](https://pic2.zhimg.com/80/v2-7977cd1fddfb6f81a57a994ef21ba8d9_720w.jpg)

- 首先，当前的价值函数Q(s, a)以及策略函数π(a | s)与真实环境进行交互，完成交互后采样出环境反馈的数据Experience{(s, a, r, s’)}。然后通过采样出的数据来训练建立的环境模型p(s’, r | s, a )，环境的模型本质上是通过输入的当前状态State以及采取的动作Action，来预测产生的Reward以及下一步的状态State，很多情况下Reward是根据先验规则或领域知识生成，这时模型只预测下一步的状态State即可。接下来在当前模型中进行Plannning，也就是通过当前模型进行数据的采样，通过数据采样的结果去训练新一轮的价值函数Q(s, a)以及策略函数π(a | s)。
- Q-Planning是最简单的MBRL算法，它通过与真实环境交互的数据来训练环境模型，然后基于当前的环境模型来进行一步Planning，从而完成Q函数的训练。其步骤是，首先采集智能体与环境交互的State以及采用的Action，将数据传入建立的黑盒模型中，采集并得到模型虚拟出来的Reward以及下一步的Next_State，接着将传入模型的State、Action以及模型虚拟出来的Reward、Next_State进行一步Q-Learning训练，这样就完成了一步Q-Planning算法的更新。期间智能体仅仅通过与环境模型交互来进行数据的采样，得到虚拟的Reward以及下一步的Next_State，并进行策略的训练和更新，智能体未通过与实际环境交互数据来进行策略的更新。
- 还可以将Q-Planning与Q-Learning结合在一起，形成 Dyna算法。Dyna算法提出于90年代早期，完整的流程示意图如下所示。可以看到，将中间的direct RL步骤去掉后，就是刚刚讨论的Q-Planning算法。
![](https://pic4.zhimg.com/80/v2-39b0ed9d4d6e34a4386d552409c2bd0f_720w.jpg)

在Dyna算法中，首先通过智能体与实际环境的交互进行一步正常的Q-Learning操作，然后通过与实际环境交互时使用的State、Action，传入环境模型中进行Q-Planning操作，真实环境中进行一步Q-Learning操作，对应环境模型中进行n步Q-Planning操作。实际环境中的采样与模型中的采样具有1：n的关系，通过这种方式来提升训练过程中的Sample efficiency。

### MBRL问题

MBRL算法的发展面临着三个关键问题，同时这也是发展的三个思路：
1. 环境模型的建立是否真的有助于Sample Efficiency的提高？
2. 所建立的模型基本都是基于神经网络建立，不可避免的会出现泛化性误差的问题，那么人们什么时候可以相信建立的模型并使用模型进行策略的训练与更新？
3. 如何适当地把握模型的使用尺度，来获得最好的或者至少获得正向的策略训练结果？



model-free方法有两种方式
- **value-based**方法先学习**值函数**（MC或TD）再更新策略
- **policy-based**方法直接将真实轨迹数据（real experience）更新策略。

而model-based方法先将着重点放在**环境模型**(environment dynamics)上，通过采样先学习一个对环境的建模，再根据学习到的环境模型做值函数/策略优化。在model-based方法中，planning步骤至关重要，正是通过在learned model基础上做planning才提高了整个强化学习算法迭代的效率。

![](https://pic1.zhimg.com/80/v2-b70189e8c85af6605c39eda35aec2398_720w.jpg)


完成了对环境的建模后，在model-based大类方法中同样有两种路径
- 一种是通过学到的model生成一些仿真轨迹，通过仿真轨迹估计值函数来优化策略；
- 另一种是通过学到的model直接优化策略，这是目前model-based方法常走的路线。

- Pros：先学习model的一个显而易见的好处，就是解决model-free强化学习方法中样本效率的问题(sample efficiency)。
- Cons：由于模型是在过程中学习的，难免存在偏差；难以保证收敛

model可以理解为由**状态转移分布**和**回报函数**组成的元组 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%28P%2C+R%29) 。其中 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D+%5Csim+P_%7B%5Ceta%7D%28S_%7Bt%2B1%7D%7CS_t%2C+A_t%29) ， ![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%5Csim+R_%7B%5Ceta%7D%28R_%7Bt%2B1%7D%7CS_t%2C+A_t%29)

如何学习模型？
- **状态-动作**对到**回报**的映射可以看作是一个**回归**问题，loss函数可以设置为均方误差；
- 学习状态-动作对到下一状态的映射可以看作**密度估计**问题，loss函数可以设置为KL散度。

模型的构造也有很多选择（神经网络、高斯过程等等），从而派生出不同的算法。

![](https://pic2.zhimg.com/80/v2-e8e93fa230d39cb0e33bb69e3892b025_720w.jpg)

RL是基于**经验**的学习方法。**实际经验**是真实的智能体和环境交互得到的数据。比如一个机械臂尝试从一堆物品中抓取物品，无人车在道路上行驶。最直接的作用是用来学习**值函数**或者**策略**，称之为**直接强化学习**（direct RL）。还可以利用实际经验来学习模型，叫做**模型学习**（model learning）。当然我们的模型不一定非得要学，根据物理定理也能建立起模型，但基于数据驱动的模型学习也是一个方法。那学习了模型之后呢？怎么用它？我们可以用学习的模型来产生经验，叫做**仿真经验**。然后利用仿真经验来辅助策略学习，叫做**间接强化学习**（indirect RL）。

Direct RL VS. Indirect RL, 谁好谁坏？直接RL和间接RL都有自己的优缺点。
- 间接RL可以充分的利用经验，减少和环境的交互。可以类比为人类的冥想，mental rehearsal。没必要都去试一下才知道怎么做，想一想可能就知道答案了，这样就节省了体力。
- 直接RL更简单，并且不会受到模型偏差的影响。试想如果一个精神病患者，你让他冥想，不知道他会做出什么举动来。

这种争论在AI和心理学领域可以归结为：**认知**和**试错**学习的争执，精细编程和反应式决策的争执。还在于：model-based 和model-free的争执。

## 表格型：Dyna-Q框架

把模型学习，规划和决策融合在一起，在线的进行规划学习

Dyna-Q中，**直接RL**就使用**Q学习**（one-step tabular Q-learning）。规划算法就使用上一节提到的随机采样Q-规划算法(one-step random sample tabular Q-planning)。目前为止，所有算法都是针对于**表格型**的问题。也就是说状态动作都是有限离散的。因此在学习模型的时候，我们就可以把所有的转移动作对 [公式] 存起来。所示实际上模型学习就是存储，规划就是从存的数据里面拿数据，然后用Q-learning。
整个Dyna-Q的学习框图
![](https://pic3.zhimg.com/80/v2-939c424679c7dc29f12cd4a2a645b3a6_720w.jpg)

如迷宫问题上，考察Dyna-Q的效果。其实可以想象，引入规划无非就是为了增加学习效率。所以Dyna-Q和Q-learning相比学的更快。实际的效果如下：
 
![](https://pic3.zhimg.com/80/v2-51d76b47dd52c1182a57f92e776205d6_720w.jpg)
 
规划的步数越多，学习的就越快。纵轴表示每个episode的步数。如果是学到了一个策略，自然步数是最小的（不走弯路）。对比一下2个episode后的策略：
 
![](https://pic2.zhimg.com/80/v2-d7050c5d548e51c7fe712cefcf41c699_720w.jpg)
 
可以发现，有规划的算法值函数的更新更有效。

摘自：[8.2 Dyna:融合规划，决策和学习](https://zhuanlan.zhihu.com/p/59896757)

## model-based策略优化

![](https://pic2.zhimg.com/80/v2-cc7f6fbcb5039810051c7f0e8d4970e9_720w.jpg)

- 如果环境模型已知，轨迹优化问题就是一个最优控制问题
  - ![[公式]](https://www.zhihu.com/equation?tex=min%5Csum_%7Bt%3D1%7D%5E%7BT%7D%7Bc%28s_t%2Ca_t%29%7D)
  - ![[公式]](https://www.zhihu.com/equation?tex=s.t.++%5Cquad+s_t%3Df%28s_%7Bt-1%7D%2Ca_%7Bt-1%7D%29)
- 如果环境模型未知，把model learning和trajectory optimization结合起来

## [Deep Reinforcement Learning Algorithms with PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)

This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. 

## **Algorithms Implemented** 

1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  
1. *DQN with Fixed Q Targets* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>
1. *Double DQN (DDQN)* <sub><sup> ([Hado van Hasselt et al. 2015](https://arxiv.org/pdf/1509.06461.pdf)) </sup></sub>
1. *DDQN with Prioritised Experience Replay* <sub><sup> ([Schaul et al. 2016](https://arxiv.org/pdf/1511.05952.pdf)) </sup></sub>
1. *Dueling DDQN* <sub><sup> ([Wang et al. 2016](http://proceedings.mlr.press/v48/wangf16.pdf)) </sup></sub>
1. *REINFORCE* <sub><sup> ([Williams et al. 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)) </sup></sub>
1. *Deep Deterministic Policy Gradients (DDPG)* <sub><sup> ([Lillicrap et al. 2016](https://arxiv.org/pdf/1509.02971.pdf) ) </sup></sub>
1. *Twin Delayed Deep Deterministic Policy Gradients (TD3)* <sub><sup> ([Fujimoto et al. 2018](https://arxiv.org/abs/1802.09477)) </sup></sub>
1. *Soft Actor-Critic (SAC)* <sub><sup> ([Haarnoja et al. 2018](https://arxiv.org/pdf/1812.05905.pdf)) </sup></sub>
1. *Soft Actor-Critic for Discrete Actions (SAC-Discrete)* <sub><sup> ([Christodoulou 2019](https://arxiv.org/abs/1910.07207)) </sup></sub> 
1. *Asynchronous Advantage Actor Critic (A3C)* <sub><sup> ([Mnih et al. 2016](https://arxiv.org/pdf/1602.01783.pdf)) </sup></sub>
1. *Syncrhonous Advantage Actor Critic (A2C)*
1. *Proximal Policy Optimisation (PPO)* <sub><sup> ([Schulman et al. 2017](https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf)) </sup></sub>
1. *DQN with Hindsight Experience Replay (DQN-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *DDPG with Hindsight Experience Replay (DDPG-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf) ) </sup></sub>
1. *Hierarchical-DQN (h-DQN)* <sub><sup> ([Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Stochastic NNs for Hierarchical Reinforcement Learning (SNN-HRL)* <sub><sup> ([Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf)) </sup></sub>
1. *Diversity Is All You Need (DIAYN)* <sub><sup> ([Eyensbach et al. 2018](https://arxiv.org/pdf/1802.06070.pdf)) </sup></sub>

All implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions), 
Bit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.

## **Environments Implemented**

1. *Bit Flipping Game* <sub><sup> (as described in [Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *Four Rooms Game* <sub><sup> (as described in [Sutton et al. 1998](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)) </sup></sub>
1. *Long Corridor Game* <sub><sup> (as described in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Ant-{Maze, Push, Fall}* <sub><sup> (as desribed in [Nachum et al. 2018](https://arxiv.org/pdf/1805.08296.pdf) and their accompanying [code](https://github.com/tensorflow/models/tree/master/research/efficient-hrl)) </sup></sub>

## **Results**

#### 1. Cart Pole and Mountain Car

Below shows various RL algorithms successfully learning discrete action game [Cart Pole](https://github.com/openai/gym/wiki/CartPole-v0)
 or continuous action game [Mountain Car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). The mean result from running the algorithms 
 with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters
 used can be found in files `results/Cart_Pole.py` and `results/Mountain_Car.py`. 
 
![Cart Pole and Mountain Car Results](results/data_and_graphs/CartPole_and_MountainCar_Graph.png) 


#### 2. Hindsight Experience Replay (HER) Experiements

Below shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits) 
and Fetch Reach environments described in the papers [Hindsight Experience Replay 2018](https://arxiv.org/pdf/1707.01495.pdf) 
and [Multi-Goal Reinforcement Learning 2018](https://arxiv.org/abs/1802.09464). The results replicate the results found in 
the papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference 
between them was whether hindsight was used or not. 

![HER Experiment Results](results/data_and_graphs/HER_Experiments.png)

#### 3. Hierarchical Reinforcement Learning Experiments

The results on the left below show the performance of DQN and the algorithm hierarchical-DQN from [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)
on the Long Corridor environment also explained in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf). The environment
requires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayed 
gratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce a 
meta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. This 
aligns with the results found in the paper. 

The results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning 
(SNN-HRL) from [Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf). DDQN is used as the comparison because
the implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of training
for SNN-HRL were used for pre-training which is why there is no reward for those episodes. 
 
![Long Corridor and Four Rooms](results/data_and_graphs/Four_Rooms_and_Long_Corridor.png)


# 应用

## 量化交易

【2021-4-6】[强化学习（Reinforcement Learning）在量化交易领域如何应用](https://www.zhihu.com/question/45116323/answer/758082798)

>讲个大实话：这个问题的答案其实都不用看，肯定都不靠谱，靠谱的肯定不会告诉你

感兴趣的朋友可以在[BigQuant AI](https://bigquant.com/%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)平台上动手实践一下
 
1\. 相比（无）监督学习，强化学习在量化领域应用时，首先需要建立一个环境，在环境中定义state，action，以及reward等。定义的方式有多种选择，比如：
- **state**: 可以将n天的价格，交易量数据组合成某一天的state,也可以用收益率或是其他因子组合作为某一天的state，如果想要定义有限个的state,可以定义为appreciated/ hold_value/ depreciated这样3类。
- **action**: 可以定义为buy/sell两种, 也可以定义为buy/sell/hold三种，或者定义为一个（-1,1）之间的一个连续的数，-1和1分别代表all out 和 holder两个极端。
- **reward**: 可以定义为新旧总资产价值之间的差，或是变化率，也可以将buy时的reward定义为0，sell时的定义为买卖价差。
 
2\. 需要选择一个具体的强化学习方法：
 
1) Q-table (具体可参考：[Reinforcement Learning Stock Trader](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer))
 
![](https://pic1.zhimg.com/50/v2-32586bdcd7b20c55790c2447583e3b2a_hd.jpg?source=1940ef5c)
 
![](https://pic1.zhimg.com/80/v2-32586bdcd7b20c55790c2447583e3b2a_720w.jpg?source=1940ef5c)
 
Q-table里state是有限的，而我们定义的state里面的数据往往都是连续的，很难在有限个state里面去很好的表达。
 
2) Deep Q Network（参考：[Reinforcement Learning for Stock Prediction](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer) ）
 
![](https://pic2.zhimg.com/50/v2-d3b707c6c64979f4309f1227728c8ab4_hd.jpg?source=1940ef5c)
 
![](https://pic2.zhimg.com/80/v2-d3b707c6c64979f4309f1227728c8ab4_720w.jpg?source=1940ef5c)
 
在1的基础上，将Q-table的功能用一个深度学习网络来实现，解决了有限个state的问题。
 
  
 
3) Actor Critic （参考：[Deep-Reinforcement-Learning-in-Stock-Trading](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)）
 
![](https://pic1.zhimg.com/50/v2-cb2603beb4737f06c371cbbe0f07d3a4_hd.jpg?source=1940ef5c)
 
![](https://pic1.zhimg.com/80/v2-cb2603beb4737f06c371cbbe0f07d3a4_720w.jpg?source=1940ef5c)
 
用两个模型，一个同DQN输出Q值，另一个直接输出行为。但由于两个模型参数更新相互影响，较难收敛。
 
4) DDPG（参考：[ml-stock-prediction](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)）
 
![](https://pic1.zhimg.com/50/v2-5caa0ee24f9e5466f54cda356c241985_hd.jpg?source=1940ef5c)
 
![](https://pic1.zhimg.com/80/v2-5caa0ee24f9e5466f54cda356c241985_720w.jpg?source=1940ef5c)
 
加入了不及时更新参数的模型，解决难收敛的问题
 
3\. 由于多数方法中都用到了深度神经网络，我们还需要对神经网络的模型，深度，还有其他参数进行一个选择。

4\. 比较简单的应用逻辑是对单个股票某一时间段进行择时，如果需要也可以在这个基础上进行一些调整，对某个股票池的股票进行分析，调整为一个选股策略。


# 问题

## 为什么说强化学习在近年不会被广泛应用

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
  - （1）**数据收集过程不可控**
    - 不同于监督学习，强化学习的数据来自agent跟环境的各种交互。对于数据平衡性问题，监督学习可以通过各种补数据加标签来达到数据平衡。但这件事情对强化学习确实非常难以解决的，因为数据收集是由policy来做的，无论是DQN的Q-network还是AC架构里的actor，它们在学习过程中，对于任务激励信号的理解的不完善的，很可能绝大部分时间都在收集一些无用且重复的数据。虽然有prioritized replay buffer来解决训练优先级的问题，但实际上是把重复的经验数据都丢弃了。在实际应用中，面对一些稍微复杂点的任务还是需要收集一大堆重复且无用的数据。这也是DRL sample efficiency差的原因之一。
  - （2）**环境限制**
    - DRL问题中，环境都是从初始状态开始，这限制了很多可能的优化方向。比如在状态空间中，可以针对比较“新”的状态重点关注，控制环境都到这个状态。但目前的任务，很多环境的state-transition function都是stochastic的，都是概率函数。即便记录下来之前的action序列，由于环境状态转移的不确定性，也很难到达类似的状态。更别提policy本身也是stochastic的，这种双重stochastic叠加，不可能针对“重点”状态进行优化。
    - 同时这种限制也使得一些测试场景成为不可能。比如自动驾驶需要测试某个弯道，很难基于当前的policy在状态空间中达到类似的状态来重复测试policy在此状态下的鲁棒性。
  - （3）玄之又玄，**可解释性较差**
    - 本来Q-learning就是一个通过逐步学习来完善当前动作对未来收益影响作出估计的过程。加入DNN后，还涉及到了神经网络近似Q的训练。这就是“不靠谱”上又套了一层“不靠谱”。如何验证策略是正确的？如何验证Q function是最终收敛成为接近真实的估计？这些问题对于查表型的Q-learning来说，是可以解决的，无非就是工作量的问题。但对于大规模连续状态空间的DQN来说，基本上没法做。论证一个policy有效，也就是看看render以后的效果，看看reward曲线，看看tensorborad上的各个参数。连监督学习基本的正确率都没有。然后还要根据这些结果来调reward function，基本上都在避免回答why这个问题。
  - （4）**随机探索**
    - DRL的探索过程还是比较原始。现在大多数探索，epsilon-greedy，UCB都是从多臂老虎机来的，只针对固定state的action选择的探索。扩展到连续状态空间上，这种随机探索还是否有效，在实际落地过程中，还是要打个问号。因为也不能太随机了。大家都说PPO好，SAC强，探索过程也只不过是用了stochastic policy，做了个策略分布的熵的最大化。本质还是纯随机。虽然有些用好奇心做探索的工作，但也还是只把探索任务强加给了reward，指标不治本。
  - 当前DRL的实际科研的进步速度要远远慢于大众对于AI=DL+RL的期望。能解决的问题：
    - **固定场景**：状态空间不大，整个trajectory不长
    - **问题不复杂**：没有太多层次化的任务目标，奖励好设计
    - **试错成本低**：咋作都没事
    - **数据收集容易**：百万千万级别的数据量，如果不能把数据收集做到小时级别，那整个任务的时间成本就不太能跟传统的监督相比
    - **目标单纯**：容易被reward function量化，比如各种super-human的游戏。对于一些复杂的目标，比如几大公司都在强调拟人化，目前没有靠谱的解决方案
  - 落地领域也就游戏了，而且是简单游戏，比如固定场景、小地图上的格斗，比如街霸、王者之类。要是大地图、开放世界的话，光捡个枪、开个宝箱就能探索到猴年马月了。也没想象中那么fancy，基本没有图像类输入，全是传感器类的内部数据，所以同类型任务的训练难度还没到Atari级别，这几年时间，DOTA2和星际基本上是游戏领域内到顶的落地
    - DeepMind近期的一篇论文：[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)。这篇论文对原始DQN框架做了一些渐进式改进，证明他们的RainbowDQN性能更优。
  - 做强化的同学们一点信息：
    - 强化学习岗位很少，因为落地难+烧钱，基本只有几个头部游戏公司会养一个规模不大的团队。
    - 纯强化的技术栈不太好跳槽，除了游戏外，别的领域很难有应用。
    - 20年huawei的强化夏令营，同时在线也有好几万人，想想这规模，未来几年这些研究生到job market会多卷。
- **强化学习是唯一一个可以明目张胆地在测试集上进行训练的机器学习网络**。都2020了强化学习除了能玩游戏还能做什么？强化学习的特点是面向目标的算法，过程基本很难拆解，没法管控，如果目标没法在商业公司被很好的认可
- 每当有人问我强化学习能否解决他们的问题时，我会说“不能”。而且我发现这个回答起码在70%的场合下是正确的。
- 更多参考：[深度强化学习的弱点和局限](https://zhuanlan.zhihu.com/p/34089913)，[原文:Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- Deep RL实现进一步发展的一些条件：[原文地址](www.alexirpan.com/2018/02/14/rl-hard.html)，[中文版本: 深度强化学习的弱点和局限](https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247485609&idx=1&sn=6b71f5f8ebd4e920384f07b97ce92a9c&chksm=eb4eec6adc39657c81169f1ae9ce477e4da692941238c35deb26a11ed7ec70073784cfd935a8#rd)
  - 易于产生近乎无限的经验；
  - 把问题简化称更简单的形式；
  - 将自我学习引入强化学习；
  - 有一个清晰的方法来定义什么是可学习的、不可取消的奖励；
  - 如果奖励必须形成，那它至少应该是种类丰富的。
- 下面是我列出的一些关于未来研究趋势的合理猜测 ，希望未来Deep RL能带给我们更多惊喜。
  - 局部最优就足够了。我们一直以来都追求全局最优，但这个想法会不会有些自大呢？毕竟人类进化也只是朝着少数几个方向发展。也许未来我们发现局部最优就够了，不用盲目追求全局最优；
  - 代码不能解决的问题，硬件来。我确信有一部分人认为人工智能的成就源于硬件技术的突破，虽然我觉得硬件不能解决所有问题，但还是要承认，硬件在这之中扮演着重要角色。机器运行越快，我们就越不需要担心效率问题，探索也更简单；
  - 添加更多的learning signal。稀疏奖励很难学习，因为我们无法获得足够已知的有帮助的信息；
  - 基于模型的学习可以释放样本效率。原则上来说，一个好模型可以解决一系列问题，就像AlphaGo一样，也许加入基于模型的方法值得一试；
  - 像参数微调一样使用强化学习。第一篇AlphaGo论文从监督学习开始，然后在其上进行RL微调。这是一个很好的方法，因为它可以让我们使用一种速度更快但功能更少的方法来加速初始学习；
  - 奖励是可以学习的。如果奖励设计如此困难，也许我们能让系统自己学习设置奖励，现在模仿学习和反强化学习都有不错的发展，也许这个思路也行得通；
  - 用迁移学习帮助提高效率。迁移学习意味着我们能用以前积累的任务知识来学习新知识，这绝对是一个发展趋势；
  - 良好的先验知识可以大大缩短学习时间。这和上一点有相通之处。有一种观点认为，迁移学习就是利用过去的经验为学习其他任务打下一个良好的基础。RL算法被设计用于任何马尔科夫决策过程，这可以说是万恶之源。那么如果我们接受我们的解决方案只能在一小部分环境中表现良好，我们应该能够利用共享来解决所有问题。之前Pieter Abbeel在演讲中称Deep RL只需解决现实世界中的任务，我赞同这个观点，所以我们也可以通过共享建立一个现实世界的先验，让Deep RL可以快速学习真实的任务，作为代价，它可以不太擅长虚拟任务的学习；
  - 难与易的辨证转换。这是BAIR（Berkeley AI Research）提出的一个观点，他们从DeepMind的工作中发现，如果我们向环境中添加多个智能体，把任务变得很复杂，其实它们的学习过程反而被大大简化了。让我们回到ImageNet：在ImageNet上训练的模型将比在CIFAR-100上训练的模型更好地推广。所以可能我们并不需要一个高度泛化的强化学习系统，只要把它作为通用起点就好了。



# 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)
- 强化学习圣经：《强化学习导论》第二版（附[PDF](http://www.incompleteideas.net/book/the-book.html)下载），配套[python代码](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
- [无需博士学位的TensorFlow深度强化学习教程](https://www.bilibili.com/video/av23286922/) by Martin Gorner

<iframe src="//player.bilibili.com/player.html?aid=23286922&bvid=BV1MW411F7yA&cid=38785810&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

- 李宏毅深度强化学习(国语)课程(2018)，[B站视频](https://www.bilibili.com/video/av24724071)

<iframe src="//player.bilibili.com/player.html?aid=24724071&bvid=BV1MW411w79n&cid=41583412&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>


# 结束


