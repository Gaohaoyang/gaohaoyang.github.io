---
layout: post
title:  "神经网络调参技巧-The tuning skills of neural network"
date:   2021-03-15 00:00:00
categories: 机器学习 
tags:  神经网络 超参
excerpt: 神经网络调参技能汇总
author: 鹤啸九天
mathjax: true
---

* content
{:toc}

# 总结

- [李飞飞高徒、特斯拉AI总结发布机器学习33条秘技](https://www.toutiao.com/a6684054932271661575/)，原文[链接](http://karpathy.github.io/2019/04/25/recipe/)
- [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
- [The Black Magic of Deep Learning - Tips and Tricks for the practitioner](https://nmarkou.blogspot.com.cy/2017/02/the-black-magic-of-deep-learning-tips.html)
- 【2020-3-2】[如何优雅地训练大型模型？](https://zhuanlan.zhihu.com/p/110278004)
  - 现实总是残酷的，其实限制大模型训练只有两个因素：时间和空间(=GPU=钱)，根据不同情况可以使用的方案大致如下：
  - ![](https://pic2.zhimg.com/v2-a135adb5fb3fefb48388449712072ad8_1440w.jpg?source=172ae18b)

# 调参秘方

- 摘自：[训练模型的“处方”](https://www.toutiao.com/a6684054932271661575)
  - ![](https://p3-tt.byteimg.com/origin/pgc-image/922d6327dfc0494581eb0f1a0c5bd911?from=pc)
- 总的来说，Andrej Karpathy的技巧就是：**不要心急** (文章结尾会道出原因) ，从简单到复杂逐步完善你的神经网络。
- 1、先别着急写代码
  - 训练神经网络前，别管代码，先从预处理数据集开始。我们先花几个小时的时间，了解数据的分布并找出其中的规律。
  - Andrej有一次在整理数据时发现了重复的样本，还有一次发现了图像和标签中的错误。所以先看一眼数据能避免我们走很多弯路。
  - 由于神经网络实际上是数据集的压缩版本，因此您将能够查看网络（错误）预测并了解它们的来源。如果你的网络给你的预测看起来与你在数据中看到的内容不一致，那么就会有所收获。
  - 一旦从数据中发现规律，可以编写一些代码对他们进行搜索、过滤、排序。把数据可视化能帮助我们发现异常值，而异常值总能揭示数据的质量或预处理中的一些错误。
- 2、设置端到端的训练评估框架
  - 处理完数据集，接下来就能开始训练模型了吗？并不能！下一步是建立一个完整的训练+评估框架。
  - 在这个阶段，我们选择一个简单又不至于搞砸的模型，比如线性分类器、CNN，可视化损失。获得准确度等衡量模型的标准，用模型进行预测。
  - 这个阶段的技巧有：
    - 固定随机种子: 使用固定的随机种子，来保证运行代码两次都获得相同的结果，消除差异因素。
    - 简单化: 在此阶段不要有任何幻想，不要扩增数据。扩增数据后面会用到，但是在这里不要使用，现在引入只会导致错误。
    - 在评估中添加有效数字: 在绘制测试集损失时，对整个测试集进行评估，不要只绘制批次测试损失图像，然后用Tensorboard对它们进行平滑处理。
    - 在初始阶段验证损失函数: 验证函数是否从正确的损失值开始。例如，如果正确初始化最后一层，则应在softmax初始化时测量-log(1/n_classes)。
    - 初始化: 正确初始化最后一层的权重。如果回归一些平均值为50的值，则将最终偏差初始化为50。如果有一个比例为1:10的不平衡数据集，请设置对数的偏差，使网络预测概率在初始化时为0.1。正确设置这些可以加速模型的收敛。
    - 人类基线: 监控除人为可解释和可检查的损失之外的指标。尽可能评估人的准确性并与之进行比较。或者对测试数据进行两次注释，并且对于每个示例，将一个注释视为预测，将第二个注释视为事实。
    - 设置一个独立于输入的基线: 最简单的方法是将所有输入设置为零，看看模型是否学会从输入中提取任何信息。
    - 过拟合一个batch: 增加了模型的容量并验证我们可以达到的最低损失。
    - 验证减少训练损失: 尝试稍微增加数据容量。
    - 在训练模型前进行数据可视化: 将原始张量的数据和标签可视化，可以节省了调试次数，并揭示了数据预处理和数据扩增中的问题。
    - 可视化预测动态: 在训练过程中对固定测试批次上的模型预测进行可视化。
    - 使用反向传播来获得依赖关系：一个方法是将第i个样本的损失设置为1.0，运行反向传播一直到输入，并确保仅在第i个样本上有非零的梯度。
    - 概括一个特例：对正在做的事情编写一个非常具体的函数，让它运行，然后在以后过程中确保能得到相同的结果。
- 3、过拟合
  - 首先我们得有一个足够大的模型，它可以过拟合，减少训练集上的损失，然后适当地调整它，放弃一些训练集损失，改善在验证集上的损失）。
  - 这一阶段的技巧有：
    - 挑选模型: 为了获得较好的训练损失，我们需要为数据选择合适的架构。不要总想着一步到位。如果要做图像分类，只需复制粘贴ResNet-50，我们可以在稍后的过程中做一些自定义的事。
    - Adam方法是安全的: 在设定基线的早期阶段，使用学习率为3e-4的Adam 。根据经验，亚当对超参数更加宽容，包括不良的学习率。
    - 一次只复杂化一个: 如果多个信号输入分类器，建议逐个输入，然后增加复杂性，确保预期的性能逐步提升，而不要一股脑儿全放进去。比如，尝试先插入较小的图像，然后再将它们放大。
    - 不要相信学习率衰减默认值: 如果不小心，代码可能会过早地将学习率减少到零，导致模型无法收敛。我们完全禁用学习率衰减避免这种状况的发生。
- 4、正则化
  - 理想的话，我们现在有一个大模型，在训练集上拟合好了。现在，该正则化了。舍弃一点训练集上的准确率，可以换取验证集上的准确率。
  - 这里有一些技巧：
    - 获取更多数据: 至今大家最偏爱的正则化方法，就是添加一些真实训练数据。不要在一个小数据集花太大功夫，试图搞出大事情来。有精力去多收集点数据，这是唯一一个确保性能单调提升的方法。
    - 数据扩增: 把数据集做大，除了继续收集数据之外，就是扩增了。旋转，翻转，拉伸，做扩增的时候可以野性一点。
    - 有创意的扩增: 还有什么办法扩增数据集？比如域随机化 (Domain Randomization) ，模拟 (Simulation) ，巧妙的混合 (Hybrids) ，比如把数据插进场景里去。甚至可以用上GAN。
    - 预训练: 当然，就算你手握充足的数据，直接用预训练模型也没坏处。
    - 跟监督学习死磕: 不要对无监督预训练太过兴奋了。至少在视觉领域，无监督到现在也没有非常强大的成果。虽然，NLP领域有了BERT，有了会讲故事的GPT-2，但我们看到的效果很大程度上还是经过了人工挑选。
    - 输入低维一点: 把那些可能包含虚假信号的特征去掉，因为这些东西很可能造成过拟合，尤其是数据集不大的时候。同理，如果低层细节不是那么重要的话，就输入小一点的图片，捕捉高层信息就好了。
    - 模型小一点: 许多情况下，都可以给网络加上领域知识限制 (Domain Knowledge Constraints) ，来把模型变小。比如，以前很流行在ImageNet的骨架上放全连接层，但现在这种操作已经被平均池化取代了，大大减少了参数。
    - 减小批尺寸: 对批量归一化 (Batch Normalization) 这项操作来说，小批量可能带来更好的正则化效果 (Regularization) 。
    - Dropout: 给卷积网络用dropout2d。不过使用需谨慎，因为这种操作似乎跟批量归一化不太合得来。
    - 权重衰减: 增加权重衰减 (Weight Decay) 的惩罚力度。
    - 早停法: 不用一直一直训练，可以观察验证集的损失，在快要过拟合的时候，及时喊停。
    - 也试试大点的模型: 注意，这条紧接上条 (且仅接上条) 。我发现，大模型很容易过拟合，几乎是必然，但早停的话，模型可以表现很好。
  - 最后的最后，如果想要更加确信，自己训练出的网络，是个不错的分类器，就把第一层的权重可视化一下，看看边缘 (Edges) 美不美。
  - 如果第一层的过滤器看起来像噪音，就需要再搞一搞了。同理，激活 (Activations) 有时候也会看出瑕疵来，那样就要研究一下哪里出了问题。
- 5、调参
  - 读到这里，你的AI应该已经开始探索广阔天地了。这里，有几件事需要注意。
    - 随机网格搜索: 在同时调整多个超参数的情况下，网格搜索听起来是很诱人，可以把各种设定都包含进来。但是要记住，随机搜索才是最好的。直觉上说，这是因为网络通常对其中一些参数比较敏感，对其他参数不那么敏感。如果参数a是有用的，参数b起不了什么作用，就应该对a取样更彻底一些，不要只在几个固定点上多次取样。
    - 超参数优化: 世界上，有许多许多靓丽的贝叶斯超参数优化工具箱，很多小伙伴也给了这些工具好评。但我个人的经验是，State-of-the-Art都是用实习生做出来的 (误) 。
- 6、还能怎么压榨**
  - 当你已经找到了好用的架构和好用的超参数，还是有一些技巧，可以在帮你现有模型上获得更好的结果，榨干最后一丝潜能：
    - 模型合体: 把几个模型结合在一起，至少可以保证提升2%的准确度，不管是什么任务。如果，你买不起太多的算力，就用蒸馏 (Distill) 把模型们集合成一个神经网络。
    - 放那让它训练吧: 通常，人类一看到损失趋于平稳，就停止训练了。但我感觉，还是训练得昏天黑地，不知道多久了，比较好。有一次，我意外把一个模型留在那训练了一整个寒假。我回来的时候，它就成了State-of-the-Art。


# 超参优化

【2020-4-23】机器学习中模型性能的好坏往往与超参数(如batch size,filter size等)有密切的关系。

- 超参数优化存在着一些比较复杂的问题：
  - 1、非线性，比如不能单独的固定xgboost的所有参数，单独优化n_estimators，因为参数和最终的评价指标之间并不一定是线性关系；
  - 2、非凸，比如逻辑回归的正则化系数，直接写入损失函数中作为一个变量你会发现无法使用梯度下降法来求解，
  - 3、组合优化：不同超参数的组合非常多，比如神经网络的层数，每层的神经元个数，每层使用的激活函数，使用的最优化算法等，这些参数随便组合起来就上百万了。
  - 4、混合优化：参数中既有连续变量（比如lr的正则项系数），又有类别变量（比如选择l1或者l2）。
  - 5、成本高，每一次训练都消耗一定的时间，如果数据量大模型复杂更是消耗时间，所以不可能穷举。

[马东什么](https://zhuanlan.zhihu.com/p/90374910)

- 最开始为了找到一个好的超参数，通常都是靠人工试错的方式找到"最优"超参数。但是这种方式效率太慢，所以相继提出了网格搜索(Grid Search, GS) 和 随机搜索(Random Search,RS)。
- 但是GS和RS这两种方法总归是盲目地搜索，所以贝叶斯优化(Bayesian Optimization,BO) 算法闪亮登场。BO算法能很好地吸取之前的超参数的经验，更快更高效地最下一次超参数的组合进行选择。
- 调参算法的输入是用户指定的参数及其范围，比如设定学习率范围为[0.0001, 0.01]。比较常见的算法为网格搜索，随机搜索和贝叶斯优化等。
- Grid search
  - 遍历所有可能的参数组合。网格搜索很容易理解和实现，例如我们的超参数A有2种选择，超参数B有3种选择，超参数C有5种选择，那么我们所有的超参数组合就有2 * 3 * 5也就是30种，我们需要遍历这30种组合并且找到其中最优的方案，对于连续值我们还需要等间距采样。实际上这30种组合不一定取得全局最优解，而且计算量很大很容易组合爆炸，并不是一种高效的参数调优方法。
- Random search
  - 限定搜索次数，随机选择参数进行实验。业界公认的Random search效果会比Grid search好，Random search其实就是随机搜索，例如前面的场景A有2种选择、B有3种、C有5种、连续值随机采样，那么每次分别在A、B、C中随机取值组合成新的超参数组合来训练。虽然有随机因素，但随机搜索可能出现效果特别差、也可能出现效果特别好，在尝试次数和Grid search相同的情况下一般最值会更大
- Bayesian Optimization
  - 业界的很多参数调优系统都是基于贝叶斯优化的，如Google Vizier [1], SigOpt[2].
  - 该算法要求已经存在几个样本点（一开始可以采用随机搜索来确定几个初始点），并且通过高斯过程回归（假设超参数间符合联合高斯分布）计算前面n个点的后验概率分布，得到每一个超参数在每一个取值点的期望均值和方差，其中均值代表这个点最终的期望效果，均值越大表示模型最终指标越大，方差表示这个点的效果不确定性，方差越大表示这个点不确定是否可能取得最大值非常值得去探索。
  - 但是BO算法也有它的缺点，如下：
    - 对于那些具有未知平滑度和有噪声的高维、非凸函数，BO算法往往很难对其进行拟合和优化，而且通常BO算法都有很强的假设条件，而这些条件一般又很难满足。
  - 为了解决上面的缺点，有的BO算法结合了启发式算法(heuristics)，但是这些方法很难做到并行化

# 初始化

- [谷歌工程师：聊一聊深度学习的weight initialization](https://www.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html?ulu-rcmd=1_5021df_hot_3_89ae5af0b19d4401a442e91aabc49f20)
- ![](https://static.leiphone.com/uploads/new/article/740_740/201703/58ca605474065.jpg?imageMogr2/format/jpg/quality/90)


# Normalization

- [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
- 深度神经网络模型训练之难众所周知，其中一个重要的现象就是 `Internal Covariate Shift`. Batch Norm 大法自 2015 年由 Google 提出之后，就成为深度学习必备之神器。自 BN 之后， Layer Norm / Weight Norm / Cosine Norm 等也横空出世。本文从 Normalization 的背景讲起，用一个公式概括 Normalization 的基本思想与通用框架，将各大主流方法一一对号入座进行深入的对比分析，并从参数和数据的伸缩不变性的角度探讨 Normalization 有效的深层原因。

## 1 为什么需要 Normalization

> 深度学习中的 Internal Covariate Shift 问题及其影响

### 1.1  独立同分布与白化

- 机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为莫过于 “**独立同分布**” 了，即 **independent and identically distributed**，简称为 `i.i.d`. 独立同分布并非所有机器学习模型的必然要求
  - 比如 Naive Bayes 模型就建立在特征彼此独立的基础之上
  - 而 Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型）
  - 但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。
- 因此，在把数据喂给机器学习模型之前，“**白化**（whitening）” 是一个重要的数据预处理步骤。白化一般包含两个目的：
  - （1）去除特征之间的相关性 —> **独立**；
  - （2）使得所有特征具有相同的均值和方差 —> **同分布**。
- 白化最典型的方法就是 **PCA**，可以参考阅读 PCA Whitening（http://t.cn/R9xZyMG ）。

### 1.2  深度学习中的 Internal Covariate Shift

- 深度神经网络模型的训练为什么会很困难？
- 其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。
- 为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。
- Google 将这一现象总结为 **Internal Covariate Shift**，简称 **ICS**. （Covariate协变量）什么是 ICS 呢？@[魏秀参](http://t.cn/R8tNCtF)在一个[回答](http://t.cn/RGJST79)中做出了一个很好的解释：

> - 大家都知道在统计机器学习中的一个经典假设是 “**源空间**（source domain）和**目标空间**（target domain）的**数据分布**（distribution）是一致的”。
> - 如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。
> - 而 **covariate shift** 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同
> - 神经网络的各层输出经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但它们 “指示” 的样本标记（label）仍然是不变的，这便符合了 **covariate shift** 的定义。由于是对层间信号的分析，也即是 “**internal**” 的来由。


### 1.3  ICS 会导致什么问题？

简而言之，每个神经元的输入数据不再是 “**独立同分布**”。
- 其一，上层参数需要不断适应新的输入数据分布，降低学习速度。
- 其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
- 其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

## 2 Normalization 的通用框架与基本思想

> 从主流 Normalization 方法中提炼出的抽象框架

- 由于 ICS 问题的存在，x 的分布可能相差很大。要解决独立同分布的问题，“理论正确” 的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。
- 因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的**白化**操作。
- 基本思想是：在将 x  送给神经元之前，先对其做平移和伸缩变换， 将 x 的分布规范化成在固定区间范围的标准分布。
- [通用变化框架](http://5b0988e595225.cdn.sohucs.com/images/20180201/3fe93c7511bf4367b8ee963715c82d29.png)
  - ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/3fe93c7511bf4367b8ee963715c82d29.png)
  - 这个公式中的各个参数。
    - （1）μ 是**平移参数**（shift parameter），σ 是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换：x1 = (x-μ)/σ, 得到的数据符合均值为 0、方差为 1 的标准分布。
    - （2）b 是**再平移参数**（re-shift parameter），g 是**再缩放参数**（re-scale parameter）。将 上一步得到的x1, 进一步变换为 y = g*x1+b ![](https://www.zhihu.com/equation?tex=%5Cbold%7By%7D%3D%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+%2B+%5Cbold%7Bb%7D%5C%5C)最终得到的数据符合均值为 b、方差为g^2的分布
- 第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。
- （1）说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？
  - 答案是——<font color='blue'>为了保证模型的表达能力不因为规范化而下降。</font>
- （2）难道我们底层神经元人民就在做无用功吗？
  - 为了尊重底层神经网络的学习结果，将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 b、方差为g^2）。
  - ①rescale 和 reshift 的参数都是可学习的，这就使得Normalization 层可以学习如何去尊重底层的学习结果。
  - ②保证获得非线性的表达能力. Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。
- （3）变回来再变过去，会不会跟没变一样？
  - 不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中，x 的均值取决于下层神经网络的复杂关联；但在新参数中，y=g*x1+b，仅由 b 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。
- （4）这样的 Normalization 离标准的白化还有多远？
  - 标准白化操作的目的是 “独立同分布”。独立就不说了，暂不考虑。变换为均值为 b、方差为g^2的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。


## 3 主流 Normalization 方法梳理

> 结合上述框架，将 BatchNorm / LayerNorm / WeightNorm / CosineNorm 对号入座，各种方法之间的异同水落石出。BN, LN, WN 和 CN 之间的来龙去脉

- Normalization 的通用公式
  - ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/6a42ac9263cc4a2fbebad9885b0ffc3a.png)
- 对照于这一公式，我们来梳理主流的四种规范化方法。

### 3.1  Batch Normalization —— 纵向规范化

- ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/3fe53653cd1342a6839c4d46c0e33759.jpeg)
- Batch Normalization 于 2015 年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元的均值和方差, 因而称为 Batch Normalization。
- ![](https://www.zhihu.com/equation?tex=%5Cmu_i+%3D+%5Cfrac%7B1%7D%7BM%7D%5Csum%7Bx_i%7D%2C+%5Cquad+%5Csigma_i%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7BM%7D%5Csum%7B%28x_i-%5Cmu_i%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C)
- 其中 M 是 mini-batch 的大小。按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。BN 独立地规范化每一个输入维度xi，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch 的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。
- 因此，BN 比较适用的场景是：**每个 mini-batch 比较大，数据分布比较接近**。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。
- 另外，由于 BN 需要在运行过程中统计每个 mini-batch 的**一阶**统计量和**二阶**统计量，因此**不适用于动态的网络结构 和 RNN 网络**。不过，也有研究者专门提出了适用于 RNN 的 BN 使用方法

### 3.2  Layer Normalization —— 横向规范化

- ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/47ef026cf0ee485fb77d0e9c94543778.jpeg)
- **层规范化**就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。
  - ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/af65614be205475eb3906c6d9fb82b5d.png)
  - 其中 i 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 μ，σ，g，b 均为标量（BN 中是向量），所有输入共享一个规范化变换。
- LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小 mini-batch 场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
- 但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

### 3.3  Weight Normalization —— 参数规范化

- 前面我们讲的模型框架![](https://www.zhihu.com/equation?tex=h%3Df%5Cleft%28%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%2B%5Cbold%7Bb%7D%5Cright%29%5C%5C)中，经过规范化之后的 y 作为输入送到下一个神经元，应用以 w 为参数的fw(-), 函数定义的变换。最普遍的变换是线性变换fw(x)=w*x
- BN 和 LN 均将规范化应用于输入的特征数据 x，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 w，这就是 WN 名称的来源。
  - WN 提出的方案是，将权重向量 w 分解为向量方向 v 和向量模 g 两部分
  - ![](https://www.zhihu.com/equation?tex=%7B%5Cbold%7Bw%7D%7D+%3D+g%5Ccdot%5Chat%7B%5Cbold%7Bv%7D%7D+%3D+g%5Ccdot%5Cfrac%7B%5Cbold%7Bv%7D%7D%7B%5Cbold%7B%7C%7Cv%7C%7C%7D%7D%5C%5C)
- BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。
- 另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令μ=0。但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 μ。
- WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

### 3.4 Cosine Normalization —— 余弦规范化

- Normalization 还能怎么做？
- 我们再来看看神经元的经典变换fw(x)=w*x
  - 对输入数据 x 的变换已经做过了，横着来是 LN，纵着来是 BN。
  - 对模型参数 w 的变换也已经做过了，就是 WN。
  - 好像没啥可做的了。
- 然而天才的研究员们盯上了中间的那个点, 对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——**点积**，权重向量 w 和 x 特征数据向量 的点积。向量点积是无界（unbounded）的啊！
- 有没有其他的相似度衡量方法呢？有，夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围，多么的美好
- 于是，Cosine Normalization 就出世了。他们不处理权重向量  w，也不处理特征数据向量 x ，就改了一下线性变换的函数
- 总结，CN 与 WN 还是很相似的。我们看到上式中，分子还是 w 和 x 的内积，而分母则可以看做用 w 和 x 二者的模之积进行规范化。一定程度上可以理解为，WN 用 权重的模对输入向量进行 scale，而 CN 在此基础上用输入向量的模对输入向量进行了进一步的 scale.
- CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的 scale 信息。去掉 scale 信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。

### 其它

- [超越BN和GN！谷歌提出新的归一化层：FRN](https://zhuanlan.zhihu.com/p/98688349)
- 目前主流的深度学习模型都会采用BN层（Batch Normalization）来加速模型训练以及提升模型效果，对于CNN模型，BN层已经上成为了标配。
- 但是BN层在训练过程中需要在batch上计算中间统计量，这使得BN层严重依赖batch，造成训练和测试的不一致性，当训练batch size较小，往往会恶化性能。
- GN（Group Normalization）通过将特征在channel维度分组来解决这一问题，GN在batch size不同时性能是一致的，但对于大batch size，GN仍然难以匹敌BN。这里介绍谷歌提出的一种新的归一化方法FRN，和GN一样不依赖batch，但是性能却优于BN和GN。
- 以CNN模型为例，中间特征的维度为[B, H, W, C]，BN首先在计算在（N H, W)维度上的均值[公式]和方差[公式]，然后各个通道上（C维度）进行标准归一化。最后对归一化的特征进行放缩和平移（scale and shift），这里[公式]和[公式]是可学习的参数（参数大小为C）。
- BN的一个问题是训练时batch size一般较大，但是测试时batch size一般为1，而均值和方差的计算依赖batch，这将导致训练和测试不一致。
- BN的解决方案
  - （1）在**训练时估计一个均值和方差量**来作为测试时的归一化参数，一般对每次mini-batch的均值和方差进行指数加权平均（滑动平均）来得到这个量。
    - 虽然解决了训练和测试的不一致性，但是BN对于batch size比较敏感，当batch size较小时，模型性能会明显恶化。对于一个比较大的模型，由于显存限制，batch size难以很大，比如目标检测模型，这时候BN层可能会成为一种限制。
    - 当训练集和测试集分布差异大时，训练集上预计算好的数据并不能代表测试数据，这就导致在训练，验证，测试这三个阶段存在inconsistency
  - （2）另外一个方向是**避免在batch维度进行归一化**，这样当然就不会带来训练和测试的不一致性问题。这些方法包括Layer Normalization (LN)， Instance Normalization (IN)以及最新的Group Normalization（GN），这些方法与BN的区别可以从图1中看出来：
    - ![](https://pic1.zhimg.com/80/v2-f7d9521b50950db1594e23ce28f16d50_1440w.jpg)
    - 这些方法处理方式和BN类似，但是归一化的维度不一样，BN是在(N, H, W)维度上，LN是在(H,W,C)维度上，IN是在(H,W)维度上，GN更巧妙，其通过对C分组，此时特征可以从[N, H, W, C]变成[N, H, W, G, C/G]，GN的计算是在[H, W, G]维度上。LN，IN以及GN都没有在B维度上进行归一化，所以不会有BN的问题.GN基本不受batch size的影响，而BN在batch size较小时性能大幅度恶化，但是在较大batch size，BN的效果是稍好于GN的。
  - （3）解决BN在小batch性能较差的另外一个方向是**直接降低训练和测试之间不一致性**
    - 比较常用的方法是**Batch Renormalization** (BR)，它主要的思路是限制训练过程中batch统计量的值范围。
    - 另外的一个解决办法是采用**多卡BN**方法训练，相当于增大batch size。
- FRN
  - 谷歌的提出的FRN层包括**归一化层**FRN（Filter Response Normalization）和**激活层**TLU（Thresholded Linear Unit），如图3所示。FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。
  - ![](https://pic1.zhimg.com/80/v2-68b5a449f6e7b5bc19712f7b12479840_1440w.jpg)


## 4 Normalization 为什么会有效？

> 从参数和数据的伸缩不变性探讨 Normalization 有效的深层原因。

- 以这个简化的神经网络为例
  - ![](http://5b0988e595225.cdn.sohucs.com/images/20180201/d3e7c7a5509541d281614a3943d74ecd.jpeg)

### 4.1 Normalization 的权重伸缩不变性

- 权重伸缩不变性（weight scale invariance）指的是，当权重 W 按照常量 λ 进行伸缩时，得到的规范化后的值保持不变
- 作用
  - 权重伸缩不变性可以有效地提高反向传播的效率
  - 权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率

### 4.2 Normalization 的数据伸缩不变性

- 数据伸缩不变性（data scale invariance）指的是，当数据 x 按照常量 λ 进行伸缩时，得到的规范化后的值保持不变
- 数据伸缩不变性仅对 BN、LN 和 CN 成立。因为这三者对输入数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母互相抵消。而 WN 不具有这一性质。
- 数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。

# 结束


