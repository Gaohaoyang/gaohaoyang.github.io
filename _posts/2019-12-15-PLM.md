---
layout: post
title:  è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ Self-supervised Learning and Pretrain Language Model
date:   2019-12-15 16:52:00
categories: æ·±åº¦å­¦ä¹  è‡ªç„¶è¯­è¨€å¤„ç†
tags: è¡¨ç¤ºå­¦ä¹  transformer bert gpt Attention Faiss Facebook TextCNN ES è‡ªç›‘ç£ Milvus elasticsearch es å¯è§†åŒ– unilm simcse gpu è¿ç§»å­¦ä¹  sentence å¥å‘é‡ å‘é‡åŒ– ä¹”å§†æ–¯åŸº
excerpt: é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çŸ¥è¯†ç‚¹æ±‡æ€»
mathjax: true
permalink: /plm
---

* content
{:toc}

# è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ 

- ã€2020-6-19ã€‘[NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ](https://www.toutiao.com/i6839892851711541764),[è‹±æ–‡åŸæ–‡](https://amitness.com/2020/05/self-supervised-learning-nlp/)  

è™½ç„¶è®¡ç®—æœºè§†è§‰åœ¨è‡ªç›‘ç£å­¦ä¹ æ–¹é¢å–å¾—äº†æƒŠäººçš„è¿›å±•ï¼Œä½†åœ¨å¾ˆé•¿ä¸€æ®µæ—¶é—´å†…ï¼Œ**è‡ªç›‘ç£**å­¦ä¹ ä¸€ç›´æ˜¯NLPç ”ç©¶é¢†åŸŸçš„ä¸€ç­‰å…¬æ°‘ã€‚è¯­è¨€æ¨¡å‹æ—©åœ¨90å¹´ä»£å°±å·²ç»å­˜åœ¨ï¼Œç”šè‡³åœ¨â€œè‡ªæˆ‘ç›‘ç£å­¦ä¹ â€è¿™ä¸ªæœ¯è¯­å‡ºç°ä¹‹å‰ã€‚2013å¹´çš„Word2Vecè®ºæ–‡æ¨å¹¿äº†è¿™ä¸€æ¨¡å¼ï¼Œåœ¨è®¸å¤šé—®é¢˜ä¸Šåº”ç”¨è¿™äº›è‡ªç›‘ç£çš„æ–¹æ³•ï¼Œè¿™ä¸ªé¢†åŸŸå¾—åˆ°äº†è¿…é€Ÿçš„å‘å±•ã€‚

è¿™äº›è‡ªç›‘ç£æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå«åš â€œpretext taskâ€ çš„æ¡†æ¶ï¼Œä½¿ç”¨æ•°æ®æœ¬èº«æ¥ç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨ç›‘ç£çš„æ–¹æ³•æ¥è§£å†³éç›‘ç£çš„é—®é¢˜ã€‚è¿™äº›ä¹Ÿè¢«ç§°ä¸ºâ€œauxiliary taskâ€ï¼ˆ**è¾…åŠ©**ä»»åŠ¡ï¼‰æˆ–â€œpre-training taskâ€œï¼ˆ**é¢„è®­ç»ƒ**ä»»åŠ¡ï¼‰ã€‚é€šè¿‡æ‰§è¡Œæ­¤ä»»åŠ¡è·å¾—çš„è¡¨ç¤ºå¯ä»¥ç”¨ä½œä¸‹æ¸¸ç›‘ç£ä»»åŠ¡çš„èµ·ç‚¹ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/47ba10919c1440c781c0b14f1c14de82?from=pc)

ä¸‹é¢æ¦‚è¿°ç ”ç©¶äººå‘˜åœ¨æ²¡æœ‰æ˜ç¡®çš„æ•°æ®æ ‡æ³¨çš„æƒ…å†µä¸‹ä»æ–‡æœ¬è¯­æ–™åº“ä¸­å­¦ä¹ è¡¨ç¤ºçš„å„ç§pretext tasksã€‚
- é‡ç‚¹æ˜¯ä»»åŠ¡çš„åˆ¶å®šï¼Œè€Œä¸æ˜¯å®ç°å®ƒä»¬çš„æ¶æ„ã€‚

## è¯­è¨€æ¨¡å‹æ¼”å˜

é•¿æœŸä»¥æ¥ï¼Œäººç±»ä¸€ç›´æ¢¦æƒ³ç€**è®©æœºå™¨æ›¿ä»£äºº**æ¥å®Œæˆå„ç§å·¥ä½œï¼ŒåŒ…æ‹¬è¯­è¨€ç›¸å…³å·¥ä½œï¼Œå¦‚ç¿»è¯‘æ–‡å­—ï¼Œè¯†åˆ«è¯­è¨€ï¼Œæ£€ç´¢ã€ç”Ÿæˆæ–‡å­—ç­‰ã€‚å®Œæˆè¿™äº›ç›®æ ‡çš„å‰ææ˜¯æœºå™¨èƒ½ç†è§£è¯­è¨€ã€‚

### è¯­æ³•è§„åˆ™

äººä»¬æœ€æ—©æƒ³åˆ°çš„åŠæ³•ï¼šè®©æœºå™¨æ¨¡æ‹Ÿäººç±»è¿›è¡Œå­¦ä¹ ï¼Œå¦‚å­¦ä¹ äººç±»é€šè¿‡å­¦ä¹ è¯­æ³•è§„åˆ™ã€è¯æ€§ã€æ„è¯æ³•ã€åˆ†æè¯­å¥ç­‰å­¦ä¹ è¯­è¨€ã€‚å°¤å…¶æ˜¯åœ¨`ä¹”å§†æ–¯åŸº`ï¼ˆNoam Chomsky æœ‰å²ä»¥æ¥æœ€ä¼Ÿå¤§çš„è¯­è¨€å­¦å®¶ï¼‰æå‡º â€œ`å½¢å¼è¯­è¨€`â€ ä»¥åï¼Œäººä»¬æ›´åšå®šäº†åˆ©ç”¨**è¯­æ³•è§„åˆ™**çš„åŠæ³•è¿›è¡Œæ–‡å­—å¤„ç†çš„ä¿¡å¿µã€‚
- é—æ†¾çš„æ˜¯ï¼Œå‡ åå¹´è¿‡å»äº†ï¼Œåœ¨è®¡ç®—æœºå¤„ç†è¯­è¨€é¢†åŸŸï¼ŒåŸºäºè¿™ä¸ªè¯­æ³•è§„åˆ™çš„æ–¹æ³•å‡ ä¹æ¯«æ— çªç ´ã€‚

### ç»Ÿè®¡è¯­è¨€æ¨¡å‹

`é¦™å†œ`å¾ˆæ—©å°±æå‡ºäº†ç”¨**æ•°å­¦æ–¹æ³•**æ¥å¤„ç†è‡ªç„¶è¯­è¨€ã€‚ä½†æ˜¯å½“æ—¶ç”¨è®¡ç®—æœºæŠ€æœ¯æ— æ³•è¿›è¡Œå¤§é‡çš„ä¿¡æ¯å¤„ç†ã€‚
- ä¸è¿‡éšç€è®¡ç®—æœºæŠ€æœ¯çš„å‘å±•ï¼Œè¿™ä¸ªæ€è·¯æˆäº†ä¸€ç§å¯èƒ½ã€‚
- é¦–å…ˆæˆåŠŸåˆ©ç”¨æ•°å­¦æ–¹æ³•è§£å†³è‡ªç„¶è¯­è¨€é—®é¢˜çš„æ˜¯`è´¾é‡Œå°¼å…‹` (Fred Jelinek) åŠä»–é¢†å¯¼çš„IBM Wasonå®éªŒå®¤ã€‚`è´¾é‡Œå°¼å…‹`æå‡ºçš„æ–¹æ³•ä¹Ÿååˆ†ç®€å•ï¼šåˆ¤æ–­ä¸€ä¸ªè¯åºåˆ—ï¼ˆçŸ­è¯­ï¼Œå¥å­ï¼Œæ–‡æ¡£ç­‰ï¼‰æ˜¯å¦åˆç†ï¼Œå°±çœ‹å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚
- ä¸¾ä¸ªä¾‹å­ï¼šåˆ¤æ–­ â€œI have a penâ€ ç¿»è¯‘ä¸ºä¸­æ–‡ â€æˆ‘æœ‰ä¸ªç¬”â€œæ˜¯å¦åˆç†ï¼Œåªéœ€è¦åˆ¤æ–­â€I have a pen.æˆ‘æœ‰ä¸ªç¬”â€ è¿™ä¸ªåºåˆ—çš„å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚
- å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªè¯åºåˆ—çš„å¯èƒ½æ€§ï¼Œå°±éœ€è¦å¯¹è¿™ä¸ªè¯åºåˆ—çš„æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ï¼Œä¹Ÿå°±æ˜¯`ç»Ÿè®¡è¯­è¨€æ¨¡å‹`

å¦‚æœè¯ç”±å…¶å‰é¢çš„Nâˆ’1ä¸ªè¯å†³å®šï¼Œåˆ™å¯¹åº”çš„æ˜¯Nå…ƒæ¨¡å‹ï¼ˆ`n-gram`ï¼‰, n=2,3,4, (4ä»¥ä¸Šç»„åˆçˆ†ç‚¸ğŸ’¥ï¼Œéš¾ä»¥è®¡ç®—)

- ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼šåŸºäºVSMçš„è¯­è¨€æ¨¡å‹ï¼Œé™¤äº†N-Gramï¼Œé™„ä¸Šå…¸å‹çš„LDAã€pLSA

### ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹

ç»Ÿè®¡è¯­è¨€æ¨¡å‹æœ‰å¾ˆå¤šé—®é¢˜ï¼š
1. è®­ç»ƒè¯­æ–™ä¸­æœªå‡ºç°è¿‡çš„è¯ï¼ˆå¥å­ï¼‰å¦‚ä½•å¤„ç†(OOV);
2. é•¿å°¾ä½é¢‘è¯å¦‚ä½•å¹³æ»‘ï¼›
3. one-hot å‘é‡å¸¦æ¥çš„ç»´åº¦ç¾éš¾ï¼›
4. æœªè€ƒè™‘è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ç­‰ã€‚

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œ`Yoshua Bengio`(æ·±åº¦å­¦ä¹ ä¸‰å·¨å¤´ä¹‹ä¸€ï¼‰åœ¨2003å¹´æå‡ºç”¨**ç¥ç»ç½‘ç»œ**æ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶å­¦ä¹ è¯çš„**ä½ç»´åº¦**çš„åˆ†å¸ƒå¼è¡¨å¾(distributed representation),å…·ä½“çš„ï¼š
1. ä¸ç›´æ¥å¯¹ $P(w^n_1)$ å»ºæ¨¡ï¼Œè€Œæ˜¯å¯¹ $P(w_i\|w^{iâˆ’1}_1)$ è¿›è¡Œå»ºæ¨¡;
2. ç®€åŒ–æ±‚è§£æ—¶ï¼Œä¸é™åˆ¶åªèƒ½æ˜¯å·¦è¾¹çš„è¯ï¼Œä¹Ÿå¯ä»¥å«å³è¾¹çš„è¯ï¼Œå³å¯ä»¥æ˜¯ä¸€ä¸ª**ä¸Šä¸‹æ–‡çª—å£**(context) å†…çš„æ‰€æœ‰è¯ï¼›
3. å…±äº«ç½‘ç»œå‚æ•°ã€‚

ç”±äºå½“æ—¶çš„è®¡ç®—æœºæŠ€æœ¯çš„é™åˆ¶ï¼Œ`ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹`çš„æ¦‚ç‡ç»“æœå¾€å¾€éƒ½ä¸å¥½ï¼ˆä¸€å±‚MLPæ•ˆæœè‚¯å®šå¥½ä¸äº†ï¼‰ï¼Œæ‰€ä»¥å½“æ—¶ä¸»è¦è¿˜æ˜¯ç”¨è¿™ä¸ªå½¢å¼è®­ç»ƒè¯å‘é‡ã€‚
- 2013å¹´ï¼Œword2vecè¯ç”Ÿ

ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼š
- Bengio åœ¨ 2003 å¹´æå‡ºçš„ç¥ç»æ¦‚ç‡è¯­è¨€æ¨¡å‹(Neural Probabilistic Language Model,Â NPLM)ï¼Œå…‹æœäº†n-gramæ¨¡å‹é‡Œè¯å…¸å¼€é”€å¤§ã€éœ€è¦åšå¹³æ»‘çš„ç¼ºç‚¹ï¼Œå¹¶æå‡ºç”¨ RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰åšè¯­è¨€æ¨¡å‹
- VSMå‡çº§ä¸ºè¯è¢‹æ¨¡å‹ï¼ˆCBoWåŠSGï¼‰
- 2010å¹´ï¼ŒTomas Mikolov æå‡ºåŸºäº RNN çš„è¯­è¨€æ¨¡å‹(Recurrent Neural Network Language Model,Â RNNLM)ï¼ŒRNNLM æ ¹æœ¬æ€æƒ³å’Œ NPLM å…±é€šï¼Œä¸åŒç‚¹æ˜¯è¯é€ä¸ªè¾“å…¥åˆ°æ¨¡å‹ï¼Œéšè—å±‚æœ‰ä¸ªã€Œè®°å¿†åŠŸèƒ½ã€å•å…ƒã€‚
- 2013å¹´ï¼ŒTomas Mikolov å†æ¥å†å‰ï¼Œåˆ©ç”¨å·¥ç¨‹ç»éªŒï¼Œå¼•å…¥å±‚æ¬¡softmaxå’Œè´Ÿé‡‡æ ·ï¼Œè§£å†³äº†RNNLMè®­ç»ƒé—®é¢˜ï¼Œç»ˆäºè¯ç”Ÿäº†ä¸€ä¸ªçœŸæ­£æ„ä¹‰ä¸Šçš„ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ Word2vecï¼Œä¸»å®°äº†NLPé¢†åŸŸé¢„è¨€æ¨¡å‹5å¹´å¤šï¼Œç›´è‡³2018å¹´BERTè¯ç”Ÿã€‚

### GPT

éšç€æ•°æ®ã€ç®—åŠ›ã€æ¨¡å‹æ¶æ„ã€èŒƒå¼ç­‰çš„å‡çº§ï¼Œ`ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹`ä¹Ÿå¾—åˆ°äº†é•¿è¶³çš„å‘å±•ã€‚
- å¦‚æ¨¡å‹æ¶æ„ä» mlp åˆ° cnn/rnn åˆåˆ°ç›®å‰çš„ transformer-base ï¼Œå¯¹åº”çš„èƒ½åŠ›ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼Œä»ä¹‹å‰åªå¯¹$P(w_i\|w_{iâˆ’1})$
 å»ºæ¨¡ï¼Œé€šè¿‡â€â€œå¹¶è¡Œâ€æˆ–â€œä¸²è¡Œâ€ çš„æ–¹å¼ï¼Œä¹Ÿå¯ä»¥å¯¹ $ P(w^n_i) $
å»ºæ¨¡ã€‚
- æ±‚è§£NLP task ä»åŸæ¥çš„ word2vector + ML å‘å±•ä¸º pretrain + fine-tuningã€‚
- ç›®å‰æœ€æœ‰ä»£è¡¨æ€§çš„å°±æ˜¯`BERT`å’Œ`GPT`(1-2)ã€‚

éšç€NLPè¿›å…¥`BERT`æ—¶ä»£åï¼Œpretrain + fine tune è¿™ç§æ–¹å¼å¯ä»¥è§£å†³å¤§é‡çš„NLP ä»»åŠ¡ï¼Œä½†æ˜¯ä»–ä¾ç„¶æœ‰å¾ˆå¤šé™åˆ¶ï¼š
1. æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œè¿™å¤§å¤§é™åˆ¶äº†æ¨¡å‹çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰å¤§é‡ä¸å¥½æ”¶é›†æ ‡æ³¨æ•°æ®çš„ä»»åŠ¡å­˜åœ¨ï¼›
2. è™½ç„¶pretrain é˜¶æ®µæ¨¡å‹å¸æ”¶äº†å¤§é‡çŸ¥è¯†ï¼Œä½†æ˜¯fine-tuned åæ¨¡å‹åˆè¢«â€œç¼©â€åˆ°ä¸€ä¸ªå¾ˆçª„çš„ä»»åŠ¡ç›¸å…³çš„åˆ†å¸ƒä¸Šï¼Œè¿™ä¹Ÿå¯¼è‡´äº†ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚åœ¨OODï¼ˆout-of-distribution) ä¸Šè¡¨ç°ä¸å¥½ï¼›
3. å¦‚æœå‚è€ƒäººç±»çš„è¯ï¼Œäººç±»é€šå¸¸ä¸éœ€è¦åœ¨å¤§é‡çš„æ ‡æ³¨æ•°æ®ä¸Šå­¦ä¹ åæ‰èƒ½åšä»»åŠ¡ï¼Œè€Œåªéœ€è¦ä½ æ˜ç¡®å‘ŠçŸ¥ä½ æƒ³è®©ä»–å¹²å˜›ï¼ˆæ¯”å¦‚ï¼šå°†æ‰€ç»™å•è¯ç¿»è¯‘ä¸ºè‹±è¯­ï¼šçº¢è‰²->ï¼‰æˆ–è€…ç»™ä»–å‡ ä¸ªä¾‹å­(æ¯”å¦‚ï¼šè“è‰²->blue,ç»¿è‰²->green,çº¢è‰²->)ï¼Œä¹‹åä¾¿èƒ½å¤„ç†æ–°çš„ä»»åŠ¡äº†ã€‚

### GPT 3

ç»ˆæç›®æ ‡æ˜¯å¸Œæœ›æ¨¡å‹èƒ½åƒäººä¸€æ ·ï¼Œçµæ´»çš„å­¦ä¹ å¦‚ä½•å®Œæˆå·¥ä½œã€‚ä¸€ä¸ªå¯èƒ½çš„æ–¹å‘å°±æ˜¯`å…ƒå­¦ä¹ `ï¼ˆmeta-learning)ï¼š**å­¦ä¹ å¦‚ä½•å­¦ä¹ **ã€‚
- è€Œåœ¨LMè¯­å¢ƒä¸‹ï¼Œå¸Œæœ›LM åœ¨è®­ç»ƒçš„æ—¶å€™èƒ½è·å¾—å¤§é‡çš„**æŠ€èƒ½**å’Œ**æ¨¡å¼è¯†åˆ«**çš„èƒ½åŠ›ï¼Œè€Œåœ¨é¢„æµ‹æ—¶èƒ½å¿«é€Ÿå°†æŠ€èƒ½è¿ç§»åˆ°**æ–°ä»»åŠ¡**æˆ–è€…è¯†åˆ«å‡ºæ–°ä»»åŠ¡ã€‚
- ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªæ˜¾ç°å‡ºä¸€å®šæœ‰æ•ˆæ€§çš„æ–¹æ³•å°±æ˜¯â€**in-context learning**â€: ç”¨`æŒ‡ä»¤`(instruction)æˆ–è€…**å°‘é‡ç¤ºä¾‹**(demonstrations)ç»„æˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¾“å…¥ï¼ŒæœŸæœ›æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯ä»¥å®Œæˆå¯¹åº”çš„ä»»åŠ¡ã€‚
- æ ¹æ®æä¾›å¤šå°‘ä¸ªç¤ºä¾‹ï¼Œåˆå¯ä»¥åˆ†ä¸º`zero-shot`, `one-shot`, `few-shot`ã€‚
- ![img1](https://xv44586.github.io/2023/01/09/zero-to-chatgpt/few-shot.png)
- ![img2](https://xv44586.github.io/2023/01/09/zero-to-chatgpt/in-context.png)

è™½ç„¶ in-context learning è¢«è¯æ˜å…·æœ‰ä¸€å®šçš„æœ‰æ•ˆæ€§ï¼Œä½†æ˜¯å…¶ç»“æœç›¸æ¯” fine-tuing è¿˜æœ‰ä¸€å®šçš„è·ç¦»ã€‚è€Œéšç€`é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹`(PTM)è§„æ¨¡çš„æ‰©å¤§(scaling up), å¯¹åº”çš„åœ¨ä¸‹æ¸¸task ä¸Šçš„è¡¨ç°ä¹Ÿåœ¨é€æ­¥ä¸Šå‡ï¼Œæ‰€ä»¥OpenAIå°±çŒœæƒ³ï¼š
- PTMçš„è¿›ä¸€æ­¥scaling up, å¯¹åº”çš„ in-context learning çš„èƒ½åŠ›æ˜¯ä¸æ˜¯ä¹Ÿä¼šè¿›ä¸€æ­¥æå‡ï¼Ÿ
- äºæ˜¯ä»–ä»¬åšäº† GPT-3 ç³»åˆ—æ¨¡å‹ï¼Œæœ€å¤§çš„ä¸º GPT-3 175Bã€‚

æœ€ç»ˆæ¨¡å‹æ•ˆæœï¼š
- ä¸€äº›ä»»åŠ¡ä¸Šfew-shot (zero-shot) èƒ½èµ¶ä¸Šç”šè‡³è¶…è¿‡ä¹‹å‰ fine-tuned SOTA(å¦‚ï¼šPIQA)
- æœ‰äº›ä»»åŠ¡ä¸Šè¿˜è¾¾ä¸åˆ°ä¹‹å‰çš„SOTA(å¦‚ï¼šOpenBookQA)ï¼›
- èƒ½åšä¸€äº›æ–°taskï¼Œå¦‚3ä½æ•°ç®—æ•°ã€‚

ä¸è¿‡ä»–ä»¬ä¹Ÿå‘ç°äº†æ¨¡å‹å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€äº›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆæ‰€ä»¥OpenAI åœ¨2020 å¹´å°±å®šä¸‹äº†æœªæ¥çš„æ–¹å‘ï¼ŒæŒç»­æŠ•å…¥è‡³ä»Šï¼‰


### Prompt engineering

zero-shot/few-shot è¿™ç§è®¾å®šç¡®å®ç»™ NLP ç¤¾åŒºå¸¦æ¥äº†æ–°æ€è·¯ï¼Œä½†æ˜¯175B çš„æ¨¡å‹å®åœ¨æ˜¯å¤ªå¤§äº†ï¼Œå³ä¸å¥½è®­ç»ƒåˆä¸å¥½å¾®è°ƒä¹Ÿä¸å¥½éƒ¨ç½²ä¸Šçº¿ï¼Œå¦‚ä½•åœ¨å°æ¨¡å‹ä¸Šåº”ç”¨å‘¢ï¼Ÿæ­¤å¤–ï¼Œä¸åŒçš„ pattern(prompt)ä¸‹åŒä¸€ä¸ªtask çš„æ•ˆæœå·®è·ä¹Ÿéå¸¸å¤§ï¼Œå¦‚ä½•æ‰¾åˆ°æ•ˆæœæœ€å¥½çš„prompt å‘¢ï¼Ÿ

äºæ˜¯å¤§å®¶å°±å¼€å§‹èŠ±å¼æ¢ç´¢ `prompt`, NLPer ä¹Ÿå˜æˆäº† prompt-engineer (è¯¯). PSï¼šprompt çš„è¯­ä¹‰ç›®å‰å³å¯ä»¥æŒ‡æ¨¡å‹çš„è¾“å…¥ï¼Œä¹Ÿå¯ä»¥æŒ‡è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚
- ![prompt](https://xv44586.github.io/2023/01/09/zero-to-chatgpt/prompt.jpeg)

#### PET

`PET`(Pattern-Exploiting Training) åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªï¼ˆè‡³å°‘æ˜¯æˆ‘çŸ¥é“çš„ï¼‰åœ¨å°æ¨¡å‹ä¸Šåœ¨few-shot è®¾å®šä¸‹æˆåŠŸåº”ç”¨çš„å·¥ä½œã€‚

`PET` çš„ä¸»è¦æ€è·¯ï¼š
- ç”¨é€šé¡ºè¯­è¨€ä¸ºtask æ„é€ ä¸€ä¸ª **pattern**(prompt), å¦‚: â€œä¸‹é¢æ˜¯{label}æ–°é—»ã€‚{x}â€;
- å°† label æ˜ å°„ä¸ºæ–‡å­—ã€‚å¦‚: â€œ0->ä½“è‚² ï¼Œ1-> è´¢ç», 2->ç§‘æŠ€â€;
- å°†æ ·æœ¬æŒ‰ç…§ pattern é‡æ„ï¼Œå†»ç»“æ¨¡å‹ä¸»ä½“ï¼Œåªæ›´æ–° label å¯¹åº”çš„ tokenï¼ˆembedding),ç»§ç»­ LM (MLM) è®­ç»ƒï¼›
- é¢„æµ‹æ—¶ï¼Œå°† label å¯¹åº”ä½ç½®çš„ token å†æ˜ å°„å›labelã€‚

![PET](https://xv44586.github.io/2023/01/09/zero-to-chatgpt/bert-pet.jpeg)

PET åœ¨few-shot çš„è®¾å®šä¸‹ï¼Œåˆ©ç”¨ BERT-base å°±èƒ½è·å¾—æ¯” GPT-3 175B æ›´å¥½çš„ç»“æœã€‚ä¸è¿‡pattern æ˜¯éœ€è¦äººæ¥æ„é€ çš„ï¼Œpattern çš„â€œå¥½åâ€ ç›´æ¥å½±å“æœ€ç»ˆçš„æ•ˆæœã€‚

æ€è€ƒï¼š
- PETä¸­çš„fine-tuning æ˜¯ä¸å…¶ pretrain çš„å½¢å¼æ˜¯ä¸€è‡´çš„ï¼Œè€Œ pretrain ä¸ fine-tuning å½¢å¼ä¸€è‡´èƒ½å¤Ÿwork æ‰æ˜¯ä¸€ç§â€œè‡ªç„¶â€çš„äº‹æƒ…ï¼Œpretrain + fine-tuning è¿™ç§ä¸‹æ¸¸ä»»åŠ¡ä¸é¢„è®­ç»ƒå½¢å¼ä¸ä¸€è‡´èƒ½work å…¶å®ä¸æ˜¯ä¸€ä¸ªè‡ªç„¶çš„äº‹æƒ…ï¼Œä¸ºä»€ä¹ˆpretrain + fine-tuning èƒ½work å€¼å¾—æ€è€ƒã€‚

Automated Discrete Prompt
äººæ¥å†™promptè¿˜æ˜¯éœ€è¦å¤§é‡çš„æ—¶é—´å’Œç»éªŒï¼Œè€Œä¸”ï¼Œå³ä½¿ä¸€ä¸ªç»éªŒä¸°å¯Œçš„äººï¼Œå†™å‡ºçš„prompt ä¹Ÿå¯èƒ½æ˜¯æ¬¡ä¼˜çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä¸€ç§åŠæ³•å°±æ˜¯â€œè‡ªåŠ¨â€çš„å¸®åŠ©æˆ‘ä»¬å¯»æ‰¾æœ€ä¼˜çš„promptã€‚

Prompt Mining: è¯¥æ–¹æ³•æ˜¯åœ¨è¯­æ–™ä¸Šç»Ÿè®¡è¾“å…¥X ä¸è¾“å‡ºY ä¹‹é—´çš„ä¸­é—´è¯æˆ–è€…ä¾èµ–è·¯å¾„ï¼Œé€‰å–æœ€é¢‘ç¹çš„ä½œä¸ºprompt,å³ï¼š{X} {middle words} {Y}.
Prompt Paraphrasing: è¯¥æ–¹æ³•æ˜¯åŸºäºè¯­ä¹‰çš„ï¼Œé¦–å…ˆæ„é€ ç§å­promptsï¼Œç„¶åå°†å…¶è½¬è¿°æˆè¯­ä¹‰ç›¸è¿‘çš„è¡¨è¾¾ä½œä¸ºå€™é€‰promptsï¼Œé€šè¿‡åœ¨ä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯ï¼Œæœ€ç»ˆé€‰æ‹©æ•ˆæœæœ€ä¼˜çš„ã€‚
Gradient-based Search: é€šè¿‡æ¢¯åº¦ä¸‹é™æœç´¢çš„æ–¹å¼æ¥å¯»æ‰¾ã€ç»„åˆè¯æ„æˆæœ€ä¼˜promptã€‚
Prompt Generation: ç”¨NLG çš„æ–¹å¼ï¼Œç›´æ¥ç”Ÿæˆæ¨¡å‹çš„promptsã€‚
Prompt Scoring: æ„é€ æ¨¡å‹å¯¹ä¸åŒçš„prompt è¿›è¡Œæ‰“åˆ†ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„prompt ä½œä¸ºæœ€ä¼˜promptã€‚
Automated Continuous Prompt
è™½ç„¶PETæœ€åˆåœ¨æ„é€ prompt æ—¶è®¤ä¸ºpromptéœ€è¦æ˜¯é€šé¡ºæµç•…çš„è‡ªç„¶è¯­è¨€ã€‚è€Œéšç€å„ç§è‡ªåŠ¨åŒ–æ–¹æ³•æ„é€ å‡ºäº†å¾ˆå¤šè™½ç„¶å¥å­ä¸é€šé¡ºä½†æ˜¯æ•ˆæœæ›´å¥½çš„promptï¼Œå¤§å®¶ä¹Ÿå‘ç°ï¼šé€šé¡ºæµç•…çš„è‡ªç„¶è¯­è¨€æˆ–è€…æ˜¯è‡ªç„¶è¯­è¨€çš„è¦æ±‚åªæ˜¯ä¸ºäº†æ›´å¥½çš„å®ç°é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡çš„â€œä¸€è‡´æ€§â€ï¼Œä½†æ˜¯è¿™å¹¶ä¸æ˜¯å¿…é¡»çš„ï¼Œæˆ‘ä»¬å…¶å®å¹¶ä¸å…³å¿ƒè¿™ä¸ªpattern å…·ä½“é•¿ä»€ä¹ˆæ ·ï¼Œæˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æ˜¯ä»–æœ‰å“ªäº›token ç»„æˆï¼Œéƒ½æ’å…¥åœ¨ä»€ä¹ˆä½ç½®ï¼Œè¾“å‡ºç©ºé—´æ˜¯ä»€ä¹ˆï¼Œä»¥åŠæœ€é‡è¦çš„åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æœ‰å¤šå¥½ã€‚

## è‡ªç›‘ç£çš„æ–¹æ¡ˆæ€»ç»“

- ã€2020-6-21ã€‘[NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://www.toutiao.com/i6839892851711541764/)ï¼Œ[è‹±æ–‡åŸæ–‡](https://amitness.com/2020/05/self-supervised-learning-nlp/)
- è‡ªç›‘ç£çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå«åš â€œpretext taskâ€ çš„æ¡†æ¶ï¼Œå®ƒå…è®¸æˆ‘ä»¬ä½¿ç”¨æ•°æ®æœ¬èº«æ¥ç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨ç›‘ç£çš„æ–¹æ³•æ¥è§£å†³éç›‘ç£çš„é—®é¢˜ã€‚è¿™äº›ä¹Ÿè¢«ç§°ä¸ºâ€œauxiliary taskâ€æˆ–â€œpre-training taskâ€œã€‚é€šè¿‡æ‰§è¡Œæ­¤ä»»åŠ¡è·å¾—çš„è¡¨ç¤ºå¯ä»¥ç”¨ä½œæˆ‘ä»¬çš„ä¸‹æ¸¸ç›‘ç£ä»»åŠ¡çš„èµ·ç‚¹ã€‚
   - ![](http://p3.pstatp.com/large/pgc-image/47ba10919c1440c781c0b14f1c14de82)
- ã€2020-7-12ã€‘åˆ˜çŸ¥è¿œæ–°ä¹¦ï¼š[Representation Learning for Natural Language Processing](http://nlp.csai.tsinghua.edu.cn/news/%E4%B8%93%E8%91%97representation-learning-for-natural-language-processing%E6%AD%A3%E5%BC%8F%E5%87%BA%E7%89%88/)ï¼Œ[ç”µå­ç‰ˆä¸‹è½½](https://link.springer.com/book/10.1007%2F978-981-15-5573-2)

- ã€2020-7-21ã€‘è‡ªç›‘ç£å­¦ä¹ ç»¼è¿°ï¼ˆæ¸…åå”æ°å›¢é˜Ÿï¼‰ï¼šSelf-supervised Learning: Generative or Contrastive
<iframe src="https://view.officeapps.live.com/op/embed.aspx?src=https%3A%2F%2Fstatic%2Eaminer%2Ecn%3A443%2Fupload%2Fppt%2F332%2F651%2F1679%2F5ee8986f91e011e66831c59b%5F1%2Epptx&amp;wdAr=1.4440104166666667" width="700px" height="500px" frameborder="0">è¿™æ˜¯åµŒå…¥ <a target="_blank" href="https://office.com">Microsoft Office</a> æ¼”ç¤ºæ–‡ç¨¿ï¼Œç”± <a target="_blank" href="https://office.com/webapps">Office</a> æä¾›æ”¯æŒã€‚</iframe>


### NLPèŒƒå¼

NLPé¢†åŸŸæŠ€æœ¯å‘å±•çš„å››ä¸ªâ€œèŒƒå¼â€ã€‚
- **éç¥ç»ç½‘ç»œ**æ—¶ä»£çš„`å®Œå…¨ç›‘ç£å­¦ä¹ ` ï¼ˆFully Supervised Learning, Non-Neural Networkï¼‰ï¼Œæ­¤æ—¶ç ”ç©¶é‡ç‚¹å…³æ³¨äº`ç‰¹å¾å·¥ç¨‹`ï¼ˆfeature engineeringï¼‰ã€‚
- åŸºäº**ç¥ç»ç½‘ç»œ**çš„`å®Œå…¨ç›‘ç£å­¦ä¹ ` (Fully Supervised Learning, Neural Network)ï¼Œæ­¤æ—¶ç”±äºç¥ç»ç½‘ç»œçš„å¼•å…¥ä½¿å¾—ç‰¹å¾é€‰å–å˜ä¸ºäº†é»‘ç›’ï¼Œç ”ç©¶é‡ç‚¹å…³æ³¨äºæ¶æ„å·¥ç¨‹ï¼ˆarchitecture engineeringï¼‰ã€‚
- `é¢„è®­ç»ƒ-ç²¾è°ƒ`èŒƒå¼ (Pre-train, Fine-tune)ã€‚NLPé¢†åŸŸçš„ç¬¬ä¸€æ¬¡é‡å¤§å˜åŒ–ã€‚å·¥ä½œé‡å¿ƒåˆè½¬ç§»åˆ°äº†**ç›®æ ‡å‡½æ•°å·¥ç¨‹**ï¼ˆobjective engineeringï¼‰ä¸Šæ¥ã€‚
- `é¢„è®­ç»ƒ-æç¤º-é¢„æµ‹`èŒƒå¼ï¼ˆPre-train, Prompt, Predictï¼‰ã€‚NLPé¢†åŸŸçš„ç¬¬äºŒæ¬¡é‡å¤§å˜åŒ–ã€‚ä½†**å¦‚ä½•é€‰å–è®¾è®¡åˆé€‚çš„prompt**åˆ™æˆäº†ç ”ç©¶çƒ­ç‚¹ï¼Œå³å½“å‰çš„å·¥ä½œé‡å¿ƒåˆè½¬ç§»åˆ°äº†prompt engineeringä¸Šã€‚
- ![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/499bb3f2c8394589870317e50e803d3e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image?)


ä½œè€…ï¼š[QåŒå­¦](https://juejin.cn/post/7091824751874113550)

## è‡ªç›‘ç£ä»»åŠ¡

æ€»ç»“
- 1. é¢„æµ‹**ä¸­å¿ƒè¯**
- 2. é¢„æµ‹**é‚»å±…è¯**
- 3. **ç›¸é‚»å¥å­**çš„é¢„æµ‹
- 4. **è‡ªå›å½’**è¯­è¨€å»ºæ¨¡
- 6. **ä¸‹ä¸€ä¸ªå¥å­**é¢„æµ‹
- 7. **å¥å­é¡ºåº**é¢„æµ‹
- 8. **å¥å­é‡æ’**
- 9. **æ–‡æ¡£æ—‹è½¬**
- 10. è¡¨æƒ…ç¬¦å·é¢„æµ‹

<div class="mermaid">
    flowchart LR
    %% èŠ‚ç‚¹é¢œè‰²
    classDef red fill:#f02;
    classDef green fill:#5CF77B;
    classDef blue fill:#6BE0F7;
    classDef orange fill:#F7CF6B;
    classDef grass fill:#C8D64B;
    %%èŠ‚ç‚¹å…³ç³»å®šä¹‰
    O(è‡ªç›‘ç£å­¦ä¹ )-->|é¢„æµ‹ä¸­å¿ƒè¯|A(ä¸­å¿ƒè¯,CBOW):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|é¢„æµ‹é‚»å±…è¯|B(é‚»å±…è¯,skipgram):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|é¢„æµ‹é‚»å±…å¥å­|C(é‚»å±…å¥å­,skip thought vector):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|è‡ªå›å½’è¯­è¨€å»ºæ¨¡|D(è‡ªå›å½’æ¨¡å‹,N-Gram,GPT):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|æ©ç è¯­è¨€å»ºæ¨¡|E(æ©ç æ¨¡å‹,BERT):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|é¢„æµ‹ä¸‹ä¸€å¥|F(ä¸‹ä¸€å¥,nsp):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|é¢„æµ‹å¥å­é¡ºåº|G(å¥å­é¡ºåº,ALBert):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|å¥å­é¡ºåºæ‰“ä¹±|H(å¥å­é‡æ’,BART):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|å¥å­é¡ºåºæ—‹è½¬|I(å¥å­æ—‹è½¬,BART):::grass
    O(è‡ªç›‘ç£å­¦ä¹ )-->|è¡¨æƒ…ç¬¦å·|J(è¡¨æƒ…ç¬¦å·,DeepMoji):::grass
</div>

## 1. é¢„æµ‹ä¸­å¿ƒè¯ï¼ˆword2vecçš„CBOWæ¨¡å‹ï¼‰
 
åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå–ä¸€å®šçª—å£å¤§å°çš„ä¸€å°å—æ–‡æœ¬ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ ¹æ®å‘¨å›´çš„å•è¯é¢„æµ‹ä¸­å¿ƒå•è¯ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p3-tt.byteimg.com/origin/pgc-image/b217d92b952d4601937a1629bc867642?from=pc)
  - ä¾‹å¦‚ï¼Œä¸‹å›¾ä¸­ï¼Œæœ‰ä¸€ä¸ªå¤§å°ä¸º1çš„çª—å£ï¼Œåœ¨ä¸­é—´å•è¯çš„ä¸¤è¾¹å„æœ‰ä¸€ä¸ªå•è¯ï¼Œç”¨è¿™äº›ç›¸é‚»çš„è¯é¢„æµ‹ä¸­å¿ƒè¯ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p3-tt.byteimg.com/origin/pgc-image/62516862aa444fed9885c34aaf955b05?from=pc)

è¿™ä¸ªæ–¹æ¡ˆå·²ç»åœ¨è‘—åçš„Word2Vecè®ºæ–‡çš„â€œContinuous Bag of Wordsâ€æ–¹æ³•ä¸­ä½¿ç”¨è¿‡ã€‚
 
## 2. é¢„æµ‹é‚»å±…è¯ï¼ˆword2vecçš„skip-gramæ¨¡å‹ï¼‰

åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå–ä¸€å®šçª—å£å¤§å°çš„æ–‡æœ¬å¼ æˆçš„ç©ºé—´ï¼Œç›®æ ‡æ˜¯åœ¨ç»™å®šä¸­å¿ƒè¯çš„æƒ…å†µä¸‹é¢„æµ‹å‘¨å›´çš„è¯ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p6-tt.byteimg.com/origin/pgc-image/63df64f4f6af401db506372dc06c0b2f?from=pc)

è¿™ä¸ªæ–¹æ¡ˆå·²ç»åœ¨è‘—åçš„Word2Vecè®ºæ–‡çš„â€œskip-gramâ€æ–¹æ³•ä¸­å®ç°ã€‚
 
## 3. ç›¸é‚»å¥å­çš„é¢„æµ‹ï¼ˆSkip-Thought Vectorsï¼Œå¥å­çº§åˆ«çš„skip-gramï¼‰
 
åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå–ä¸‰ä¸ªè¿ç»­çš„å¥å­ï¼Œè®¾è®¡ä¸€ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­ç»™å®šä¸­å¿ƒå¥ï¼Œç”Ÿæˆå‰ä¸€ä¸ªå¥å­å’Œä¸‹ä¸€ä¸ªå¥å­ã€‚å®ƒç±»ä¼¼äºä¹‹å‰çš„skip-gramæ–¹æ³•ï¼Œä½†é€‚ç”¨äºå¥å­è€Œä¸æ˜¯å•è¯ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p6-tt.byteimg.com/origin/pgc-image/6c88f0845d7e4ae38ea626bb3bfd9000?from=pc)

è¿™ä¸ªæ–¹æ¡ˆå·²ç»åœ¨Skip-Thought Vectorsçš„è®ºæ–‡ä¸­ä½¿ç”¨è¿‡ã€‚
 
## 4. è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆn-gram/gptï¼‰
 
åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œå–å¤§é‡æœªæ ‡æ³¨çš„æ–‡æœ¬ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªä»»åŠ¡ï¼Œæ ¹æ®å‰é¢çš„å•è¯é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚å› ä¸ºä¸‹ä¸€ä¸ªæ¥è‡ªè¯­æ–™åº“çš„å•è¯å·²çŸ¥ï¼Œæ‰€ä»¥ä¸éœ€è¦æ‰‹å·¥æ ‡æ³¨ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/bf36697cfb06464c8faf6ab3c88a2493?from=pc)
  - ä¾‹å¦‚ï¼Œé€šè¿‡é¢„æµ‹ç»™å®šå‰ä¸€ä¸ªå•è¯çš„ä¸‹ä¸€ä¸ªå•è¯æ¥å°†ä»»åŠ¡è®¾ç½®ä¸º**ä»å·¦åˆ°å³**çš„è¯­è¨€å»ºæ¨¡ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p6-tt.byteimg.com/origin/pgc-image/1ddb11e50efa4a6f955bac7e14218b1b?from=pc)
  - ä¹Ÿå¯ä»¥ç”¨è¿™ä¸ªæ–¹æ¡ˆæ¥é€šç»™å®šæœªæ¥çš„å•è¯é¢„æµ‹ä¹‹å‰çš„å•è¯ï¼Œæ–¹å‘æ˜¯**ä»å³åˆ°å·¦**ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p6-tt.byteimg.com/origin/pgc-image/bbb35ee25289497a9c40e3eeee901fe0?from=pc)

è¿™ä¸ªæ–¹æ¡ˆå·²ç»ä½¿ç”¨åœ¨è®¸å¤šè®ºæ–‡ä¸­ï¼Œä»n-gramæ¨¡å‹åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹æ¯”å¦‚ç¥ç»æ¦‚ç‡è¯­è¨€æ¨¡å‹ (GPT) ã€‚
 
## 5. æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆbertç³»åˆ—ï¼‰
 
åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæ–‡æœ¬ä¸­çš„å•è¯æ˜¯éšæœºæ©ç çš„ï¼Œä»»åŠ¡æ˜¯é¢„æµ‹å®ƒä»¬ã€‚ä¸è‡ªå›å½’å…¬å¼ç›¸æ¯”ï¼Œåœ¨é¢„æµ‹æ©ç å•è¯æ—¶å¯ä»¥åŒæ—¶ä½¿ç”¨å‰ä¸€ä¸ªè¯å’Œä¸‹ä¸€ä¸ªè¯çš„ä¸Šä¸‹æ–‡ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/a9b530083051422f9f88c1613eff489f?from=pc)

è¿™ä¸ªæ–¹æ¡ˆå·²ç»åœ¨BERTã€RoBERTaå’ŒALBERTçš„è®ºæ–‡ä¸­ä½¿ç”¨è¿‡ã€‚ä¸è‡ªå›å½’ç›¸æ¯”ï¼Œåœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œåªé¢„æµ‹äº†ä¸€å°éƒ¨åˆ†æ©ç è¯ï¼Œå› æ­¤ä»æ¯å¥è¯ä¸­å­¦åˆ°çš„ä¸œè¥¿æ›´å°‘ã€‚
 
## 6. ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆNSPä»»åŠ¡ï¼Œbertä½¿ç”¨ï¼‰
 
åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬å–æ–‡ä»¶ä¸­å‡ºç°çš„ä¸¤ä¸ªè¿ç»­çš„å¥å­ï¼Œä»¥åŠåŒä¸€æ–‡ä»¶æˆ–ä¸åŒæ–‡ä»¶ä¸­éšæœºå‡ºç°çš„å¦ä¸€ä¸ªå¥å­ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p3-tt.byteimg.com/origin/pgc-image/8d58e6c913dc445d895429086cc50ae9?from=pc)

ç„¶åï¼Œä»»åŠ¡æ˜¯åŒºåˆ†ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯è¿è´¯çš„ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p3-tt.byteimg.com/origin/pgc-image/f50ff4fe179a4678926ddcc3769289f0?from=pc)

åœ¨BERTçš„è®ºæ–‡ä¸­ï¼Œå®ƒè¢«ç”¨äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦ç†è§£å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œæ¯”å¦‚è‡ªç„¶è¯­è¨€æ¨ç†(NLI)å’Œé—®é¢˜å›ç­”ã€‚ç„¶è€Œï¼Œåæ¥çš„ç ”ç©¶å¯¹å…¶æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ã€‚
 
## 7. å¥å­é¡ºåºçš„é¢„æµ‹ï¼ˆalbertï¼Œå–ä»£NSPï¼‰
 
åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬ä»æ–‡æ¡£ä¸­æå–æˆå¯¹çš„è¿ç»­å¥å­ã€‚ç„¶åäº’æ¢è¿™ä¸¤ä¸ªå¥å­çš„ä½ç½®ï¼Œåˆ›å»ºå‡ºå¦å¤–ä¸€å¯¹å¥å­ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/0638806390584a488e06c82a11832455?from=pc)

ç›®æ ‡æ˜¯å¯¹ä¸€å¯¹å¥å­è¿›è¡Œåˆ†ç±»ï¼Œçœ‹é¡ºåºæ˜¯å¦æ­£ç¡®ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/2ba2388f8dfa4778ba4684a8bbbc57fe?from=pc)

åœ¨ALBERTçš„è®ºæ–‡ä¸­ï¼Œå®ƒè¢«ç”¨æ¥å–ä»£â€œä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹â€ä»»åŠ¡ã€‚
 
## 8. å¥å­é‡æ’ï¼ˆbartï¼‰
 
åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œä»è¯­æ–™åº“ä¸­å–å‡ºä¸€ä¸ªè¿ç»­çš„æ–‡æœ¬ï¼Œå¹¶ç ´å¼€çš„å¥å­ã€‚ç„¶åï¼Œå¯¹å¥å­çš„ä½ç½®è¿›è¡Œéšæœºæ‰“ä¹±ï¼Œä»»åŠ¡æ˜¯æ¢å¤å¥å­çš„åŸå§‹é¡ºåºã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p6-tt.byteimg.com/origin/pgc-image/451d926ff39a4ce6b3f2ef4b97d18469?from=pc)
å®ƒå·²ç»åœ¨BARTçš„è®ºæ–‡ä¸­è¢«ç”¨ä½œé¢„è®­ç»ƒçš„ä»»åŠ¡ä¹‹ä¸€ã€‚
 
## 9. æ–‡æ¡£æ—‹è½¬ï¼ˆbartï¼‰
 
åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæ–‡æ¡£ä¸­çš„ä¸€ä¸ªéšæœºtokenè¢«é€‰æ‹©ä¸ºæ—‹è½¬ç‚¹ã€‚ç„¶åï¼Œå¯¹æ–‡æ¡£è¿›è¡Œæ—‹è½¬ï¼Œä½¿å¾—è¿™ä¸ªtokenæˆä¸ºå¼€å§‹è¯ã€‚ä»»åŠ¡æ˜¯ä»è¿™ä¸ªæ—‹è½¬çš„ç‰ˆæœ¬ä¸­æ¢å¤åŸæ¥çš„å¥å­ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p3-tt.byteimg.com/origin/pgc-image/8095d77a0fc440978eb825289a83acf6?from=pc)

å®ƒå·²ç»åœ¨BARTçš„è®ºæ–‡ä¸­è¢«ç”¨ä½œé¢„è®­ç»ƒçš„ä»»åŠ¡ä¹‹ä¸€ã€‚ç›´è§‰ä¸Šï¼Œè¿™å°†è®­ç»ƒæ¨¡å‹å¼€å§‹è¯†åˆ«æ–‡æ¡£ã€‚
 
## 10. è¡¨æƒ…ç¬¦å·é¢„æµ‹
 
è¿™ä¸ªæ–¹æ¡ˆè¢«ç”¨åœ¨äº†DeepMojiçš„è®ºæ–‡ä¸­ï¼Œç”¨è¡¨æƒ…ç¬¦å·æ¥è¡¨è¾¾æ¨æ–‡çš„æƒ…æ„Ÿã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œç”¨æ¨ç‰¹ä¸Šçš„è¡¨æƒ…ç¬¦å·ä½œä¸ºæ ‡ç­¾ï¼Œå¹¶åˆ¶å®šä¸€ä¸ªç›‘ç£ä»»åŠ¡ï¼Œåœ¨ç»™å‡ºæ–‡æœ¬æ—¶é¢„æµ‹è¡¨æƒ…ç¬¦å·ã€‚
- ![NLPä¸­çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œå…¨æ˜¯åŠ¨å›¾ï¼Œå¾ˆè¿‡ç˜¾çš„](https://p1-tt.byteimg.com/origin/pgc-image/501d4092a3c4433298824a230c07eeb1?from=pc)

DeepMojiçš„ä½œè€…ä»¬ä½¿ç”¨è¿™ä¸ªæ¦‚å¿µå¯¹ä¸€ä¸ªæ¨¡å‹è¿›è¡Œäº†12äº¿æ¡æ¨æ–‡çš„é¢„è®­ç»ƒï¼Œç„¶ååœ¨æƒ…ç»ªåˆ†æã€ä»‡æ¨è¯­è¨€æ£€æµ‹å’Œä¾®è¾±æ£€æµ‹ç­‰ä¸æƒ…ç»ªç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚


# é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html),Harvard NLPå‡ºå“ï¼Œå«pytorchç‰ˆä»£ç å®ç°
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Transformeræ¨¡å‹çš„PyTorchå®ç°](https://luozhouyang.github.io/transformer/),[A PyTorch implementation of the Transformer model in "Attention is All You Need"](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

ã€2023-1-31ã€‘[ä»ChatGPTè¯´èµ·ï¼ŒAIGCç”Ÿæˆæ¨¡å‹å¦‚ä½•æ¼”è¿›](https://m.gelonghui.com/p/572090)

Transformer: æ”¹å˜äº†NLPå‘å±•å›°å¢ƒ
- 2017å¹´ï¼ŒGoogle å‘è¡¨çš„è‘—åæ–‡ç«  attention is all you needï¼Œæå‡ºäº†transformeræ¨¡å‹æ¦‚å¿µï¼Œä½¿å¾—NLPä¸Šå‡äº†å·¨å¤§çš„å°é˜¶ã€‚Transformeræ¶æ„çš„æ ¸å¿ƒæ˜¯ Self-Attentionæœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä½¿å¾—Transformerèƒ½å¤Ÿæœ‰æ•ˆæå–**é•¿åºåˆ—**ç‰¹å¾ï¼Œç›¸è¾ƒäº CNNèƒ½å¤Ÿæ›´å¥½çš„è¿˜åŸå…¨å±€ã€‚

å› ä¸ºæŠ›å¼ƒäº†ä¼ ç»Ÿçš„RNNæ¨¡å‹
- å½»åº•è§„é¿äº†RNNä¸èƒ½å¾ˆå¥½**å¹¶è¡Œè®¡ç®—**çš„å›°æ‰°
- æ¯ä¸€æ­¥è®¡ç®—ä¸ä¾èµ–äºä¸Šä¸€æ­¥çš„è®¡ç®—ç»“æœï¼Œå› æ­¤æå¤§æé«˜äº†æ¨¡å‹å¹¶è¡Œè®­ç»ƒè®¡ç®—çš„æ•ˆç‡ã€‚

æ­¤å¤–ï¼Œå®ƒèƒ½å®ç°è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚
- è‡ªæˆ‘ç›‘ç£ï¼šä¸éœ€è¦æ ‡æ³¨çš„æ ·æœ¬ï¼Œä½¿ç”¨æ ‡å‡†çš„è¯­æ–™æˆ–è€…å›¾åƒã€æ¨¡å‹å°±èƒ½å­¦ä¹ äº†ã€‚

éšç€Transformerçš„æ¨ªç©ºå‡ºä¸–ï¼Œæ ¹æ®Decoder/Encoderåˆå¯ä»¥åˆ’åˆ†ä¸ºGPT/BERTæ¨¡å‹ã€‚
- ![](https://img3.gelonghui.com/222f4-1030a359-91f9-42a2-831c-a9fe876ca888.png)
- BERTä¸GPTåˆ†åˆ«å¯¹åº”äº†Transformerçš„ç¼–ç å™¨ä¸è§£ç å™¨ã€‚å…¶ä¸­BERTå¯ä»¥è¢«ç†è§£ä¸ºåŒå‘çš„ç†è§£æ¨¡å‹ï¼Œè€ŒGPTå¯ä»¥è¢«ç†è§£ä¸ºå•å‘çš„ç”Ÿæˆæ¨¡å‹ã€‚
- ![](https://img3.gelonghui.com/2d910-ac98c07f-faf8-4e0f-81dc-84df1bccd60a.png)

## é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹

- ã€2021-6-7ã€‘[ä¸€æ–‡äº†è§£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹](https://mp.weixin.qq.com/s/meDVXt91pypl4Gn_1A6iVg), é…å¥—ä¹¦ç±ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œ2021-5å‡ºç‰ˆ
- ã€2021-6-17ã€‘[é¢„è®­ç»ƒæ¨¡å‹æœ€æ–°ç»¼è¿°ï¼šè¿‡å»ã€ç°åœ¨å’Œæœªæ¥](https://zhuanlan.zhihu.com/p/381121057) [Pre-Trained Models: Past, Present and Future](https://arxiv.org/abs/2106.07139)ï¼Œå…¨é¢å›é¡¾äº† PTM çš„æœ€æ–°çªç ´ã€‚è¿™äº›çªç ´æ˜¯ç”±è®¡ç®—èƒ½åŠ›çš„æ¿€å¢å’Œæ•°æ®å¯ç”¨æ€§å¢åŠ æ¨åŠ¨çš„ï¼Œæœç€å››ä¸ªé‡è¦æ–¹å‘å‘å±•ï¼šè®¾è®¡æœ‰æ•ˆçš„æ¶æ„ã€åˆ©ç”¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€æé«˜è®¡ç®—æ•ˆç‡ä»¥åŠè¿›è¡Œè§£é‡Šå’Œç†è®ºåˆ†æã€‚
- PTMå‘å±•è¿‡ç¨‹ [github](https://github.com/thunlp/PLMpapers)ï¼šæ¸…åå¤§å­¦çš„ä¸¤ä½åŒå­¦â€”â€”`ç‹æ™“æ™º`å’Œ`å¼ æ­£å½¦`ï¼ˆåœ¨è¯»æœ¬ç§‘ç”Ÿï¼‰æ•´ç†çš„ä¸€ä»½å…³äºé¢„è®­ç»ƒæ¨¡å‹çš„å…³ç³»å›¾ï¼Œåˆ™å¯ä»¥ä»åŠŸèƒ½æ–¹é¢æ›´ç®€å•æ˜äº†çš„å¸®æˆ‘ä»¬ç†è§£è¯¥ç±»æ¨¡å‹ç±»åˆ«ã€‚[Pre-Trained Models: Past, Present and Future](https://arxiv.org/abs/2106.07139) è®ºæ–‡äºŒä½œ
  - ![](https://pic2.zhimg.com/80/v2-d82cd793c1b59c20ee7f97d95f53c675_720w.jpg)
  - [æ–‡å­—è®²è§£](http://wujiawen.xyz/archives/nlp%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B0%8F%E7%BB%BC%E8%BF%B0)
- ä¸ºäº†è§£å†³è¯æ±‡é—®é¢˜ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸é‡‡ç”¨ subwordï¼ˆæ·±å…¥ç†è§£ NLP Subword ç®—æ³•ï¼šBPEã€WordPieceã€ULMï¼‰åˆ›å»ºè¯è¡¨ã€‚å¸¸è§çš„ subword æ–¹å¼æœ‰ `BPE`ï¼ˆOpenAI GPT-2 ä¸ Facebook RoBERTaï¼‰, `WordPiece`ï¼ˆBERTï¼‰ï¼Œ`Unigram Language Model` ç­‰ã€‚
- é‡‡ç”¨ Transformer Encoder æ¶æ„çš„ BERT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œéšæœºå¯¹éƒ¨åˆ†`åˆ†è¯`ï¼ˆsubwordï¼‰è¿›è¡Œäº†æ©ç ï¼ˆå¦‚ dog, \#\#ing å˜ [MASK] ï¼‰ã€‚å¯¹äºä¸­æ–‡ï¼Œä¸Šè¿°å¯¹åˆ†è¯éšæœºæ©ç çš„æ–¹å¼å­˜åœ¨åˆ†å‰²å•è¯ã€ç ´åè¯­ä¹‰çš„æƒ…å†µï¼Œå› æ­¤ BERT-WWM ï¼ˆPre-Training with Whole Word Masking for Chinese BERT ï¼‰æå‡ºäº† whole word mask ã€‚é’ˆå¯¹æ•´ä¸ª ä¸­æ–‡å•è¯ è¿›è¡Œæ©ç ã€‚

- è¿ç§»å­¦ä¹ åˆ†ç±»
  - ![](https://pic2.zhimg.com/80/v2-67138799a41ee6e489727b15c0b1e731_720w.jpg)
- ã€2020-8-13ã€‘[æ‰“ç ´BERTå¤©èŠ±æ¿ï¼š11ç§èŠ±å¼ç‚¼ä¸¹æœ¯åˆ·çˆ†NLPåˆ†ç±»SOTAï¼](https://blog.csdn.net/abcdefg90876/article/details/108016310)
  - <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy81ZmtuYjQxaWI5cUgxd240a08wQ1FpYkJlZGNiZzduemZCUTNKMTlPcTNnRFZxY1ZFbU1lMjhPWjlwZkQ0SkswanV1YVVZNjYwTEtzcUJteE5BUTU4WlRnLzY0MA" height="100%" width="100" />
- NLPåˆ†ç±»æ¨¡å‹æ—¶é—´çº¿
  - ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy81ZmtuYjQxaWI5cUYzaWJLQ05yOG9FakZjRDF5bE9pY1o5VHVHTlpKcUN1N0ZtcWliMHZKbmU3c0V5Z2ljQkFzdTc3RDdTbjN2a0pTR1hDaWM5OUZRelRpY0dqU3cvNjQw)

- 2020å¹´3æœˆ18æ—¥ï¼Œé‚±é”¡é¹è€å¸ˆå‘è¡¨äº†å…³äºNLPé¢„è®­ç»ƒæ¨¡å‹çš„ç»¼è¿°ã€Š[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271v2)ã€‹
- ã€2020-9-9ã€‘[é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLMs)èµ°çš„é£å¿«](https://zhuanlan.zhihu.com/p/93781241)
- ![](https://pic1.zhimg.com/v2-447ae7707604e7ac520555249332c42c_1440w.jpg)
- é¢„è®­ç»ƒæ¨¡å‹åœ¨ç»å†ä¸­4ä¸ªæ—¶ä»£
  - ç¬¬ä¸€ä¸ªæ˜¯è½°åŠ¨æ€§çš„**è¯åµŒå…¥**ï¼ˆWord Embeddingï¼‰æ—¶ä»£ï¼Œ æ°å‡ºä»£è¡¨æ˜¯Word2Vecå’ŒGloveï¼›
  - ç¬¬äºŒä¸ªæ˜¯**ä¸Šä¸‹æ–‡åµŒå…¥**ï¼ˆContext Word Embeddingï¼‰ï¼Œä»£è¡¨ä¸ºCoVeå’ŒELMOï¼›
  - ç¬¬ä¸‰ä¸ªæ—¶ä»£æ˜¯**é¢„è®­ç»ƒæ¨¡å‹**ï¼Œä»£è¡¨æ˜¯GPTå’ŒBERTï¼› 
  - ç¬¬å››ä¸ªæ—¶ä»£æ˜¯**æ”¹è¿›å‹**å’Œ**é¢†åŸŸå®šåˆ¶å‹**ã€‚ 
    - æ”¹è¿›å‹ä»£è¡¨ä¸ºALBERTå’ŒXLNet
    - é¢†åŸŸå®šåˆ¶åŒ–(Domain Specific)ä»£è¡¨ä¸ºSciBert (Scientific Bert) å’ŒBioBert(Biomedical Bert)ã€‚ 
- ã€2020-9-30ã€‘nlpä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
  - ![](https://pic4.zhimg.com/v2-0e78a280939451bef50bc0b1a521c45b_1440w.jpg)
- ä¸»è¦åŒ…æ‹¬3å¤§æ–¹é¢ï¼Œæ¶‰åŠåˆ°çš„æ¨¡å‹æœ‰ï¼š
  - **å•å‘**ç‰¹å¾è¡¨ç¤ºçš„**è‡ªå›å½’**é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç»Ÿç§°ä¸º**å•å‘æ¨¡å‹**ï¼šELMO/ULMFiT/SiATL/GPT1.0/GPT2.0ï¼›
  - **åŒå‘**ç‰¹å¾è¡¨ç¤ºçš„**è‡ªç¼–ç **é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç»Ÿç§°ä¸º**BERTç³»åˆ—æ¨¡å‹**ï¼š(BERT/MASS/UNILM/ERNIE1.0/ERNIE(THU)/MTDNN/ERNIE2.0/SpanBERT/RoBERTa)
  - **åŒå‘**ç‰¹å¾è¡¨ç¤ºçš„**è‡ªå›å½’**é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼šXLNetï¼›
- PTMs: Pre-trained-Models in NLPï¼Œ[NLPé¢„è®­ç»ƒæ¨¡å‹çš„å…¨é¢æ€»ç»“(æŒç»­æ›´æ–°ä¸­)](https://github.com/loujie0822/Pre-trained-Models/blob/master/README.md)
- 2020å¹´3æœˆ18æ—¥ï¼Œé‚±é”¡é¹è€å¸ˆå‘è¡¨äº†å…³äºNLPé¢„è®­ç»ƒæ¨¡å‹çš„ç»¼è¿°ã€Š[Pre-trained Models for Natural Language Processing: A Survey](https://zhuanlan.zhihu.com/p/115014536?utm_source=qq&utm_medium=social&utm_oi=27211553832960#ref_1)ã€‹
- çŸ¥ä¹æ–‡ç« 1:  [å…¨é¢æ€»ç»“ï¼PTMsï¼šNLPé¢„è®­ç»ƒæ¨¡å‹](https://zhuanlan.zhihu.com/p/115014536)ï¼Œ[å›¾ç‰‡ä¸‹è½½](https://github.com/loujie0822/Pre-trained-Models/blob/master/resources/PTMs.jpg)
- çŸ¥ä¹æ–‡ç« 2ï¼š[nlpä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“](https://zhuanlan.zhihu.com/p/76912493)
- çŸ¥ä¹æ–‡ç« 3ï¼š[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”](https://zhuanlan.zhihu.com/p/56382372)

<img src="https://pic3.zhimg.com/80/v2-0ace60ca3d843fc9b69c6965731f288e_720w.jpg" style="zoom:20%;" />

- å¯¹æ¯”åˆ†æï¼Œæ‘˜è‡ªï¼š[è®ºæ–‡ç¬”è®° - Pre-trained Models for Natural Language Processing](http://www.shuang0420.com/2020/05/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Pre-trained%20Models%20for%20Natural%20Language%20Processing/)ï¼Œ[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)
   - `LM`ï¼ˆLanguage Modelingï¼‰æ˜¯ NLP ä¸­æœ€å¸¸è§çš„æ— ç›‘ç£ä»»åŠ¡ï¼Œé€šå¸¸ç‰¹æŒ‡è‡ªå›å½’æˆ–å•å‘è¯­è¨€å»ºæ¨¡ï¼ŒBiLM è™½ç„¶ç»“åˆäº†ä¸¤ä¸ªæ–¹å‘çš„è¯­è¨€æ¨¡å‹ï¼Œä½†åªæ˜¯ä¸¤ä¸ªæ–¹å‘çš„ç®€å•æ‹¼æ¥ï¼Œå¹¶ä¸æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„åŒå‘è¯­è¨€æ¨¡å‹ã€‚
   - `MLM`ï¼ˆMasked Language Modelingï¼‰å¯ä»¥å…‹æœä¼ ç»Ÿ**å•å‘**è¯­è¨€æ¨¡å‹çš„ç¼ºé™·ï¼Œç»“åˆåŒå‘çš„ä¿¡æ¯ï¼Œä½†æ˜¯ \[MASK] çš„å¼•å…¥ä½¿å¾—é¢„è®­ç»ƒå’Œ fine-tune ä¹‹é—´å‡ºç° gap
   - `PLM`ï¼ˆPermuted Language Modelingï¼‰åˆ™å…‹æœäº†è¿™ä¸ªé—®é¢˜ï¼Œå®ç°äº†**åŒå‘**è¯­è¨€æ¨¡å‹å’Œ**è‡ªå›å½’**æ¨¡å‹çš„**ç»Ÿä¸€**ã€‚
   - `DAE`ï¼ˆDenoising Autoencoderï¼‰æ¥å—éƒ¨åˆ†æŸåçš„è¾“å…¥ï¼Œå¹¶ä»¥æ¢å¤åŸå§‹è¾“å…¥ä¸ºç›®æ ‡ã€‚ä¸ MLM ä¸åŒï¼ŒDAE ä¼šç»™è¾“å…¥é¢å¤–åŠ ä¸€äº›**å™ªå£°**ã€‚
   - `CTL`ï¼ˆContrastive Learningï¼‰ çš„åŸç†æ˜¯åœ¨**å¯¹æ¯”**ä¸­å­¦ä¹ ï¼Œå…¶å‡è®¾æ˜¯ä¸€äº› observed pairs of text åœ¨è¯­ä¹‰ä¸Šæ¯”éšæœºé‡‡æ ·çš„æ–‡æœ¬æ›´ä¸ºæ¥è¿‘ã€‚CTL æ¯” LM è®¡ç®—å¤æ‚åº¦æ›´ä½ã€‚
- ç»¼è¿°ä»å››ä¸ªæ–¹é¢ï¼ˆRepresentation Typesã€Architecturesã€Pre-training Task Typesã€Extensionsï¼‰å¯¹ç°æœ‰ PTMs (Pre-trained Models) è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ï¼Œä¸€å¹…[å›¾](http://images.shuang0420.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Pre-trained%20Models%20for%20Natural%20Language%20Processing/taxonomy.png)æ¥æ¦‚æ‹¬å…¨æ–‡ç²¾åï¼š

## 1ã€è®ºæ–‡æ±‡æ€»ï¼š

PTMs-Papers
1. æ¸…å[PLMpapers](https://github.com/thunlp/PLMpapers)
2. [BERT-related-papers](https://github.com/tomohideshibata/BERT-related-papers)
3. [awesome-bert-nlp](https://github.com/cedrickchee/awesome-bert-nlp)
4. [bertlang](https://bertlang.unibocconi.it/)
5. [bertviz](https://github.com/jessevig/bertviz)

## 2. PTMså•æ¨¡å‹è§£è¯»

1. è‡ªç›‘ç£å­¦ä¹ ï¼š[Self-Supervised Learning å…¥é—¨ä»‹ç»](https://zhuanlan.zhihu.com/p/108625273)
2. è‡ªç›‘ç£å­¦ä¹ ï¼š[Self-supervised Learning å†æ¬¡å…¥é—¨](https://zhuanlan.zhihu.com/p/108906502)
3. è¯å‘é‡æ€»ç»“ï¼š[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼šword2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)
4. è¯å‘é‡æ€»ç»“ï¼š[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
5. ELMoè§£è¯»ï¼š[å…³äºELMoçš„è‹¥å¹²é—®é¢˜æ•´ç†è®°å½•](https://zhuanlan.zhihu.com/p/82602015)
6. BERTè§£è¯»ï¼š [Bertæ—¶ä»£çš„åˆ›æ–°ï¼šBertåº”ç”¨æ¨¡å¼æ¯”è¾ƒåŠå…¶å®ƒ](https://zhuanlan.zhihu.com/p/65470719)
7. XLNETè§£è¯»ï¼š[XLNet:è¿è¡Œæœºåˆ¶åŠå’ŒBertçš„å¼‚åŒæ¯”è¾ƒ](https://zhuanlan.zhihu.com/p/70257427)
8. XLNETè§£è¯»ï¼š[XLnetï¼šæ¯”Bertæ›´å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹](https://zhuanlan.zhihu.com/p/71759544)
9. RoBERTaè§£è¯»ï¼š[RoBERT: æ²¡é”™ï¼Œæˆ‘å°±æ˜¯èƒ½æ›´å¼ºâ€”â€”æ›´å¤§æ•°æ®è§„æ¨¡å’Œä»”ç»†è°ƒå‚ä¸‹çš„æœ€ä¼˜BERT](https://zhuanlan.zhihu.com/p/75629127)
10. é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“ï¼š[nlpä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“(å•å‘æ¨¡å‹ã€BERTç³»åˆ—æ¨¡å‹ã€XLNet)](https://zhuanlan.zhihu.com/p/76912493)
11. é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“ï¼š[8ç¯‡è®ºæ–‡æ¢³ç†BERTç›¸å…³æ¨¡å‹è¿›å±•ä¸åæ€](https://zhuanlan.zhihu.com/p/81157740)
12. ELECTRAè§£è¯»: [ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡å‹](https://zhuanlan.zhihu.com/p/89763176)
13. æ¨¡å‹å‹ç¼© LayerDrop:[ç»“æ„å‰ªæï¼šè¦ä¸ª4å±‚çš„BERTæœ‰å¤šéš¾ï¼Ÿ](https://zhuanlan.zhihu.com/p/93207254)
14. æ¨¡å‹å‹ç¼© BERT-of-Theseus:[bert-of-theseusï¼Œä¸€ä¸ªéå¸¸äº²æ°‘çš„bertå‹ç¼©æ–¹æ³•](https://zhuanlan.zhihu.com/p/112787764)
15. æ¨¡å‹å‹ç¼© TinyBERT:[æ¯” Bert ä½“ç§¯æ›´å°é€Ÿåº¦æ›´å¿«çš„ TinyBERT](https://zhuanlan.zhihu.com/p/94359189)
16. æ¨¡å‹å‹ç¼©æ€»ç»“ï¼š[BERT ç˜¦èº«ä¹‹è·¯ï¼šDistillationï¼ŒQuantizationï¼ŒPruning](https://zhuanlan.zhihu.com/p/86900556)


## ä¸­æ–‡æ¨¡å‹ä¸‹è½½

- 2019å¹´ï¼Œå“ˆå·¥å¤§ä¸è®¯é£å‡ºå“ï¼š[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
- æœ¬ç›®å½•ä¸­ä¸»è¦åŒ…å«baseæ¨¡å‹ï¼Œæ•…æˆ‘ä»¬ä¸åœ¨æ¨¡å‹ç®€ç§°ä¸­æ ‡æ³¨`base`å­—æ ·ã€‚å¯¹äºå…¶ä»–å¤§å°çš„æ¨¡å‹ä¼šæ ‡æ³¨å¯¹åº”çš„æ ‡è®°ï¼ˆä¾‹å¦‚largeï¼‰ã€‚

* **`BERT-largeæ¨¡å‹`**ï¼š24-layer, 1024-hidden, 16-heads, 330M parameters  
* **`BERT-baseæ¨¡å‹`**ï¼š12-layer, 768-hidden, 12-heads, 110M parameters  

| æ¨¡å‹ç®€ç§° | è¯­æ–™ | Googleä¸‹è½½ | è®¯é£äº‘ä¸‹è½½ |
| :------- | :--------- | :---------: | :---------: |
| **`RBTL3, Chinese`** | **ä¸­æ–‡ç»´åŸº+<br/>é€šç”¨æ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)**<br/>**[PyTorch](https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv)** | **[TensorFlowï¼ˆå¯†ç vySWï¼‰](https://pan.iflytek.com:443/link/0DD18FAC080BAF75DBA28FB5C0047760)**<br/>**[PyTorchï¼ˆå¯†ç rgCsï¼‰](https://pan.iflytek.com:443/link/7C6A513BED2D42170B6DBEE5A866FB3F)** |
| **`RBT3, Chinese`** | **ä¸­æ–‡ç»´åŸº+<br/>é€šç”¨æ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi)**<br/>**[PyTorch](https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed)** | **[TensorFlowï¼ˆå¯†ç b9nxï¼‰](https://pan.iflytek.com:443/link/275E5B46185C982D4AF5AC295E1651B6)**<br/>**[PyTorchï¼ˆå¯†ç Yoepï¼‰](https://pan.iflytek.com:443/link/A094EB0A73B1E7209FEBC6C5CF7AEF27)** |
| **`RoBERTa-wwm-ext-large, Chinese`** | **ä¸­æ–‡ç»´åŸº+<br/>é€šç”¨æ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)**<br/>**[PyTorch](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq)** | **[TensorFlowï¼ˆå¯†ç u6gCï¼‰](https://pan.iflytek.com:443/link/AC056611607108F33A744A0F56D0F6BE)**<br/>**[PyTorchï¼ˆå¯†ç 43eHï¼‰](https://pan.iflytek.com:443/link/9B46A0ABA70C568AAAFCD004B9A2C773)** |
| **`RoBERTa-wwm-ext, Chinese`** | **ä¸­æ–‡ç»´åŸº+<br/>é€šç”¨æ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)** <br/>**[PyTorch](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25)** | **[TensorFlowï¼ˆå¯†ç Xe1pï¼‰](https://pan.iflytek.com:443/link/98D11FAAF0F0DBCB094EE19CCDBC98BF)** <br/>**[PyTorchï¼ˆå¯†ç waV5ï¼‰](https://pan.iflytek.com:443/link/92ADD2C34C91F3B44E0EC97F101F89D8)**|
| **`BERT-wwm-ext, Chinese`** | **ä¸­æ–‡ç»´åŸº+<br/>é€šç”¨æ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)** <br/>**[PyTorch](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)** | **[TensorFlowï¼ˆå¯†ç 4cMGï¼‰](https://pan.iflytek.com:443/link/653637473FFF242C3869D77026C9BDB5)** <br/>**[PyTorchï¼ˆå¯†ç XHu4ï¼‰](https://pan.iflytek.com:443/link/B9ACE1C9F228A0F42242672EF6CE1721)** |
| **`BERT-wwm, Chinese`** | **ä¸­æ–‡ç»´åŸº** | **[TensorFlow](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)** <br/>**[PyTorch](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)** | **[TensorFlowï¼ˆå¯†ç 07Xjï¼‰](https://pan.iflytek.com:443/link/A2483AD206EF85FD91569B498A3C3879)** <br/>**[PyTorchï¼ˆå¯†ç hteXï¼‰](https://pan.iflytek.com:443/link/5DBDD89414E5B565D3322D6B7937DF47)** |
| `BERT-base, Chinese`<sup>Google</sup> | ä¸­æ–‡ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | - |
| `BERT-base, Multilingual Cased`<sup>Google</sup>  | å¤šè¯­ç§ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | - |
| `BERT-base, Multilingual Uncased`<sup>Google</sup>  | å¤šè¯­ç§ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | - |

> [1] é€šç”¨æ•°æ®åŒ…æ‹¬ï¼šç™¾ç§‘ã€æ–°é—»ã€é—®ç­”ç­‰æ•°æ®ï¼Œæ€»è¯æ•°è¾¾5.4Bï¼Œå¤„ç†åçš„æ–‡æœ¬å¤§å°çº¦10G

ä»¥ä¸Šé¢„è®­ç»ƒæ¨¡å‹ä»¥TensorFlowç‰ˆæœ¬çš„æƒé‡ä¸ºå‡†ã€‚ä¸­å›½å¤§é™†å¢ƒå†…å»ºè®®ä½¿ç”¨è®¯é£äº‘ä¸‹è½½ç‚¹ï¼Œå¢ƒå¤–ç”¨æˆ·å»ºè®®ä½¿ç”¨è°·æ­Œä¸‹è½½ç‚¹ï¼Œbaseæ¨¡å‹æ–‡ä»¶å¤§å°çº¦**400M**ã€‚


[pytorchä¸­æ–‡è¯­è¨€æ¨¡å‹berté¢„è®­ç»ƒä»£ç ](https://zhuanlan.zhihu.com/p/161301389)

å¯¹äºPyTorchç‰ˆæœ¬ï¼Œä½¿ç”¨çš„æ˜¯ç”±`Huggingface`å‡ºå“çš„[PyTorch-Transformers 1.0](https://github.com/huggingface/pytorch-transformers)æä¾›çš„è½¬æ¢è„šæœ¬ã€‚å¦‚æœä½¿ç”¨çš„æ˜¯å…¶ä»–ç‰ˆæœ¬ï¼Œè¯·è‡ªè¡Œè¿›è¡Œæƒé‡è½¬æ¢ã€‚

huggingfaceé¡¹ç›®ä¸­è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç”¨**maskæ–¹å¼**å¦‚ä¸‹ã€‚ä»æ˜¯æŒ‰ç…§`15%`çš„æ•°æ®éšæœºmaskç„¶åé¢„æµ‹è‡ªèº«ã€‚å¦‚æœè¦åšä¸€äº›é«˜çº§æ“ä½œæ¯”å¦‚whole word maskingæˆ–è€…å®ä½“é¢„æµ‹ï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹transformers.DataCollatorForLanguageModelingã€‚[ä»£ç ](https://github.com/zhusleep/pytorch_chinese_lm_pretrain)

```python
def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.
        """
        if self.tokenizer.mask_token is None:
            raise ValueError(
                "This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer."
            )

        labels = inputs.clone()
        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)
        probability_matrix = torch.full(labels.shape, self.mlm_probability)
        special_tokens_mask = [
            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()
        ]
        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)
        if self.tokenizer._pad_token is not None:
            padding_mask = labels.eq(self.tokenizer.pad_token_id)
            probability_matrix.masked_fill_(padding_mask, value=0.0)
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # We only compute loss on masked tokens

        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)

        # 10% of the time, we replace masked input tokens with random word
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
        inputs[indices_random] = random_words[indices_random]

        # The rest of the time (10% of the time) we keep the masked input tokens unchanged
        return inputs, labels
```

ä¸‰ä¸ªå¸¸è§çš„ä¸­æ–‡bertè¯­è¨€æ¨¡å‹ï¼š<font color='blue'>ERNIE ï¼ roberta-wwm-ext ï¼ bert-base-chinese </font>
- [bert-base-chinese](https://huggingface.co/bert-base-chinese)ï¼šæœ€å¸¸è§çš„ä¸­æ–‡bertè¯­è¨€æ¨¡å‹ï¼ŒGoogleåŸºäºä¸­æ–‡ç»´åŸºç™¾ç§‘ç›¸å…³è¯­æ–™è¿›è¡Œé¢„è®­ç»ƒã€‚æŠŠå®ƒä½œä¸º**baseline**ï¼Œåœ¨é¢†åŸŸå†…æ— ç›‘ç£æ•°æ®è¿›è¡Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¾ˆç®€å•ã€‚
- [roberta-wwm-ext](https://github.com/ymcui/Chinese-BERT-wwm)ï¼š**å“ˆå·¥å¤§**è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚é¢„è®­ç»ƒçš„æ–¹å¼æ˜¯é‡‡ç”¨robertaç±»ä¼¼çš„æ–¹æ³•ï¼Œæ¯”å¦‚åŠ¨æ€maskï¼Œæ›´å¤šçš„è®­ç»ƒæ•°æ®ç­‰ç­‰ã€‚åœ¨å¾ˆå¤šä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹æ•ˆæœè¦ä¼˜äºbert-base-chineseã€‚
- [ernie](https://github.com/nghuyong/ERNIE-Pytorch%25EF%25BC%2589)

### bert-base-chinese

é¢„è®­ç»ƒä»£ç ï¼š

```shell
python run_language_modeling.py \
    --output_dir=output \
    --model_type=bert \
    --model_name_or_path=bert-base-chinese \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE \
    --mlm
```
å…¶ä¸­$TRAIN_FILE ä»£è¡¨é¢†åŸŸç›¸å…³ä¸­æ–‡è¯­æ–™åœ°å€ã€‚

- ã€2021-8-26ã€‘ä¸­æ–‡æ¨¡å‹ï¼šbert-base-chineseï¼Œè·‘ä¸é€šï¼

### roberta-wwm-ext

ä»£ç ï¼š

```python
import torch
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
roberta = BertModel.from_pretrained("hfl/chinese-roberta-wwm-ext")

# åˆ‡è®°ä¸å¯ä½¿ç”¨å®˜æ–¹æ¨èçš„ä»¥ä¸‹è¯­å¥!
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
model = AutoModel.from_pretrained("hfl/chinese-roberta-wwm-ext")
```

æ³¨æ„ï¼š<font color='red'>åˆ‡è®°ä¸å¯ä½¿ç”¨å®˜æ–¹æ¨èçš„Autoè¯­å¥!</font>
- ä¸­æ–‡robertaç±»çš„é…ç½®æ–‡ä»¶æ¯”å¦‚vocab.txtï¼Œéƒ½æ˜¯é‡‡ç”¨bertçš„æ–¹æ³•è®¾è®¡çš„ã€‚è‹±æ–‡robertaæ¨¡å‹è¯»å–é…ç½®æ–‡ä»¶çš„æ ¼å¼é»˜è®¤æ˜¯vocab.jsonã€‚å¯¹äºä¸€äº›è‹±æ–‡robertaæ¨¡å‹ï¼Œå€’æ˜¯å¯ä»¥é€šè¿‡AutoModelè‡ªåŠ¨è¯»å–ã€‚è¿™å°±è§£é‡Šäº†huggingfaceçš„æ¨¡å‹åº“çš„**ä¸­æ–‡robertaç¤ºä¾‹ä»£ç ä¸ºä»€ä¹ˆè·‘ä¸é€š**ã€‚

å¦‚æœè¦åŸºäºä¸Šé¢çš„ä»£ç run_language_modeling.pyç»§ç»­é¢„è®­ç»ƒrobertaã€‚è¿˜éœ€è¦åšä¸¤ä¸ªæ”¹åŠ¨ã€‚
- ä¸‹è½½roberta-wwm-extåˆ°æœ¬åœ°ç›®å½•hflrobertaï¼Œåœ¨config.jsonä¸­ä¿®æ”¹â€œmodel_typeâ€:"roberta"ä¸º"model_type":"bert"ã€‚
- å¯¹ä¸Šé¢çš„run_language_modeling.pyä¸­çš„AutoModelå’ŒAutoTokenizeréƒ½è¿›è¡Œæ›¿æ¢ä¸ºBertModelå’ŒBertTokenizerã€‚

```shell
python run_language_modeling_roberta.py \
    --output_dir=output \
    --model_type=bert \
    --model_name_or_path=hflroberta \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE \
    --mlm
```

### ernie

ernieæ˜¯ç™¾åº¦å‘å¸ƒçš„åŸºäºç™¾åº¦çŸ¥é“è´´å§ç­‰ä¸­æ–‡è¯­æ–™ç»“åˆå®ä½“é¢„æµ‹ç­‰ä»»åŠ¡ç”Ÿæˆçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹çš„å‡†ç¡®ç‡åœ¨æŸäº›ä»»åŠ¡ä¸Šè¦ä¼˜äºbert-base-chineseå’Œrobertaã€‚å¦‚æœåŸºäºernie1.0æ¨¡å‹åšé¢†åŸŸæ•°æ®é¢„è®­ç»ƒçš„è¯åªéœ€è¦ä¸€æ­¥ä¿®æ”¹ã€‚
- ä¸‹è½½ernie1.0åˆ°æœ¬åœ°ç›®å½•ernieï¼Œåœ¨config.jsonä¸­å¢åŠ å­—æ®µ"model_type":"bert"ã€‚

```shell
python run_language_modeling.py \
    --output_dir=output \
    --model_type=bert \
    --model_name_or_path=ernie \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE \
    --mlm
```


## å‘é‡æœåŠ¡

- ![](https://pic3.zhimg.com/80/v2-09c5df603126e72b4ba2b0a9a45ee1b6_720w.jpg)

èµ„æ–™
- ã€2022-8-19ã€‘è…¾è®¯AI Labå¼€å‘çš„è¿‘ä¹‰è¯æŸ¥è¯¢å·¥å…·ï¼š[è¿‘é‚»è¯æ±‡æ£€ç´¢](https://tool.mingdawoo.com/lang/nearby_word/search?q=%E7%A5%9E%E9%A9%AC&ds=1)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/2d04c2364fac4cdea886ef3ab1bf8ef0~noop.image)

[æ–‡æœ¬è¯­ä¹‰åŒ–å‘é‡åŒ–](https://zhuanlan.zhihu.com/p/443790030)æ˜¯æŠŠæ–‡æœ¬ç”¨Embeddingå‘é‡æ¥è¡¨ç¤ºï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬çš„embeddingå‘é‡ä¹Ÿç›¸ä¼¼ã€‚è¯­ä¹‰åŒ–å‘é‡å¯ä»¥ç”¨äºæ–‡æœ¬å»é‡ã€åŒ¹é…ï¼Œåœ¨æœç´¢å’Œæ¨èä¸šåŠ¡ä¸­éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚

ä¸€èˆ¬ç»“æ„ï¼š BERT Encoder --> Pooling --> Loss Layer
- ![](https://pic3.zhimg.com/80/v2-3a70824e658d04ebd37f1973a3168eea_1440w.png)

Pooling Layer
- P1ï¼šæŠŠencoderçš„æœ€åä¸€å±‚çš„[CLS]å‘é‡æ‹¿å‡ºæ¥ã€‚
- P2ï¼šæŠŠPoolerï¼ˆBERTç”¨æ¥åšNSPä»»åŠ¡ï¼‰å¯¹åº”çš„å‘é‡æ‹¿å‡ºæ¥ï¼Œè·ŸP1çš„åŒºåˆ«æ˜¯å¤šäº†ä¸ªçº¿æ€§å˜æ¢å±‚+æ¿€æ´»å±‚ã€‚
- P3ï¼šæŠŠencoderçš„æœ€åä¸€å±‚çš„æ‰€æœ‰å‘é‡å–å¹³å‡ã€‚
- P4ï¼šæŠŠencoderçš„ç¬¬ä¸€å±‚ä¸æœ€åä¸€å±‚çš„æ‰€æœ‰å‘é‡å–å¹³å‡ã€‚
- P5ï¼šæŠŠencoderçš„æœ€åä¸€å±‚å’Œå€’æ•°ç¬¬äºŒå±‚çš„æ‰€æœ‰å‘é‡å–å¹³å‡ã€‚
	
ç›¸å…³æ¨¡å‹
- (1) <span style='color:blue'>Sentence-BERT</span>
  - Sentence-BERT(SBERT)åˆ©ç”¨`å­ªç”Ÿç½‘ç»œ`ç”Ÿæˆå…·æœ‰è¯­ä¹‰çš„Sentence embeddingï¼Œè¯­ä¹‰ç›¸è¿‘çš„å¥å­çš„embeddingå‘é‡ä¹Ÿç›¸ä¼¼ã€‚
  - å°†Sentence Aå’ŒSentence Bç»è¿‡ BERT+Pooling åå¾—åˆ°å‘é‡æ‹¼åœ¨ä¸€èµ·ï¼Œç„¶åè¾“å…¥softmaxåˆ†ç±»å™¨ï¼Œåˆ¤æ–­Aå’ŒBçš„ç›¸ä¼¼æ€§ã€‚
  - ![](https://pic4.zhimg.com/80/v2-fb807b09b106535a26bf2728b26f91a7_1440w.jpg)
- (2) <span style='color:blue'>SBERT-WK</span>
  - Motivationï¼š BERTä¸åŒå±‚çš„å‘é‡å…·æœ‰ä¸åŒçš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¸€å¥è¯ä¸åŒtokenå…·æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚
  - Methodï¼šå¦‚æœä¸€ä¸ªtokenåœ¨æŸå±‚çš„embeddingå’Œç›¸é‚»å±‚çš„embeddingçš„ç›¸å…³æ€§è¶Šå¤§ï¼Œé‚£ä¹ˆè¿™ä¸€å±‚embeddingåŒ…å«çš„ä¿¡æ¯è¶Šå°‘ï¼Œæƒé‡è¶Šå°ã€‚å¦‚æœä¸€ä¸ªtokenç›¸é‚»å±‚çš„ç›¸å…³ç³»æ•°çš„æ–¹å·®è¶Šå¤§ï¼Œè¿™ä¸ªtokençš„ä¿¡æ¯è¶Šå¤šï¼Œè¿™ä¸ªtokençš„æƒé‡è¶Šå¤§ã€‚
- (3) <span style='color:blue'>BERT-Whitening</span>
  - Motivationï¼šç”¨cosinæ¥è®¡ç®—ç›¸ä¼¼åº¦çš„æ—¶å€™ï¼Œå‘é‡çš„å„ä¸ªç»´åº¦åº”è¯¥æ˜¯**å„é¡¹åŒæ€§**isotropyã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡whiteningæ“ä½œæ¥å°†anisotropyçš„å‘é‡è½¬åŒ–ä¸ºisotropyã€‚
  - `å„å‘åŒæ€§`ï¼šæŒ‡ç‰©ä½“çš„ç‰©ç†ã€åŒ–å­¦ç­‰æ–¹é¢çš„æ€§è´¨ä¸ä¼šå› æ–¹å‘çš„ä¸åŒè€Œæœ‰æ‰€å˜åŒ–çš„ç‰¹æ€§ï¼Œå³æŸä¸€ç‰©ä½“åœ¨ä¸åŒçš„æ–¹å‘æ‰€æµ‹å¾—çš„æ€§èƒ½æ•°å€¼å®Œå…¨ç›¸åŒï¼Œäº¦ç§°å‡è´¨æ€§ã€‚åœ¨sentence embeddingä¸­æŒ‡å„ä¸ªç»´åº¦å…·æœ‰ç›¸åŒçš„é‡è¦æ€§ã€‚
  - BERT-whiteningçš„æ€è·¯å¾ˆç®€å•ï¼Œå°±æ˜¯åœ¨å¾—åˆ°æ¯ä¸ªå¥å­çš„å¥å‘é‡${x_i}_{i=1}^N$åï¼Œå¯¹è¿™äº›çŸ©é˜µè¿›è¡Œä¸€ä¸ªç™½åŒ–ä½¿å¾—æ¯ä¸ªç»´åº¦çš„å‡å€¼ä¸º0ã€åæ–¹å·®çŸ©é˜µä¸ºå•ä½é˜µï¼Œç„¶åä¿ç•™ k ä¸ªä¸»æˆåˆ†
- (4) <span style='color:blue'>SimCSE å¯¹æ¯”å­¦ä¹ </span>
  - é€šè¿‡ contrastive loss æ¥å­¦ä¹ æ— ç›‘ç£ã€æœ‰ç›‘ç£çš„è¯­ä¹‰åŒ–å‘é‡ã€‚é€šè¿‡dropoutæ¥äº§ç”Ÿsentenceçš„ä¸åŒaugmentationsã€‚
  - ![](https://pic1.zhimg.com/80/v2-c97cf4e01144956d451bc6b929592c00_1440w.jpg)
- (5) <span style='color:blue'>Contrastive Tension</span>
  - Motivationï¼š
    - ![](https://pic4.zhimg.com/80/v2-9a6aed54002f22a5a96fa3324d3d5253_1440w.jpg)
  - ä¼˜åŒ–ç›®æ ‡ï¼š
    - ![](https://pic2.zhimg.com/80/v2-66628ba8bf10c99bfc740f1cb2764c01_1440w.jpg)
  - è·Ÿ Word2Vec çš„skip gramçš„è®­ç»ƒç›®æ ‡å¾ˆåƒã€‚
  - è®­ç»ƒå‰ï¼šåœ¨è¯­ä¹‰åŒ–ä»»åŠ¡ä¸­ï¼Œä½å±‚ã€ä¸­å±‚çš„å‘é‡æ¯”é¡¶å±‚çš„å¥½ã€‚åŸå› æ˜¯é¢„è®­ç»ƒæ¨¡å‹æœ‰task-biasã€‚
    - ![](https://pic1.zhimg.com/80/v2-6908956a9e474190a1aad877e9d4f7e0_1440w.jpg)
  - è®­ç»ƒåï¼šé¡¶å±‚çš„è¯­ä¹‰åŒ–å‘é‡æ•ˆæœæ˜æ˜¾å˜å¥½ã€‚
    - ![](https://pic3.zhimg.com/80/v2-a53dee7a51f5bbce81ad03b216491b8e_1440w.jpg)
- (6) <span style='color:blue'>TSDAE</span>
  - ç”¨DAEçš„æ€è·¯æ¥åšè¯­ä¹‰åŒ–ç¼–ç ã€‚åŒæ•°æ®é›†ä¸Šï¼Œæ— ç›‘ç£è¯­ä¹‰åŒ–å‘é‡èƒ½è¾¾åˆ°æœ‰ç›‘ç£è¯­ä¹‰å‘é‡çš„93.1%çš„æ•ˆæœã€‚
  - ![](https://pic2.zhimg.com/80/v2-eba26aed71def32dcfcab51552918ee9_1440w.jpg)
  - TSDAE é€šè¿‡å‘è¾“å…¥å¥å­æ·»åŠ æŸç§ç±»å‹çš„å™ªå£°ï¼ˆä¾‹å¦‚åˆ é™¤æˆ–äº¤æ¢å•è¯ï¼‰æ¥è®­ç»ƒSentence Embeddingã€‚å°†æŸåçš„å¥å­ç¼–ç ä¸ºå›ºå®šå¤§å°çš„å‘é‡ï¼Œç„¶åå°†å‘é‡é‡å»ºä¸ºåŸå§‹è¾“å…¥ã€‚
	
### ESå¥å‘é‡

- ã€2020-9-15ã€‘[ElasticTransformers](https://github.com/md-experiments/elastic_transformers)
  - Elastic Transformersï¼šJupyter Notebooké‡Œçš„å¯æ‰©å±•BERTè¯­ä¹‰æœç´¢
- ![](https://github.com/md-experiments/elastic_transformers/raw/master/assets/architecture.png)


### Faiss

- `Faiss`æ˜¯Facebook AIå›¢é˜Ÿå¼€æºçš„é’ˆå¯¹èšç±»å’Œç›¸ä¼¼æ€§æœç´¢åº“ï¼Œä¸ºç¨ å¯†å‘é‡æä¾›**é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢å’Œèšç±»**ï¼Œæ”¯æŒ**åäº¿**çº§åˆ«å‘é‡çš„æœç´¢ï¼Œæ˜¯ç›®å‰æœ€ä¸ºæˆç†Ÿçš„**è¿‘ä¼¼è¿‘é‚»æœç´¢åº“**ã€‚å®ƒåŒ…å«å¤šç§æœç´¢**ä»»æ„**å¤§å°å‘é‡é›†ï¼ˆå¤‡æ³¨ï¼šå‘é‡é›†å¤§å°ç”±RAMå†…å­˜å†³å®šï¼‰çš„ç®—æ³•ï¼Œä»¥åŠç”¨äºç®—æ³•è¯„ä¼°å’Œå‚æ•°è°ƒæ•´çš„æ”¯æŒä»£ç ã€‚Faissç”¨C++ç¼–å†™ï¼Œå¹¶æä¾›ä¸Numpyå®Œç¾è¡”æ¥çš„Pythonæ¥å£ã€‚é™¤æ­¤ä»¥å¤–ï¼Œå¯¹ä¸€äº›æ ¸å¿ƒç®—æ³•æä¾›äº†GPUå®ç°ã€‚ç›¸å…³ä»‹ç»å‚è€ƒã€Š[Faissï¼šFacebook å¼€æºçš„ç›¸ä¼¼æ€§æœç´¢ç±»åº“](https://infoq.cn/article/2017/11/Faiss-Facebook)ã€‹
- Faisså¯¹ä¸€äº›åŸºç¡€çš„ç®—æ³•æä¾›äº†éå¸¸é«˜æ•ˆçš„å®ç°
  - èšç±»Faissæä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„k-meanså®ç°
  - PCAé™ç»´ç®—æ³•
  - PQ(ProductQuantizer)ç¼–ç /è§£ç 
- ç»„ä»¶
  - Faissä¸­æœ€å¸¸ç”¨çš„æ˜¯ç´¢å¼•Indexï¼Œè€Œåæ˜¯PCAé™ç»´ã€PQä¹˜ç§¯é‡åŒ–ï¼Œè¿™é‡Œé’ˆå¯¹Indexå’ŒPQè¿›è¡Œè¯´æ˜ï¼ŒPCAé™ç»´ä»æµç¨‹ä¸Šéƒ½å¯ä»¥ç†è§£ã€‚
- ä»¥å›¾ç‰‡æœç´¢ä¸ºä¾‹ï¼Œæ‰€è°“ç›¸ä¼¼åº¦æœç´¢ï¼Œä¾¿æ˜¯åœ¨ç»™å®šçš„ä¸€å †å›¾ç‰‡ä¸­ï¼Œå¯»æ‰¾å‡ºæˆ‘æŒ‡å®šçš„ç›®æ ‡æœ€åƒçš„Kå¼ å›¾ç‰‡ï¼Œä¹Ÿç®€ç§°ä¸ºKNNï¼ˆKè¿‘é‚»ï¼‰é—®é¢˜ã€‚
  - ![](https://img2018.cnblogs.com/blog/1408825/201903/1408825-20190320225405798-259149897.png)

- [Faissæµç¨‹ä¸åŸç†åˆ†æ](https://www.cnblogs.com/yhzhou/p/10568728.html)

Faiss ä½¿ç”¨åœºæ™¯ï¼šæœ€å¸¸è§çš„äººè„¸æ¯”å¯¹ï¼ŒæŒ‡çº¹æ¯”å¯¹ï¼ŒåŸºå› æ¯”å¯¹ç­‰ã€‚

**Indexä½¿ç”¨**

Faisså¤„ç†å›ºå®šç»´æ•°dçš„å‘é‡é›†åˆï¼Œå‘é‡ç»´åº¦dé€šå¸¸ä¸ºå‡ ååˆ°å‡ ç™¾ã€‚

faiss ä¸‰ä¸ªæœ€åŸºç¡€çš„ index. åˆ†åˆ«æ˜¯ IndexFlatL2, IndexIVFFlat, IndexIVFPQï¼Œæ›´å¤šå‚è§[Guidelines to choose an index](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)
- `IndexFlatL2`ï¼šæœ€åŸºç¡€çš„Index
- `IndexIVFFlat`ï¼šæ›´å¿«çš„æœç´¢ï¼Œå°†æ•°æ®é›†åˆ†å‰²æˆå‡ éƒ¨åˆ†ï¼ŒåŠ å¿«æœç´¢
  - dç»´ç©ºé—´ä¸­å®šä¹‰Voronoiå•å…ƒæ ¼ï¼Œå¹¶ä¸”æ¯ä¸ªæ•°æ®åº“çŸ¢é‡éƒ½è½å…¥å…¶ä¸­ä¸€ä¸ªå•å…ƒæ ¼ä¸­ã€‚åœ¨æœç´¢æ—¶ï¼Œåªæœ‰æŸ¥è¯¢xæ‰€åœ¨å•å…ƒä¸­åŒ…å«çš„æ•°æ®åº“å‘é‡yä¸å°‘æ•°å‡ ä¸ªç›¸é‚»æŸ¥è¯¢å‘é‡è¿›è¡Œæ¯”è¾ƒã€‚(åˆ’åˆ†æœç´¢ç©ºé—´)
    - ä¸æ•°æ®åº“å‘é‡å…·æœ‰ç›¸åŒåˆ†å¸ƒçš„ä»»ä½•å‘é‡é›†åˆä¸Šæ‰§è¡Œè®­ç»ƒ
    - å»ºç´¢å¼•ï¼Œå³`é‡åŒ–å™¨`(quantizer)ï¼Œå®ƒå°†çŸ¢é‡åˆ†é…ç»™Voronoiå•å…ƒã€‚æ¯ä¸ªå•å…ƒç”±ä¸€ä¸ªè´¨å¿ƒå®šä¹‰ï¼Œæ‰¾åˆ°ä¸€ä¸ªçŸ¢é‡æ‰€åœ¨çš„Voronoiå•å…ƒåŒ…æ‹¬åœ¨è´¨å¿ƒé›†ä¸­æ‰¾åˆ°è¯¥çŸ¢é‡çš„æœ€è¿‘é‚»å±…ã€‚è¿™æ˜¯å¦ä¸€ä¸ªç´¢å¼•çš„ä»»åŠ¡ï¼Œé€šå¸¸æ˜¯ç´¢å¼•IndexFlatL2ã€‚
- `IndexIVFPQ`ï¼šå†…å­˜å¼€é”€æ›´å°.
  - IndexFlatL2å’ŒIndexIVFFlatéƒ½å­˜å‚¨å®Œæ•´çš„å‘é‡ï¼Œå†…å­˜å¼€é”€å¤§
  - åŸºäºäº§å“é‡åŒ–å™¨çš„æœ‰æŸå‹ç¼©æ¥å‹ç¼©å­˜å‚¨çš„å‘é‡çš„å˜ä½“ã€‚å‹ç¼©çš„æ–¹æ³•åŸºäºä¹˜ç§¯é‡åŒ–([Product Quantizer](https://hal.archives-ouvertes.fr/file/index/docid/514462/filename/paper_hal.pdf))ï¼ŒçŸ¢é‡æ²¡æœ‰ç²¾ç¡®å­˜å‚¨ï¼Œæœç´¢æ–¹æ³•è¿”å›çš„è·ç¦»ä¹Ÿæ˜¯è¿‘ä¼¼å€¼ã€‚


IndexIVFFlat Demo å®Œæ•´ä»£ç 

```python
# encoding:utf-8
 
# Copyright (c) 2015-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the BSD+Patents license found in the
# LICENSE file in the root directory of this source tree.
 
# author    : Facebook
# translate : h-j-13
 
import numpy as np
d = 64                              # å‘é‡ç»´åº¦
nb = 100000                         # å‘é‡é›†å¤§å°
nq = 10000                          # æŸ¥è¯¢æ¬¡æ•°
np.random.seed(1234)                # éšæœºç§å­,ä½¿ç»“æœå¯å¤ç°
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.
 
import faiss
 
nlist = 100
k = 4
quantizer = faiss.IndexFlatL2(d)  # the other index
index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)
# here we specify METRIC_L2, by default it performs inner-product search
 
assert not index.is_trained
index.train(xb)
assert index.is_trained
 
index.add(xb)                  # æ·»åŠ ç´¢å¼•å¯èƒ½ä¼šæœ‰ä¸€ç‚¹æ…¢
D, I = index.search(xq, k)     # æœç´¢
print(I[-5:])                  # æœ€åˆäº”æ¬¡æŸ¥è¯¢çš„ç»“æœ
index.nprobe = 10              # é»˜è®¤ nprobe æ˜¯1 ,å¯ä»¥è®¾ç½®çš„å¤§ä¸€äº›è¯•è¯•
D, I = index.search(xq, k)
print(I[-5:])                  # æœ€åäº”æ¬¡æŸ¥è¯¢çš„ç»“æœ
```

IndexIVFFlat å®Œæ•´ä»£ç 

```python
# encoding:utf-8
 
# Copyright (c) 2015-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the BSD+Patents license found in the
# LICENSE file in the root directory of this source tree.
 
# author    : Facebook
# translate : h-j-13
 
import numpy as np
 
d = 64                              # å‘é‡ç»´åº¦
nb = 100000                         # å‘é‡é›†å¤§å°
nq = 10000                          # æŸ¥è¯¢æ¬¡æ•°
np.random.seed(1234)                # éšæœºç§å­,ä½¿ç»“æœå¯å¤ç°
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.
 
import faiss
 
nlist = 100
m = 8
k = 4
quantizer = faiss.IndexFlatL2(d)    # å†…éƒ¨çš„ç´¢å¼•æ–¹å¼ä¾ç„¶ä¸å˜
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)
                                    # æ¯ä¸ªå‘é‡éƒ½è¢«ç¼–ç ä¸º8ä¸ªå­—èŠ‚å¤§å°
index.train(xb)
index.add(xb)
D, I = index.search(xb[:5], k)      # æµ‹è¯•
print(I)
print(D)
index.nprobe = 10                   # ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”
D, I = index.search(xq, k)          # æ£€ç´¢
print(I[-5:])
```

Faiss ç´¢å¼•ç±»å‹ï¼š
- Exact Search for L2 #åŸºäºL2è·ç¦»çš„ç¡®å®šæœç´¢åŒ¹é…
- Exact Search for Inner Product #åŸºäºå†…ç§¯çš„ç¡®å®šæœç´¢åŒ¹é…
- Hierarchical Navigable Small World graph exploration #åˆ†å±‚ç´¢å¼•
- Inverted file with exact post-verification #å€’æ’ç´¢å¼•
- Locality-Sensitive Hashing (binary flat index) #æœ¬åœ°æ•æ„Ÿhash
- Scalar quantizer (SQ) in flat mode #æ ‡é‡é‡åŒ–ç´¢å¼•
- Product quantizer (PQ) in flat mode #ç¬›å¡å°”ä¹˜ç§¯ç´¢å¼•
- IVF and scalar quantizer #å€’æ’+æ ‡é‡é‡åŒ–ç´¢å¼•
- IVFADC (coarse quantizer+PQ on residuals) #å€’æ’+ç¬›å¡å°”ä¹˜ç§¯ç´¢å¼•
- IVFADC+R (same as IVFADC with re-ranking based on codes) # å€’æ’+ç¬›å¡å°”ä¹˜ç§¯ç´¢å¼• + åŸºäºç¼–ç å™¨é‡æ’

Faiss å¼€å‘èµ„æ–™ï¼š
- [github](https://github.com/facebookresearch/faiss)
- [tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started)
- [Faisså­¦ä¹ ç¬”è®°](https://blog.csdn.net/u013185349/article/details/103637977)
- åŸºäºFaissçš„ç‰¹å¾å‘é‡ç›¸ä¼¼åº¦æœç´¢å¼•æ“[Milvus](https://milvus.io/cn/)

### Milvus

ã€2021-5-31ã€‘[Milvus æ˜¯ä»€ä¹ˆ](https://milvus.io/cn/docs/overview.md) Milvus æ˜¯ä¸€æ¬¾å¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒé’ˆå¯¹ TB çº§å‘é‡çš„å¢åˆ æ”¹æ“ä½œå’Œè¿‘å®æ—¶æŸ¥è¯¢ï¼Œå…·æœ‰é«˜åº¦çµæ´»ã€ç¨³å®šå¯é ä»¥åŠé«˜é€ŸæŸ¥è¯¢ç­‰ç‰¹ç‚¹ã€‚Milvus é›†æˆäº† Faissã€NMSLIBã€Annoy ç­‰å¹¿æ³›åº”ç”¨çš„å‘é‡ç´¢å¼•åº“ï¼Œæä¾›äº†ä¸€æ•´å¥—ç®€å•ç›´è§‚çš„ APIï¼Œè®©ä½ å¯ä»¥é’ˆå¯¹ä¸åŒåœºæ™¯é€‰æ‹©ä¸åŒçš„ç´¢å¼•ç±»å‹ã€‚æ­¤å¤–ï¼ŒMilvus è¿˜å¯ä»¥å¯¹æ ‡é‡æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œè¿›ä¸€æ­¥æé«˜äº†å¬å›ç‡ï¼Œå¢å¼ºäº†æœç´¢çš„çµæ´»æ€§ã€‚

Milvus æœåŠ¡å™¨é‡‡ç”¨ä¸»ä»å¼æ¶æ„ (Client-server model)ã€‚åœ¨æœåŠ¡ç«¯ï¼ŒMilvus ç”± Milvus Core å’Œ Meta Store ä¸¤éƒ¨åˆ†ç»„æˆï¼š
- Milvus Core å­˜å‚¨ä¸ç®¡ç†å‘é‡å’Œæ ‡é‡æ•°æ®ã€‚
- Meta Store å­˜å‚¨ä¸ç®¡ç† SQLite å’Œ MySQL ä¸­çš„å…ƒæ•°æ®ï¼Œåˆ†åˆ«ç”¨äºæµ‹è¯•å’Œç”Ÿäº§ã€‚
åœ¨å®¢æˆ·ç«¯ï¼ŒMilvus è¿˜æä¾›äº†åŸºäº Pythonã€Javaã€Goã€C++ çš„ SDK å’Œ RESTful APIã€‚

æ•´ä½“æ¶æ„

![](https://milvus.io/static/822d9e7c7b1dd7cd0c9e27040be06bbe/1e088/milvus_arch.png)

Milvus åœ¨å…¨çƒèŒƒå›´å†…å·²è¢«æ•°ç™¾å®¶ç»„ç»‡å’Œæœºæ„æ‰€é‡‡ç”¨ï¼Œå¹¿æ³›åº”ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š
- å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰éŸ³è§†é¢‘æœç´¢é¢†åŸŸ
- æ–‡æœ¬æœç´¢ã€æ¨èå’Œäº¤äº’å¼é—®ç­”ç³»ç»Ÿç­‰æ–‡æœ¬æœç´¢é¢†åŸŸ
- æ–°è¯æœç´¢ã€åŸºå› ç­›é€‰ç­‰ç”Ÿç‰©åŒ»è¯é¢†åŸŸ
	
	
### ESé‡Œçš„BERTç´¢å¼•

- ã€2020-7-11ã€‘ESå¼€å§‹æ”¯æŒembeddingçš„BERTç´¢å¼•ï¼Œ[Elasticsearché‡ä¸ŠBERTï¼šä½¿ç”¨Elasticsearchå’ŒBERTæ„å»ºæœç´¢å¼•æ“](https://mp.weixin.qq.com/s/PzhdvwsR3ru2u_oVqSxxPQ)
- ã€2019-7-5ã€‘[BERTå’ŒTensorFlowæ„å»ºæœç´¢å¼•æ“](https://cloud.tencent.com/developer/article/1458233)
  - ![](https://ask.qcloudimg.com/http-save/yehe-5669851/5xixkmgeim.jpeg?imageView2/2/w/1620)

### BERTç»“åˆFaissçš„è¯­ä¹‰è¡¨ç¤º

ã€2021-5-31ã€‘è¯­ä¹‰åŒ¹é…æœç´¢é¡¹ç›®ä½¿ç”¨çš„ Faisså’ŒBERTçš„æ•´ä½“æ¶æ„ [image](https://img-blog.csdnimg.cn/img_convert/a6df8af67afe4b2b7ebebd3d3531d380.png), å‚è€ƒï¼š[åŸºäºæ–‡æœ¬è¯­ä¹‰çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ](https://blog.csdn.net/shenfuli/article/details/107823959)
- æ³¨ï¼šæ·±è“è‰²çº¿ä¸ºæ•°æ®å¯¼å…¥è¿‡ç¨‹ï¼Œæ©˜é»„è‰²çº¿ä¸ºç”¨æˆ·æŸ¥è¯¢è¿‡ç¨‹ã€‚ï¼‰
- é¦–å…ˆï¼Œæœ¬æ–‡é¡¹ç›®ä½¿ç”¨å¼€æºçš„ bert-serving ï¼Œ BERTåšå¥å­ç¼–ç å™¨ï¼Œæ ‡é¢˜æ•°æ®è½¬åŒ–ä¸ºå›ºå®šé•¿åº¦ä¸º 768 ç»´çš„ç‰¹å¾å‘é‡ï¼Œå¹¶å¯¼å…¥ Milvus æˆ–è€…Faissåº“ã€‚
- ç„¶åï¼Œå¯¹å­˜å…¥ Milvus/Faiss åº“ä¸­çš„ç‰¹å¾å‘é‡è¿›è¡Œå­˜å‚¨å¹¶å»ºç«‹ç´¢å¼•ï¼ŒåŒæ—¶åŸå§‹æ•°æ®æä¾›å”¯ä¸€IDç¼–ç ï¼Œå°† ID å’Œå¯¹åº”å†…å®¹å­˜å‚¨åœ¨ PostgreSQL ä¸­ã€‚
- æœ€åï¼Œç”¨æˆ·è¾“å…¥ä¸€ä¸ªæ ‡é¢˜ï¼ŒBERT å°†å…¶è½¬æˆç‰¹å¾å‘é‡ã€‚Milvus/Faiss å¯¹ç‰¹å¾å‘é‡è¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢ï¼Œå¾—åˆ°ç›¸ä¼¼çš„æ ‡é¢˜çš„ ID ï¼Œåœ¨ çŸ¥è¯†åº“ï¼ˆPostgreSQL/MySQL/SQLiteã€‚ã€‚ã€‚ï¼‰ ä¸­æ‰¾å‡º ID å¯¹åº”çš„è¯¦ç»†ä¿¡æ¯è¿”å›

![](https://img-blog.csdnimg.cn/img_convert/a6df8af67afe4b2b7ebebd3d3531d380.png)

ã€2021-10-28ã€‘[FAISS + SBERTå®ç°çš„åäº¿çº§è¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢](https://www.yanxishe.com/TextTranslation/2987?from=wcm)ï¼Œ[Billion-scale semantic similarity search with FAISS+SBERT](https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2)


### BERT-as-service

- Google å·²ç»å…¬å¼€äº† TensorFlow ç‰ˆæœ¬ [BERT](https://github.com/google-research/bert) çš„é¢„è®­ç»ƒæ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥ç”¨äºç”Ÿæˆè¯å‘é‡ï¼Œä½†æ˜¯è¿˜æœ‰æ›´ç®€å•çš„æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨å°è£…å¥½çš„åº“ [bert-as-service](https://github.com/hanxiao/bert-as-service) ã€‚
![](https://img-blog.csdnimg.cn/20190521201148390.gif)

- bert-as-service æ˜¯è…¾è®¯ AI Lab å¼€æºçš„ä¸€ä¸ª BERT æœåŠ¡ï¼ˆè‚–æ¶µå¼€å‘ï¼‰ï¼Œå®ƒè®©ç”¨æˆ·å¯ä»¥ä»¥è°ƒç”¨æœåŠ¡çš„æ–¹å¼ä½¿ç”¨ BERT æ¨¡å‹è€Œä¸éœ€è¦å…³æ³¨ BERT çš„å®ç°ç»†èŠ‚ã€‚bert-as-service åˆ†ä¸ºå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ï¼Œç”¨æˆ·å¯ä»¥ä» python ä»£ç ä¸­è°ƒç”¨æœåŠ¡ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ http çš„æ–¹å¼è®¿é—®ã€‚
- - [å¿«é€Ÿä½¿ç”¨ BERT ç”Ÿæˆè¯å‘é‡ï¼šbert-as-service](https://blog.csdn.net/qq_34832393/article/details/90414293)

- ã€2020-8-20ã€‘ä»¥fastapiä¸ºåŸºç¡€çš„[NLP as a Service](https://github.com/abhimishra91/insight)
- Project Insight is designed to create NLP as a service with code base for both front end GUI (streamlit) and backend server (FastApi) the usage of transformers models on various downstream NLP task.
  - The downstream NLP tasks covered:
  - News Classification
  - Entity Recognition
  - Sentiment Analysis
  - Summarization
  - Information Extraction To Do

#### å®‰è£…

- ç”¨ pip å‘½ä»¤è¿›è¡Œå®‰è£…ï¼Œå®¢æˆ·ç«¯ä¸æœåŠ¡ç«¯å¯ä»¥å®‰è£…åœ¨ä¸åŒçš„æœºå™¨ä¸Šï¼š

```shell
pip install bert-serving-server # æœåŠ¡ç«¯
pip install bert-serving-client # å®¢æˆ·ç«¯ï¼Œä¸æœåŠ¡ç«¯äº’ç›¸ç‹¬ç«‹
```

- å…¶ä¸­ï¼ŒæœåŠ¡ç«¯çš„è¿è¡Œç¯å¢ƒä¸º Python >= 3.5 å’Œ Tensorflow >= 1.10
- å®¢æˆ·ç«¯å¯ä»¥è¿è¡Œäº Python 2 æˆ– Python 3

#### ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

- æ ¹æ® NLP ä»»åŠ¡çš„ç±»å‹å’Œè§„æ¨¡ä¸åŒï¼Œ[Google](https://github.com/google-research/bert#pre-trained-models) æä¾›äº†å¤šç§é¢„è®­ç»ƒæ¨¡å‹ä¾›é€‰æ‹©ï¼š
  - [BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip): ç®€ç¹ä½“ä¸­æ–‡, 12-layer, 768-hidden, 12-heads, 110M parameters
  - [BERT-Base, Multilingual Cased](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip): å¤šè¯­è¨€ï¼ˆ104 ç§ï¼‰, 12-layer, 768-hidden, 12-heads, 110M parameters
  - [BERT-Base, Uncased](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip): è‹±æ–‡ä¸åŒºåˆ†å¤§å°å†™ï¼ˆå…¨éƒ¨è½¬ä¸ºå°å†™ï¼‰, 12-layer, 768-hidden, 12-heads, 110M parameters
  - [BERT-Base, Cased](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip): è‹±æ–‡åŒºåˆ†å¤§å°å†™, 12-layer, 768-hidden, 12-heads , 110M parameters
- ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸­æ–‡æ•ˆæœæ›´å¥½çš„å“ˆå·¥å¤§ç‰ˆ BERTï¼š[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
- è§£å‹ä¸‹è½½åˆ°çš„ .zip æ–‡ä»¶ä»¥åï¼Œä¼šæœ‰ 6 ä¸ªæ–‡ä»¶ï¼š
  - TensorFlow æ¨¡å‹æ–‡ä»¶ï¼ˆbert_model.ckpt) åŒ…å«é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ï¼Œæ¨¡å‹æ–‡ä»¶æœ‰ä¸‰ä¸ª
  - å­—å…¸æ–‡ä»¶ï¼ˆvocab.txt) è®°å½•è¯æ¡ä¸ id çš„æ˜ å°„å…³ç³»
  - é…ç½®æ–‡ä»¶ï¼ˆbert_config.json ) è®°å½•æ¨¡å‹çš„è¶…å‚æ•°

#### å¯åŠ¨ BERT æœåŠ¡

- ä½¿ç”¨ bert-serving-start å‘½ä»¤å¯åŠ¨æœåŠ¡ï¼š
  - å…¶ä¸­ï¼Œ-model_dir æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œ-num_worker æ˜¯çº¿ç¨‹æ•°ï¼Œè¡¨ç¤ºåŒæ—¶å¯ä»¥å¤„ç†å¤šå°‘ä¸ªå¹¶å‘è¯·æ±‚

> bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=2

- å¦‚æœå¯åŠ¨æˆåŠŸï¼ŒæœåŠ¡å™¨ç«¯ä¼šæ˜¾ç¤ºï¼š
![](https://img-blog.csdnimg.cn/20190521201200157.gif)

#### åœ¨å®¢æˆ·ç«¯è·å–å¥å‘é‡

- å¯ä»¥ç®€å•çš„ä½¿ç”¨ä»¥ä¸‹ä»£ç è·å–è¯­æ–™çš„å‘é‡è¡¨ç¤ºï¼š

```python
from bert_serving.client import BertClient
bc = BertClient()
doc_vecs = bc.encode(['First do it', 'then do it right', 'then do it better'])
```

- doc_vecs æ˜¯ä¸€ä¸ª numpy.ndarray ï¼Œå®ƒçš„æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å¥å­å‘é‡ï¼Œé•¿åº¦ç”±è¾“å…¥å¥å­çš„æœ€å¤§é•¿åº¦å†³å®šã€‚å¦‚æœè¦æŒ‡å®šé•¿åº¦ï¼Œå¯ä»¥åœ¨å¯åŠ¨æœåŠ¡ä½¿ç”¨ max_seq_len å‚æ•°ï¼Œè¿‡é•¿çš„å¥å­ä¼šè¢«ä»å³ç«¯æˆªæ–­ã€‚
- BERT çš„å¦ä¸€ä¸ªç‰¹æ€§æ˜¯å¯ä»¥è·å–ä¸€å¯¹å¥å­çš„å‘é‡ï¼Œå¥å­ä¹‹é—´ä½¿ç”¨ \|\|\| ä½œä¸ºåˆ†éš”ï¼Œä¾‹å¦‚ï¼š

```python
bc.encode(['First do it ||| then do it right'])
```

#### è·å–è¯å‘é‡

- å¯åŠ¨æœåŠ¡æ—¶å°†å‚æ•° pooling_strategy è®¾ç½®ä¸º None ï¼š

```shell
# bertæœåŠ¡ç«¯
bert-serving-start -pooling_strategy NONE -model_dir /tmp/english_L-12_H-768_A-12/
```
- è¿™æ—¶çš„è¿”å›æ˜¯è¯­æ–™ä¸­æ¯ä¸ª token å¯¹åº” embedding çš„çŸ©é˜µ

```python
# å®¢æˆ·ç«¯
bc = BertClient()
vec = bc.encode(['hey you', 'whats up?'])

vec  # [2, 25, 768]
vec[0]  # [1, 25, 768], sentence embeddings for `hey you`
vec[0][0]  # [1, 1, 768], word embedding for `[CLS]`
vec[0][1]  # [1, 1, 768], word embedding for `hey`
vec[0][2]  # [1, 1, 768], word embedding for `you`
vec[0][3]  # [1, 1, 768], word embedding for `[SEP]`
vec[0][4]  # [1, 1, 768], word embedding for padding symbol
vec[0][25]  # error, out of index!
```

#### è¿œç¨‹è°ƒç”¨ BERT æœåŠ¡

- å¯ä»¥ä»ä¸€å°æœºå™¨ä¸Šè°ƒç”¨å¦ä¸€å°æœºå™¨çš„ BERT æœåŠ¡ï¼š

```python
# on another CPU machine
from bert_serving.client import BertClient
bc = BertClient(ip='xx.xx.xx.xx')  # ip address of the GPU machine
# ä¸€æ¬¡å¤šè¾“å…¥å‡ ä¸ªï¼Œä¸è¦forå¾ªç¯ä¸€ä¸ªä¸ªè·å–ï¼
bc.encode(['First do it', 'then do it right', 'then do it better'])
```
- è¿™ä¸ªä¾‹å­ä¸­ï¼Œåªéœ€è¦åœ¨å®¢æˆ·ç«¯ pip install -U bert-serving-client

```python
from bert_serving.client import BertClient
import numpy as np

class SimilarModel:
    def __init__(self):
        # ipé»˜è®¤ä¸ºæœ¬åœ°æ¨¡å¼ï¼Œå¦‚æœbertæœåŠ¡éƒ¨ç½²åœ¨å…¶ä»–æœåŠ¡å™¨ä¸Šï¼Œä¿®æ”¹ä¸ºå¯¹åº”ip
        self.bert_client = BertClient(ip='192.168.x.x')

    def close_bert(self):
        self.bert_client.close()

    def get_sentence_vec(self,sentence):
        '''
        æ ¹æ®bertè·å–å¥å­å‘é‡
        :param sentence:
        :return:
        '''
        return self.bert_client .encode([sentence])[0]

    def cos_similar(self,sen_a_vec, sen_b_vec):
        '''
        è®¡ç®—ä¸¤ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
        :param sen_a_vec:
        :param sen_b_vec:
        :return:
        '''
        vector_a = np.mat(sen_a_vec)
        vector_b = np.mat(sen_b_vec)
        num = float(vector_a * vector_b.T)
        denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)
        cos = num / denom
        return cos

if __name__=='__main__':
    # ä»å€™é€‰é›†condinates ä¸­é€‰å‡ºä¸sentence_a æœ€ç›¸è¿‘çš„å¥å­
    condinates = ['ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è”šè“è‰²çš„','å¤ªç©ºä¸ºä»€ä¹ˆæ˜¯é»‘çš„ï¼Ÿ','å¤©ç©ºæ€ä¹ˆæ˜¯è“è‰²çš„','æ˜å¤©å»çˆ¬å±±å¦‚ä½•']
    sentence_a = 'å¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„'
    bert_client = SimilarModel()
    max_cos_similar = 0
    most_similar_sentence = ''
    for sentence_b in condinates:
        sentence_a_vec = bert_client.get_sentence_vec(sentence_a)
        sentence_b_vec = bert_client.get_sentence_vec(sentence_b)
        cos_similar = bert_client.cos_similar(sentence_a_vec,sentence_b_vec)
        if cos_similar > max_cos_similar:
            max_cos_similar = cos_similar
            most_similar_sentence = sentence_b

    print('æœ€ç›¸ä¼¼çš„å¥å­ï¼š',most_similar_sentence)
    bert_client .close_bert()
    # ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è”šè“è‰²çš„
```

æˆ–è€…HTTPè°ƒç”¨ï¼š
```shell
curl -X POST http://xx.xx.xx.xx:8125/encode \
  -H 'content-type: application/json' \
  -d '{"id": 123,"texts": ["hello world"], "is_tokenized": false}'
```

Bertçš„è¾“å‡ºæœ€ç»ˆæœ‰ä¸¤ä¸ªç»“æœå¯ç”¨
- sequence_outputï¼šç»´åº¦ã€batch_size, seq_length, hidden_sizeã€‘ï¼Œè¿™æ˜¯è®­ç»ƒåæ¯ä¸ªtokençš„è¯å‘é‡ã€‚
- pooled_outputï¼šç»´åº¦æ˜¯ã€batch_size, hidden_sizeã€‘ï¼Œæ¯ä¸ªsequenceç¬¬ä¸€ä¸ªä½ç½®CLSçš„å‘é‡è¾“å‡ºï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚

```shell
{
    "id": 123,
    "results": [[768 float-list], [768 float-list]],
    "status": 200
}
```

#### å…¶ä»–

- é…ç½®è¦æ±‚
    - BERT æ¨¡å‹å¯¹å†…å­˜æœ‰æ¯”è¾ƒé«˜çš„è¦æ±‚ï¼Œå¦‚æœå¯åŠ¨æ—¶ä¸€ç›´å¡åœ¨ load graph from model_dir å¯ä»¥å°† num_worker è®¾ç½®ä¸º 1 æˆ–è€…åŠ å¤§æœºå™¨å†…å­˜ã€‚
- å¤„ç†ä¸­æ–‡æ˜¯å¦è¦æå‰åˆ†è¯
    - åœ¨è®¡ç®—ä¸­æ–‡å‘é‡æ—¶ï¼Œå¯ä»¥ç›´æ¥è¾“å…¥æ•´ä¸ªå¥å­ä¸éœ€è¦æå‰åˆ†è¯ã€‚å› ä¸º Chinese-BERT ä¸­ï¼Œè¯­æ–™æ˜¯ä»¥å­—ä¸ºå•ä½å¤„ç†çš„ï¼Œå› æ­¤å¯¹äºä¸­æ–‡è¯­æ–™æ¥è¯´è¾“å‡ºçš„æ˜¯å­—å‘é‡ã€‚
- ä¸¾ä¸ªä¾‹å­ï¼Œå½“ç”¨æˆ·è¾“å…¥ï¼š

```python
bc.encode(['hey you', 'whats up?', 'ä½ å¥½ä¹ˆï¼Ÿ', 'æˆ‘ è¿˜ å¯ä»¥'])
```
- å®é™…ä¸Šï¼ŒBERT æ¨¡å‹çš„è¾“å…¥æ˜¯ï¼š

```
tokens: [CLS] hey you [SEP]
input_ids: 101 13153 8357 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] what ##s up ? [SEP]
input_ids: 101 9100 8118 8644 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] ä½  å¥½ ä¹ˆ ï¼Ÿ [SEP]
input_ids: 101 872 1962 720 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] æˆ‘ è¿˜ å¯ ä»¥ [SEP]
input_ids: 101 2769 6820 1377 809 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```
- åœ¨è‹±è¯­ä¸­è¯æ¡åŒ–åçš„ ##something æ˜¯ä»€ä¹ˆ
    - å½“æŸä¸ªè¯åœ¨ä¸åœ¨è¯å…¸ä¸­æ—¶ï¼Œä½¿ç”¨æœ€é•¿å­åºåˆ—çš„æ–¹æ³•è¿›è¡Œè¯æ¡åŒ–ï¼Œä¾‹å¦‚ï¼š

```python
input = "unaffable"
tokenizer_output = ["un", "##aff", "##able"]
```


## å…¸å‹æ¨¡å‹


### ELMo ï¼ˆæ”¹è¿›W2Vï¼‰

ELMOæ˜¯â€œEmbedding from Language Modelsâ€çš„ç®€ç§°ï¼Œåå­—å¹¶æ²¡æœ‰ååº”æœ¬è´¨æ€æƒ³ï¼Œ
- ELMOçš„è®ºæ–‡é¢˜ç›®ï¼šâ€œDeep contextualized word representationâ€ æ›´èƒ½ä½“ç°å…¶ç²¾é«“ï¼Œè€Œç²¾é«“åœ¨å“ªé‡Œï¼Ÿåœ¨deep contextualizedè¿™ä¸ªçŸ­è¯­ï¼Œä¸€ä¸ªæ˜¯deepï¼Œä¸€ä¸ªæ˜¯contextï¼Œå…¶ä¸­contextæ›´å…³é”®ã€‚
- ä¹‹å‰çš„Word Embeddingæœ¬è´¨ä¸Šæ˜¯ä¸ª**é™æ€**æ–¹å¼ï¼Œæ‰€è°“é™æ€æŒ‡çš„æ˜¯è®­ç»ƒå¥½ä¹‹åæ¯ä¸ªå•è¯çš„è¡¨è¾¾å°±å›ºå®šä½äº†ï¼Œä»¥åä½¿ç”¨çš„æ—¶å€™ï¼Œä¸è®ºæ–°å¥å­ä¸Šä¸‹æ–‡å•è¯æ˜¯ä»€ä¹ˆï¼Œè¿™ä¸ªå•è¯çš„Word Embeddingä¸ä¼šè·Ÿç€ä¸Šä¸‹æ–‡åœºæ™¯çš„å˜åŒ–è€Œæ”¹å˜ï¼Œ
  - æ¯”å¦‚Bankè¿™ä¸ªè¯ï¼Œå®ƒäº‹å…ˆå­¦å¥½çš„Word Embeddingä¸­æ··åˆäº†å‡ ç§è¯­ä¹‰ ï¼Œåœ¨åº”ç”¨ä¸­æ¥äº†ä¸ªæ–°å¥å­ï¼Œå³ä½¿ä»ä¸Šä¸‹æ–‡ä¸­ï¼ˆæ¯”å¦‚å¥å­åŒ…å«moneyç­‰è¯ï¼‰æ˜æ˜¾å¯ä»¥çœ‹å‡ºå®ƒä»£è¡¨çš„æ˜¯â€œé“¶è¡Œâ€çš„å«ä¹‰ï¼Œä½†æ˜¯å¯¹åº”çš„Word Embeddingå†…å®¹ä¹Ÿä¸ä¼šå˜ï¼Œå®ƒè¿˜æ˜¯æ··åˆäº†å¤šç§è¯­ä¹‰ã€‚
  - è¿™æ˜¯ä¸ºä½•è¯´å®ƒæ˜¯é™æ€çš„ï¼Œé—®é¢˜æ‰€åœ¨ã€‚
- ELMOçš„æœ¬è´¨æ€æƒ³ï¼š
  - å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡æ²¡å…³ç³»ã€‚å®é™…ä½¿ç”¨Word Embeddingçš„æ—¶ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„Word Embeddingè¡¨ç¤ºï¼Œè°ƒæ•´åçš„Word Embeddingæ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚
- æ‰€ä»¥ELMOæœ¬èº«æ˜¯æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹Word EmbeddingåŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚

ELMOé‡‡ç”¨äº†å…¸å‹çš„ä¸¤é˜¶æ®µè¿‡ç¨‹
- ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼›
- ç¬¬äºŒä¸ªé˜¶æ®µæ˜¯åœ¨åšä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œä»é¢„è®­ç»ƒç½‘ç»œä¸­æå–å¯¹åº”å•è¯çš„ç½‘ç»œå„å±‚çš„Word Embeddingä½œä¸ºæ–°ç‰¹å¾è¡¥å……åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚

ELMOçš„ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µã€‚
- ![img](https://img.6aiq.com/e/895cdfee707f409886c38c0eb4ce9c20.jpeg-imageStyle)

ç½‘ç»œç»“æ„é‡‡ç”¨äº†**åŒå±‚åŒå‘LSTM**ï¼Œè¯­è¨€æ¨¡å‹è®­ç»ƒçš„ä»»åŠ¡ç›®æ ‡æ˜¯æ ¹æ®å•è¯çš„ä¸Šä¸‹æ–‡å»æ­£ç¡®é¢„æµ‹å•è¯, 
- ä¹‹å‰çš„å•è¯åºåˆ—Context-beforeç§°ä¸ºä¸Šæ–‡
- ä¹‹åçš„å•è¯åºåˆ—Context-afterç§°ä¸ºä¸‹æ–‡

ELMOçš„é¢„è®­ç»ƒè¿‡ç¨‹ä¸ä»…ä»…å­¦ä¼šå•è¯çš„Word Embeddingï¼Œè¿˜å­¦ä¼šäº†ä¸€ä¸ªåŒå±‚åŒå‘çš„LSTMç½‘ç»œç»“æ„

è®­ç»ƒåå¦‚ä½•ä½¿ç”¨ï¼Ÿ
- ![img](https://img.6aiq.com/e/aebde62d3c8d45b580bcb1401fa25d6b.jpeg-imageStyle)
- ä¸‹æ¸¸ä»»åŠ¡çš„ä½¿ç”¨è¿‡ç¨‹
  - æ¯”å¦‚ä¸‹æ¸¸ä»»åŠ¡ä»ç„¶æ˜¯QAé—®é¢˜ï¼Œæ­¤æ—¶å¯¹äºé—®å¥Xï¼Œå¯ä»¥å…ˆå°†å¥å­Xä½œä¸ºé¢„è®­ç»ƒå¥½çš„ELMOç½‘ç»œçš„è¾“å…¥ï¼Œè¿™æ ·å¥å­Xä¸­æ¯ä¸ªå•è¯åœ¨ELMOç½‘ç»œä¸­éƒ½èƒ½è·å¾—å¯¹åº”çš„ä¸‰ä¸ªEmbeddingï¼Œä¹‹åç»™äºˆè¿™ä¸‰ä¸ªEmbeddingä¸­çš„æ¯ä¸€ä¸ªEmbeddingä¸€ä¸ªæƒé‡aï¼Œè¿™ä¸ªæƒé‡å¯ä»¥å­¦ä¹ å¾—æ¥ï¼Œæ ¹æ®å„è‡ªæƒé‡ç´¯åŠ æ±‚å’Œï¼Œå°†ä¸‰ä¸ªEmbeddingæ•´åˆæˆä¸€ä¸ªã€‚ç„¶åå°†æ•´åˆåçš„è¿™ä¸ªEmbeddingä½œä¸ºXå¥åœ¨è‡ªå·±ä»»åŠ¡çš„é‚£ä¸ªç½‘ç»œç»“æ„ä¸­å¯¹åº”å•è¯çš„è¾“å…¥ï¼Œä»¥æ­¤ä½œä¸ºè¡¥å……çš„æ–°ç‰¹å¾ç»™ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€‚å¯¹äºå›¾ç¤ºä¸‹æ¸¸ä»»åŠ¡QAä¸­çš„å›ç­”å¥å­Yæ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤å¤„ç†ã€‚å› ä¸ºELMOç»™ä¸‹æ¸¸æä¾›çš„æ˜¯æ¯ä¸ªå•è¯çš„ç‰¹å¾å½¢å¼ï¼Œæ‰€ä»¥è¿™ä¸€ç±»é¢„è®­ç»ƒçš„æ–¹æ³•è¢«ç§°ä¸ºâ€œFeature-based Pre-Trainingâ€ã€‚è‡³äºä¸ºä½•è¿™ä¹ˆåšèƒ½å¤Ÿè¾¾åˆ°åŒºåˆ†å¤šä¹‰è¯çš„æ•ˆæœï¼Œå¯ä»¥æƒ³ä¸€æƒ³ï¼Œå…¶å®æ¯”è¾ƒå®¹æ˜“æƒ³æ˜ç™½åŸå› ã€‚

é™æ€Word Embeddingæ— æ³•è§£å†³å¤šä¹‰è¯çš„é—®é¢˜ï¼Œé‚£ä¹ˆELMOå¼•å…¥ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´å•è¯çš„embeddingåå¤šä¹‰è¯é—®é¢˜è§£å†³äº†å—ï¼Ÿ
- è§£å†³äº†ï¼Œè€Œä¸”æ¯”æœŸå¾…çš„è¿˜è¦å¥½ã€‚
- ![img](https://img.6aiq.com/e/0d63d55ce77b4a20bb843e90920ff072.jpeg-imageStyle)
- å¯¹äºGloveè®­ç»ƒå‡ºçš„Word Embeddingæ¥è¯´ï¼Œå¤šä¹‰è¯æ¯”å¦‚playï¼Œæ ¹æ®å®ƒçš„embeddingæ‰¾å‡ºçš„æœ€æ¥è¿‘çš„å…¶å®ƒå•è¯å¤§å¤šæ•°é›†ä¸­åœ¨ä½“è‚²é¢†åŸŸï¼Œè¿™å¾ˆæ˜æ˜¾æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®ä¸­åŒ…å«playçš„å¥å­ä¸­ä½“è‚²é¢†åŸŸçš„æ•°é‡æ˜æ˜¾å ä¼˜å¯¼è‡´ï¼›è€Œä½¿ç”¨ELMOï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´åçš„embeddingä¸ä»…èƒ½å¤Ÿæ‰¾å‡ºå¯¹åº”çš„â€œæ¼”å‡ºâ€çš„ç›¸åŒè¯­ä¹‰çš„å¥å­ï¼Œè€Œä¸”è¿˜å¯ä»¥ä¿è¯æ‰¾å‡ºçš„å¥å­ä¸­çš„playå¯¹åº”çš„è¯æ€§ä¹Ÿæ˜¯ç›¸åŒçš„ï¼Œè¿™æ˜¯è¶…å‡ºæœŸå¾…ä¹‹å¤„ã€‚ç¬¬ä¸€å±‚LSTMç¼–ç äº†å¾ˆå¤šå¥æ³•ä¿¡æ¯ï¼Œè¿™åœ¨è¿™é‡Œèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚

ELMOç»è¿‡è¿™èˆ¬æ“ä½œï¼Œæ•ˆæœå¦‚ä½•å‘¢ï¼Ÿ
- å®éªŒæ•ˆæœï¼Œ6ä¸ªNLPä»»åŠ¡ä¸­æ€§èƒ½éƒ½æœ‰å¹…åº¦ä¸åŒçš„æå‡ï¼Œæœ€é«˜çš„æå‡è¾¾åˆ°25%å·¦å³ï¼Œè€Œä¸”è¿™6ä¸ªä»»åŠ¡çš„è¦†ç›–èŒƒå›´æ¯”è¾ƒå¹¿ï¼ŒåŒ…å«å¥å­è¯­ä¹‰å…³ç³»åˆ¤æ–­ï¼Œåˆ†ç±»ä»»åŠ¡ï¼Œé˜…è¯»ç†è§£ç­‰å¤šä¸ªé¢†åŸŸï¼Œè¿™è¯´æ˜å…¶é€‚ç”¨èŒƒå›´æ˜¯éå¸¸å¹¿çš„ï¼Œæ™®é€‚æ€§å¼ºï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ä¼˜ç‚¹ã€‚

### GPT

- GPTæ¨¡å‹ï¼Œè¯¦è§ä¸“é¢˜ï¼š[GPT](/gpt)

### BERT

- BERTæ¨¡å‹ï¼Œè¯¦è§ä¸“é¢˜ï¼š[BERT](/bert)

# NLPæ–°èŒƒå¼ï¼šPrompt

ã€2021-8-3ã€‘[Fine-tuneä¹‹åçš„NLPæ–°èŒƒå¼ï¼šPromptè¶Šæ¥è¶Šç«ï¼ŒCMUåäººåšå£«åå‡ºäº†ç¯‡ç»¼è¿°æ–‡ç« ](https://blog.csdn.net/xixiaoyaoww/article/details/119363189)ï¼ŒCMU åšå£«åç ”ç©¶å‘˜åˆ˜é¹é£ï¼šè¿‘ä»£è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•çš„ç¬¬å››èŒƒå¼å¯èƒ½æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åŠ æŒä¸‹çš„ Prompt Learningã€‚
- ä» BERT å¼€å§‹ï¼Œ**é¢„è®­ç»ƒ+finetune** å·²ç»æˆä¸ºäº†æ•´ä¸ªé¢†åŸŸçš„å¸¸è§„èŒƒå¼ã€‚ä½†æ˜¯ä» GPT-3 å¼€å§‹ï¼Œä¸€ç§æ–°èŒƒå¼å¼€å§‹å¼•èµ·å¤§å®¶çš„å…³æ³¨å¹¶è¶Šæ¥è¶Šæµè¡Œï¼š**prompting**ã€‚
- [è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2107.13586.pdf)ï¼Œæ›´å¤šç ”ç©¶è§ï¼šæ¸…åå¤§å­¦å¼€æºçš„è®ºæ–‡åˆ—è¡¨ [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers)
- ![img](https://img-blog.csdnimg.cn/img_convert/e538ffef7d05deaf84a5a66225c7f4bc.png)

## NLP èŒƒå¼

å…¨ç›‘ç£å­¦ä¹ åœ¨ NLP é¢†åŸŸä¹Ÿéå¸¸é‡è¦ã€‚ä½†æ˜¯å…¨ç›‘ç£çš„æ•°æ®é›†å¯¹äºå­¦ä¹ é«˜è´¨é‡çš„æ¨¡å‹æ¥è¯´æ˜¯ä¸å……è¶³çš„ï¼Œæ—©æœŸçš„ NLP æ¨¡å‹ä¸¥é‡ä¾èµ–ç‰¹å¾å·¥ç¨‹ã€‚éšç€ç”¨äº NLP ä»»åŠ¡çš„ç¥ç»ç½‘ç»œå‡ºç°ï¼Œä½¿å¾—ç‰¹å¾å­¦ä¹ ä¸æ¨¡å‹è®­ç»ƒç›¸ç»“åˆï¼Œç ”ç©¶è€…å°†ç ”ç©¶é‡ç‚¹è½¬å‘äº†æ¶æ„å·¥ç¨‹ï¼Œå³é€šè¿‡è®¾è®¡ä¸€ä¸ªç½‘ç»œæ¶æ„èƒ½å¤Ÿå­¦ä¹ æ•°æ®ç‰¹å¾ã€‚å„ç§æ¨¡å¼çš„å¯¹æ¯”å¦‚ä¸‹ï¼š

|æ¨¡å¼paradigm|å·¥ç¨‹é‡å¿ƒengineering|ç¤ºä¾‹|ä»»åŠ¡å…³ç³»task relation|
|---|---|---|---|
|â‘ å…¨ç›‘ç£ï¼ˆéç¥ç»ç½‘ç»œï¼‰|ç‰¹å¾|å¦‚å•è¯ï¼Œè¯æ€§ï¼Œå¥å­é•¿åº¦ç­‰|åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€è¯­è¨€æ¨¡å‹ï¼ˆæ— ç›‘ç£ï¼‰ã€ç”Ÿæˆ|
|â‘¡å…¨ç›‘ç£ï¼ˆç¥ç»ç½‘ç»œï¼‰|ç»“æ„|å¦‚å·ç§¯ã€å¾ªç¯ã€è‡ªæ³¨æ„åŠ›|åŒä¸Š|
|â‘¢pre-trainä¸fine-tune|ç›®æ ‡|æ©ç è¯­è¨€æ¨¡å‹ã€NSPä¸‹ä¸€å¥é¢„æµ‹|ä»¥è¯­è¨€æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œå«æ— ç›‘ç£è®­ç»ƒ|
|â‘£pre-trainã€promptä¸predict|æç¤º|å®Œå½¢å¡«ç©ºã€å‰ç¼€|è¯­è¨€æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œå«æ–‡æœ¬æç¤º|

- ![img](https://img-blog.csdnimg.cn/img_convert/e7be4976c76f47cf54a95a7dcd2150b9.png)

## Prompt æ¦‚è¦

è¯¥ç»¼è¿°ç ”ç©¶è¯•å›¾é€šè¿‡æä¾› prompting æ–¹æ³•çš„æ¦‚è¿°å’Œå½¢å¼åŒ–å®šä¹‰ï¼Œä»¥åŠä½¿ç”¨è¿™äº› prompt çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ¦‚è¿°ï¼Œæ¥æ¢³ç†è¿™ä¸€è¿…é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰çŸ¥è¯†çŠ¶æ€ã€‚ç„¶åè¯¥è®ºæ–‡å¯¹ prompt æ–¹æ³•è¿›è¡Œäº†æ·±å…¥çš„è®¨è®ºï¼ŒåŒ…æ‹¬ **promptå·¥ç¨‹**ã€**answerå·¥ç¨‹**ç­‰åŸºç¡€å’Œ**å¤špromptå­¦ä¹ **æ–¹æ³•ã€**promptç›¸å…³çš„è®­ç»ƒæ–¹æ³•**ç­‰æ›´é«˜çº§çš„æ¦‚å¿µã€‚

ç„¶åï¼Œè¯¥ç ”ç©¶åˆ—å‡ºäº†å·²æœ‰çš„åŸºäº prompt å­¦ä¹ æ–¹æ³•çš„å¤šç§åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†ä¸åŒåº”ç”¨åœºæ™¯ä¸­å¦‚ä½•é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ–¹æ³•ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°è¯•åœ¨ç ”ç©¶ç”Ÿæ€ç³»ç»Ÿä¸­å®šä½ prompt æ–¹æ³•çš„å½“å‰çŠ¶æ€ï¼Œå¹¶ä¸å…¶ä»–ç ”ç©¶é¢†åŸŸå»ºç«‹è”ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºä¸€äº›å¯èƒ½é€‚åˆè¿›ä¸€æ­¥ç ”ç©¶çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå¹¶é’ˆå¯¹å½“å‰ç ”ç©¶è¶‹åŠ¿è¿›è¡Œäº†åˆ†æã€‚

åŸºäº Prompt çš„å­¦ä¹ æ–¹æ³•è¯•å›¾é€šè¿‡**å­¦ä¹ LM**æ¥è§„é¿è¿™ä¸€é—®é¢˜ï¼Œè¯¥ LM å¯¹æ–‡æœ¬ x æœ¬èº«çš„æ¦‚ç‡ P(x; Î¸) è¿›è¡Œå»ºæ¨¡å¹¶ä½¿ç”¨è¯¥æ¦‚ç‡æ¥é¢„æµ‹ yï¼Œä»è€Œå‡å°‘æˆ–æ¶ˆé™¤äº†è®­ç»ƒæ¨¡å‹å¯¹å¤§å‹ç›‘ç£æ•°æ®é›†çš„éœ€æ±‚ã€‚

æœ€åŸºæœ¬çš„ Prompt å½¢å¼çš„æ•°å­¦æè¿°ï¼ŒåŒ…å«è®¸å¤šæœ‰å…³ Prompt çš„å·¥ä½œï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å†…å®¹ã€‚

åŸºç¡€ Prompt åˆ†ä¸‰æ­¥é¢„æµ‹å¾—åˆ†æœ€é«˜çš„ ^yï¼Œå³ï¼š**prompt æ·»åŠ **ã€**answer æœç´¢**å’Œ **answer æ˜ å°„**ã€‚prompting æ–¹æ³•çš„æœ¯è¯­å’Œç¬¦å·ã€‚
- ![img](https://img-blog.csdnimg.cn/img_convert/c441c670a31e4b2669da835842b9f171.png)
ä¸åŒä»»åŠ¡çš„è¾“å…¥ã€æ¨¡æ¿å’Œ answer ç¤ºä¾‹ï¼š
- ![img](https://img-blog.csdnimg.cn/img_convert/90e84a4ad6cf465e3e306fc376581bcf.png)

## Promptè®¾è®¡æ€è·¯

Prompting è®¾è®¡è€ƒè™‘ï¼š
- é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ï¼šæœ‰è®¸å¤šé¢„è®­ç»ƒ LM å¯ä»¥ç”¨æ¥è®¡ç®— P(x; Î¸)ã€‚åœ¨ç¬¬ 3 ç« ä¸­ï¼Œç ”ç©¶è€…å¯¹é¢„è®­ç»ƒ LM è¿›è¡Œäº†åˆæ­¥çš„ä»‹ç»ï¼›
- **Prompt å·¥ç¨‹**ï¼šå¦‚æœ prompt æŒ‡å®šäº†ä»»åŠ¡ï¼Œé‚£ä¹ˆé€‰æ‹©æ­£ç¡®çš„ prompt ä¸ä»…å¯¹å‡†ç¡®ç‡å½±å“å¾ˆå¤§ï¼Œè€Œä¸”å¯¹æ¨¡å‹é¦–å…ˆæ‰§è¡Œçš„ä»»åŠ¡ä¹Ÿæœ‰å¾ˆå¤§å½±å“ã€‚åœ¨ç¬¬ 4 ç« ä¸­ï¼Œç ”ç©¶è€…è®¨è®ºäº†åº”è¯¥é€‰æ‹©å“ªä¸ª prompt ä½œä¸º f_prompt(x) æ–¹æ³•ï¼›
- **Answer å·¥ç¨‹**ï¼šæ ¹æ®ä»»åŠ¡çš„ä¸åŒï¼Œä¼šæœ‰ä¸åŒçš„æ–¹å¼è®¾è®¡ Z (Answer)ï¼Œå¯èƒ½ä¼šå’Œæ˜ å°„å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚åœ¨ç¬¬ 5 ç« ä¸­ï¼Œè¯¦ç»†ä»‹ç»äº†ä¸åŒçš„è®¾è®¡æ–¹å¼ï¼›
- **æ‰©å±•èŒƒå¼**ï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œ ä¸Šé¢çš„å…¬å¼ä»…ä»…ä»£è¡¨äº†å„ç§åº•å±‚æ¡†æ¶ä¸­æœ€ç®€å•çš„ä¸€ç§ï¼Œè¿™äº›æ¡†æ¶å·²ç»è¢«æè®®ç”¨äºæ‰§è¡Œå„ç§ promptingã€‚åœ¨ ç¬¬ 6 ç« ä¸­ï¼Œç ”ç©¶è€…è®¨è®ºäº†æ‰©å±•è¿™ç§åŸºæœ¬èŒƒå¼ä»¥è¿›ä¸€æ­¥æé«˜ç»“æœæˆ–é€‚ç”¨æ€§çš„æ–¹æ³•ï¼›
- åŸºäº Prompt çš„**è®­ç»ƒç­–ç•¥**ï¼šåœ¨ç¬¬ 7 ç« ä¸­ï¼Œç ”ç©¶è€…æ€»ç»“äº†ä¸åŒçš„è®­ç»ƒç­–ç•¥å¹¶è¯¦ç»†è¯´æ˜å®ƒä»¬çš„ç›¸å¯¹ä¼˜åŠ¿

![img](https://img-blog.csdnimg.cn/img_convert/eae05d97522535d2a976353daae592c2.png)

### ä»€ä¹ˆæ˜¯ promptï¼Ÿ

NLPä¸­ Prompt ä»£è¡¨çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ

prompt å°±æ˜¯ç»™ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ çš„ä¸€ä¸ªçº¿ç´¢/æç¤ºï¼Œå¸®åŠ©å®ƒå¯ä»¥æ›´å¥½çš„ç†è§£ äººç±»çš„é—®é¢˜ã€‚

### ç¤ºä¾‹

æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼š
- â€œI missed the bus today.â€ å¥å­åç´§è·Ÿç€ç»™å‡ºè¿™æ ·ä¸€ä¸ªpromptï¼šâ€œI felt so _____â€
- â€œEnglish: I missed the bus today. French: ______â€

PLMå°†è‡ªåŠ¨è¡¥å……å•è¯sadï¼Œæˆ–ä½¿ç”¨æ³•è¯­æ¥è¿›è¡Œå¡«ç©ºã€‚

ã€2023-2-9ã€‘[ã€NLPã€‘Prompt Learning è¶…å¼ºå…¥é—¨æ•™ç¨‹](https://zhuanlan.zhihu.com/p/442486331)


# ç»“æŸ